{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panik-79/arxiv_paper_recommendation_model/blob/main/minor_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "118cec97",
      "metadata": {
        "id": "118cec97"
      },
      "source": [
        "# This Notebook Perform two things...........\n",
        "\n",
        "# 1 Section:                                                                 \n",
        "Research Area Subject Area Prediction (Large Scale classification) using shallow Multi-Layer Perceptron (MLP) model\n",
        "\n",
        "# 2 Section:\n",
        "Research Paper Recommendation for reading: using sentence transformer model\n",
        "\n",
        "Research Papers dataset link::\n",
        "https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda1c5f2-22d3-41c9-9690-a515719253ba",
      "metadata": {
        "id": "eda1c5f2-22d3-41c9-9690-a515719253ba"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2b7cb120",
      "metadata": {
        "id": "2b7cb120"
      },
      "source": [
        "# 1 Section:                                                                 "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5adda69d",
      "metadata": {
        "id": "5adda69d"
      },
      "source": [
        "# Loading tools and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a0cb9160",
      "metadata": {
        "id": "a0cb9160"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from ast import literal_eval\n",
        "# is used for safely evaluating strings containing Python literals or container displays\n",
        "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "130dc39f",
      "metadata": {
        "id": "130dc39f"
      },
      "outputs": [],
      "source": [
        "arxiv_data = pd.read_csv(\"arxiv_data_210930-054931.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "6a912841",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6a912841",
        "outputId": "c17cccd5-b6f9-4244-ad23-babe6dfcfd93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           terms  \\\n",
              "0                      ['cs.LG']   \n",
              "1             ['cs.LG', 'cs.AI']   \n",
              "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
              "3             ['cs.LG', 'cs.CR']   \n",
              "4                      ['cs.LG']   \n",
              "\n",
              "                                                                                                             titles  \\\n",
              "0  Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities   \n",
              "1       Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes   \n",
              "2                                                   Power up! Robust Graph Convolutional Network via Graph Powering   \n",
              "3                                              Releasing Graph Neural Networks with Differential Privacy Guarantees   \n",
              "4                               Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          abstracts  \n",
              "0  Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.  \n",
              "1                                                                                                                                                              Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.  \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Graph convolutional networks (GCNs) are powerful tools for graph-structured\\ndata. However, they have been recently shown to be vulnerable to topological\\nattacks. To enhance adversarial robustness, we go beyond spectral graph theory\\nto robust graph theory. By challenging the classical graph Laplacian, we\\npropose a new convolution operator that is provably robust in the spectral\\ndomain and is incorporated in the GCN architecture to improve expressivity and\\ninterpretability. By extending the original graph to a sequence of graphs, we\\nalso propose a robust training paradigm that encourages transferability across\\ngraphs that span a range of spatial and spectral characteristics. The proposed\\napproaches are demonstrated in extensive experiments to simultaneously improve\\nperformance in both benign and adversarial situations.  \n",
              "3                                                                                                                                                                                                                    With the increasing popularity of Graph Neural Networks (GNNs) in several\\nsensitive applications like healthcare and medicine, concerns have been raised\\nover the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to\\nprivacy attacks, such as membership inference attacks, even if only blackbox\\naccess to the trained model is granted. To build defenses, differential privacy\\nhas emerged as a mechanism to disguise the sensitive data in training datasets.\\nFollowing the strategy of Private Aggregation of Teacher Ensembles (PATE),\\nrecent methods leverage a large ensemble of teacher models. These teachers are\\ntrained on disjoint subsets of private data and are employed to transfer\\nknowledge to a student model, which is then released with privacy guarantees.\\nHowever, splitting graph data into many disjoint training sets may destroy the\\nstructural information and adversely affect accuracy. We propose a new\\ngraph-specific scheme of releasing a student GNN, which avoids splitting\\nprivate training data altogether. The student GNN is trained using public data,\\npartly labeled privately using the teacher GNN models trained exclusively for\\neach query node. We theoretically analyze our approach in the R\\`{e}nyi\\ndifferential privacy framework and provide privacy guarantees. Besides, we show\\nthe solid experimental performance of our method compared to several baselines,\\nincluding the PATE baseline adapted for graph-structured data. Our anonymized\\ncode is available.  \n",
              "4                                                                                                                                                                                                                                           Machine learning solutions for pattern classification problems are nowadays\\nwidely deployed in society and industry. However, the lack of transparency and\\naccountability of most accurate models often hinders their safe use. Thus,\\nthere is a clear need for developing explainable artificial intelligence\\nmechanisms. There exist model-agnostic methods that summarize feature\\ncontributions, but their interpretability is limited to predictions made by\\nblack-box models. An open challenge is to develop models that have intrinsic\\ninterpretability and produce their own explanations, even for classes of models\\nthat are traditionally considered black boxes like (recurrent) neural networks.\\nIn this paper, we propose a Long-Term Cognitive Network for interpretable\\npattern classification of structured data. Our method brings its own mechanism\\nfor providing explanations by quantifying the relevance of each feature in the\\ndecision process. For supporting the interpretability without affecting the\\nperformance, the model incorporates more flexibility through a quasi-nonlinear\\nreasoning rule that allows controlling nonlinearity. Besides, we propose a\\nrecurrence-aware decision model that evades the issues posed by unique fixed\\npoints while introducing a deterministic learning method to compute the tunable\\nparameters. The simulations show that our interpretable model obtains\\ncompetitive results when compared to the state-of-the-art white and black-box\\nmodels.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80dbe7d7-7249-48a1-b13c-c8f7e802b9ac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terms</th>\n",
              "      <th>titles</th>\n",
              "      <th>abstracts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['cs.LG']</td>\n",
              "      <td>Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities</td>\n",
              "      <td>Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['cs.LG', 'cs.AI']</td>\n",
              "      <td>Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes</td>\n",
              "      <td>Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
              "      <td>Power up! Robust Graph Convolutional Network via Graph Powering</td>\n",
              "      <td>Graph convolutional networks (GCNs) are powerful tools for graph-structured\\ndata. However, they have been recently shown to be vulnerable to topological\\nattacks. To enhance adversarial robustness, we go beyond spectral graph theory\\nto robust graph theory. By challenging the classical graph Laplacian, we\\npropose a new convolution operator that is provably robust in the spectral\\ndomain and is incorporated in the GCN architecture to improve expressivity and\\ninterpretability. By extending the original graph to a sequence of graphs, we\\nalso propose a robust training paradigm that encourages transferability across\\ngraphs that span a range of spatial and spectral characteristics. The proposed\\napproaches are demonstrated in extensive experiments to simultaneously improve\\nperformance in both benign and adversarial situations.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['cs.LG', 'cs.CR']</td>\n",
              "      <td>Releasing Graph Neural Networks with Differential Privacy Guarantees</td>\n",
              "      <td>With the increasing popularity of Graph Neural Networks (GNNs) in several\\nsensitive applications like healthcare and medicine, concerns have been raised\\nover the privacy aspects of trained GNNs. More notably, GNNs are vulnerable to\\nprivacy attacks, such as membership inference attacks, even if only blackbox\\naccess to the trained model is granted. To build defenses, differential privacy\\nhas emerged as a mechanism to disguise the sensitive data in training datasets.\\nFollowing the strategy of Private Aggregation of Teacher Ensembles (PATE),\\nrecent methods leverage a large ensemble of teacher models. These teachers are\\ntrained on disjoint subsets of private data and are employed to transfer\\nknowledge to a student model, which is then released with privacy guarantees.\\nHowever, splitting graph data into many disjoint training sets may destroy the\\nstructural information and adversely affect accuracy. We propose a new\\ngraph-specific scheme of releasing a student GNN, which avoids splitting\\nprivate training data altogether. The student GNN is trained using public data,\\npartly labeled privately using the teacher GNN models trained exclusively for\\neach query node. We theoretically analyze our approach in the R\\`{e}nyi\\ndifferential privacy framework and provide privacy guarantees. Besides, we show\\nthe solid experimental performance of our method compared to several baselines,\\nincluding the PATE baseline adapted for graph-structured data. Our anonymized\\ncode is available.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['cs.LG']</td>\n",
              "      <td>Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification</td>\n",
              "      <td>Machine learning solutions for pattern classification problems are nowadays\\nwidely deployed in society and industry. However, the lack of transparency and\\naccountability of most accurate models often hinders their safe use. Thus,\\nthere is a clear need for developing explainable artificial intelligence\\nmechanisms. There exist model-agnostic methods that summarize feature\\ncontributions, but their interpretability is limited to predictions made by\\nblack-box models. An open challenge is to develop models that have intrinsic\\ninterpretability and produce their own explanations, even for classes of models\\nthat are traditionally considered black boxes like (recurrent) neural networks.\\nIn this paper, we propose a Long-Term Cognitive Network for interpretable\\npattern classification of structured data. Our method brings its own mechanism\\nfor providing explanations by quantifying the relevance of each feature in the\\ndecision process. For supporting the interpretability without affecting the\\nperformance, the model incorporates more flexibility through a quasi-nonlinear\\nreasoning rule that allows controlling nonlinearity. Besides, we propose a\\nrecurrence-aware decision model that evades the issues posed by unique fixed\\npoints while introducing a deterministic learning method to compute the tunable\\nparameters. The simulations show that our interpretable model obtains\\ncompetitive results when compared to the state-of-the-art white and black-box\\nmodels.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80dbe7d7-7249-48a1-b13c-c8f7e802b9ac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80dbe7d7-7249-48a1-b13c-c8f7e802b9ac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80dbe7d7-7249-48a1-b13c-c8f7e802b9ac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d0d0e1ba-eead-46b5-9f0b-ac4c5ed04a71\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0d0e1ba-eead-46b5-9f0b-ac4c5ed04a71')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d0d0e1ba-eead-46b5-9f0b-ac4c5ed04a71 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "arxiv_data",
              "summary": "{\n  \"name\": \"arxiv_data\",\n  \"rows\": 56181,\n  \"fields\": [\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3402,\n        \"samples\": [\n          \"['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']\",\n          \"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ME', 'stat.ML']\",\n          \"['stat.ML', 'cs.LG', 'physics.chem-ph', 'physics.data-an']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41105,\n        \"samples\": [\n          \"Semi-supervised Federated Learning for Activity Recognition\",\n          \"SATR-DL: Improving Surgical Skill Assessment and Task Recognition in Robot-assisted Surgery with Deep Neural Networks\",\n          \"A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstracts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41115,\n        \"samples\": [\n          \"Counterfactual inference has become a ubiquitous tool in online\\nadvertisement, recommendation systems, medical diagnosis, and econometrics.\\nAccurate modeling of outcome distributions associated with different\\ninterventions -- known as counterfactual distributions -- is crucial for the\\nsuccess of these applications. In this work, we propose to model counterfactual\\ndistributions using a novel Hilbert space representation called counterfactual\\nmean embedding (CME). The CME embeds the associated counterfactual distribution\\ninto a reproducing kernel Hilbert space (RKHS) endowed with a positive definite\\nkernel, which allows us to perform causal inference over the entire landscape\\nof the counterfactual distribution. Based on this representation, we propose a\\ndistributional treatment effect (DTE) that can quantify the causal effect over\\nentire outcome distributions. Our approach is nonparametric as the CME can be\\nestimated under the unconfoundedness assumption from observational data without\\nrequiring any parametric assumption about the underlying distributions. We also\\nestablish a rate of convergence of the proposed estimator which depends on the\\nsmoothness of the conditional mean and the Radon-Nikodym derivative of the\\nunderlying marginal distributions. Furthermore, our framework allows for more\\ncomplex outcomes such as images, sequences, and graphs. Our experimental\\nresults on synthetic data and off-policy evaluation tasks demonstrate the\\nadvantages of the proposed estimator.\",\n          \"Current multi-person localisation and tracking systems have an over reliance\\non the use of appearance models for target re-identification and almost no\\napproaches employ a complete deep learning solution for both objectives. We\\npresent a novel, complete deep learning framework for multi-person localisation\\nand tracking. In this context we first introduce a light weight sequential\\nGenerative Adversarial Network architecture for person localisation, which\\novercomes issues related to occlusions and noisy detections, typically found in\\na multi person environment. In the proposed tracking framework we build upon\\nrecent advances in pedestrian trajectory prediction approaches and propose a\\nnovel data association scheme based on predicted trajectories. This removes the\\nneed for computationally expensive person re-identification systems based on\\nappearance features and generates human like trajectories with minimal\\nfragmentation. The proposed method is evaluated on multiple public benchmarks\\nincluding both static and dynamic cameras and is capable of generating\\noutstanding performance, especially among other recently proposed deep neural\\nnetwork based approaches.\",\n          \"Unifying text detection and text recognition in an end-to-end training\\nfashion has become a new trend for reading text in the wild, as these two tasks\\nare highly relevant and complementary. In this paper, we investigate the\\nproblem of scene text spotting, which aims at simultaneous text detection and\\nrecognition in natural images. An end-to-end trainable neural network named as\\nMask TextSpotter is presented. Different from the previous text spotters that\\nfollow the pipeline consisting of a proposal generation network and a\\nsequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and\\nsmooth end-to-end learning procedure, in which both detection and recognition\\ncan be achieved directly from two-dimensional space via semantic segmentation.\\nFurther, a spatial attention module is proposed to enhance the performance and\\nuniversality. Benefiting from the proposed two-dimensional representation on\\nboth detection and recognition, it easily handles text instances of irregular\\nshapes, for instance, curved text. We evaluate it on four English datasets and\\none multi-language dataset, achieving consistently superior performance over\\nstate-of-the-art methods in both detection and end-to-end text recognition\\ntasks. Moreover, we further investigate the recognition module of our method\\nseparately, which significantly outperforms state-of-the-art methods on both\\nregular and irregular text datasets for scene text recognition.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "arxiv_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796e8c0b",
      "metadata": {
        "id": "796e8c0b"
      },
      "source": [
        "# Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "0f6ce7b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f6ce7b0",
        "outputId": "ad8e2c05-a95a-455e-a9ae-f7663696c747"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(56181, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "arxiv_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "9dd86c28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dd86c28",
        "outputId": "b9112ee2-e5f0-4cb6-ffcf-ebe28c17a729"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "terms        0\n",
              "titles       0\n",
              "abstracts    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "arxiv_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "efcdf227",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efcdf227",
        "outputId": "c91ae2f4-fce3-44c0-91f2-9df73ec1bc30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15054"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "arxiv_data.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_data[\"terms\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_pz-XvGXzSr",
        "outputId": "5c8a39ef-0a2a-4ebc-f7b0-7943060e26ab"
      },
      "id": "C_pz-XvGXzSr",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                          ['cs.LG']\n",
              "1                                 ['cs.LG', 'cs.AI']\n",
              "2                      ['cs.LG', 'cs.CR', 'stat.ML']\n",
              "3                                 ['cs.LG', 'cs.CR']\n",
              "4                                          ['cs.LG']\n",
              "                            ...                     \n",
              "56176                             ['cs.CV', 'cs.IR']\n",
              "56177    ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']\n",
              "56178                                      ['cs.LG']\n",
              "56179                ['stat.ML', 'cs.LG', 'math.OC']\n",
              "56180                  ['cs.LG', 'cs.AI', 'stat.ML']\n",
              "Name: terms, Length: 56181, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "c14beaba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c14beaba",
        "outputId": "95eeb68b-ad9d-4151-c2fc-76881157c0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels : ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
            " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
            "lenght : 1177\n"
          ]
        }
      ],
      "source": [
        "# getting unique labels\n",
        "labels_column = arxiv_data[\"terms\"].apply(literal_eval)\n",
        "labels = labels_column.explode().unique()\n",
        "print(\"labels :\",labels)\n",
        "print(\"lenght :\",len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "d52c4165",
      "metadata": {
        "id": "d52c4165",
        "outputId": "709a9125-4d96-4aa3-84e1-ac678e811bc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 41105 rows in the deduplicated dataset.\n",
            "2503\n",
            "3401\n"
          ]
        }
      ],
      "source": [
        "# remove duplicate entries based on the \"titles\" (terms) column\n",
        "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
        "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
        "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
        "# There are some terms with occurrence as low as 1.\n",
        "print(sum(arxiv_data['terms'].value_counts()==1))\n",
        "# how many unique terms\n",
        "print(arxiv_data['terms'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "5883bdf4",
      "metadata": {
        "id": "5883bdf4",
        "outputId": "6cc2d378-61c4-42fd-e479-a27c73e252ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38602, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
        "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
        "arxiv_data_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e35864",
      "metadata": {
        "id": "a8e35864",
        "outputId": "5c77abe2-cee5-48c9-a2ee-379af1ecfec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
              "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
        "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
        "arxiv_data_filtered['terms'].values[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ba3861",
      "metadata": {
        "id": "22ba3861"
      },
      "source": [
        "# train and test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "fd52b83c",
      "metadata": {
        "id": "fd52b83c",
        "outputId": "1058ad54-8bf3-47b2-90d9-56168aff3b3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in training set: 34741\n",
            "Number of rows in validation set: 1930\n",
            "Number of rows in test set: 1931\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35432           ['cs.LG', 'stat.ML']\n",
              "33434                      ['cs.CV']\n",
              "9844                       ['cs.CV']\n",
              "16392                      ['cs.CV']\n",
              "16742             ['cs.LG', 'cs.NE']\n",
              "                    ...             \n",
              "13083                      ['cs.CV']\n",
              "25330    ['cs.CV', 'cs.CL', 'cs.LG']\n",
              "35579                      ['cs.CV']\n",
              "39230                      ['cs.CV']\n",
              "29105                      ['cs.CV']\n",
              "Name: terms, Length: 34741, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "test_split = 0.1\n",
        "\n",
        "# Initial train and test split.\n",
        "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
        "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
        "\n",
        "# Splitting the test set further into validation\n",
        "# and new test sets.\n",
        "val_df = test_df.sample(frac=0.5)\n",
        "test_df.drop(val_df.index, inplace=True)\n",
        "\n",
        "print(f\"Number of rows in training set: {len(train_df)}\")\n",
        "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
        "print(f\"Number of rows in test set: {len(test_df)}\")\n",
        "\n",
        "train_df[\"terms\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "ee848e0d",
      "metadata": {
        "id": "ee848e0d",
        "outputId": "96313400-d086-46cd-d1f2-43a55726b7ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "\n",
            "['14J60 (Primary) 14F05, 14J26 (Secondary)' '60L10, 60L20' '62H30' '62H35'\n",
            " '62H99' '65D19' '68' '68Q32' '68T01' '68T05' '68T07' '68T10' '68T30'\n",
            " '68T45' '68T99' '68Txx' '68U01' '68U10'\n",
            " 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3' 'F.2.2; I.2.7' 'G.3'\n",
            " 'H.3.1; H.3.3; I.2.6; I.2.7' 'H.3.1; I.2.6; I.2.7' 'I.2' 'I.2.0; I.2.6'\n",
            " 'I.2.1' 'I.2.10' 'I.2.10; I.2.6' 'I.2.10; I.4.8' 'I.2.10; I.4.8; I.5.4'\n",
            " 'I.2.10; I.4; I.5' 'I.2.10; I.5.1; I.4.8' 'I.2.1; J.3' 'I.2.6'\n",
            " 'I.2.6, I.5.4' 'I.2.6; I.2.10' 'I.2.6; I.2.7'\n",
            " 'I.2.6; I.2.7; H.3.1; H.3.3' 'I.2.6; I.2.8' 'I.2.6; I.2.9' 'I.2.6; I.5.1'\n",
            " 'I.2.6; I.5.4' 'I.2.7' 'I.2.8' 'I.2; I.2.6; I.2.7' 'I.2; I.4; I.5'\n",
            " 'I.2; I.5' 'I.2; J.2' 'I.3.7' 'I.4' 'I.4.0' 'I.4.1' 'I.4.3' 'I.4.4'\n",
            " 'I.4.5' 'I.4.6' 'I.4.6; I.4.8' 'I.4.8' 'I.4.9' 'I.4.9; I.5.4' 'I.4; I.5'\n",
            " 'I.5.2' 'I.5.4' 'K.3.2' 'astro-ph.IM' 'cond-mat.dis-nn'\n",
            " 'cond-mat.mtrl-sci' 'cond-mat.soft' 'cond-mat.stat-mech' 'cs.AI' 'cs.AR'\n",
            " 'cs.CC' 'cs.CE' 'cs.CG' 'cs.CL' 'cs.CR' 'cs.CV' 'cs.CY' 'cs.DB' 'cs.DC'\n",
            " 'cs.DL' 'cs.DM' 'cs.DS' 'cs.ET' 'cs.FL' 'cs.GR' 'cs.GT' 'cs.HC' 'cs.IR'\n",
            " 'cs.IT' 'cs.LG' 'cs.LO' 'cs.MA' 'cs.MM' 'cs.MS' 'cs.NA' 'cs.NE' 'cs.NI'\n",
            " 'cs.PF' 'cs.PL' 'cs.RO' 'cs.SC' 'cs.SD' 'cs.SE' 'cs.SI' 'cs.SY' 'econ.EM'\n",
            " 'econ.GN' 'eess.AS' 'eess.IV' 'eess.SP' 'eess.SY' 'hep-ex' 'hep-ph'\n",
            " 'math-ph' 'math.AP' 'math.AT' 'math.CO' 'math.DS' 'math.FA' 'math.IT'\n",
            " 'math.LO' 'math.MP' 'math.NA' 'math.OC' 'math.PR' 'math.SP' 'math.ST'\n",
            " 'nlin.AO' 'nlin.CD' 'physics.ao-ph' 'physics.app-ph' 'physics.bio-ph'\n",
            " 'physics.chem-ph' 'physics.class-ph' 'physics.comp-ph' 'physics.data-an'\n",
            " 'physics.flu-dyn' 'physics.geo-ph' 'physics.med-ph' 'physics.optics'\n",
            " 'physics.plasm-ph' 'physics.soc-ph' 'q-bio.BM' 'q-bio.GN' 'q-bio.MN'\n",
            " 'q-bio.NC' 'q-bio.OT' 'q-bio.PE' 'q-bio.QM' 'q-bio.TO' 'q-fin.CP'\n",
            " 'q-fin.EC' 'q-fin.GN' 'q-fin.PM' 'q-fin.RM' 'q-fin.ST' 'q-fin.TR'\n",
            " 'quant-ph' 'stat.AP' 'stat.CO' 'stat.ME' 'stat.ML' 'stat.TH']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Convert the terms column to a list of lists\n",
        "terms_list = train_df['terms'].apply(lambda x: eval(x)).tolist()\n",
        "\n",
        "# Initialize MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Fit MultiLabelBinarizer on the terms list to build the vocabulary\n",
        "mlb.fit(terms_list)\n",
        "\n",
        "# Transform the terms list into a multi-hot encoded representation\n",
        "multi_hot_encoded = mlb.transform(terms_list)\n",
        "\n",
        "# Get the vocabulary (terms)\n",
        "vocab = mlb.classes_\n",
        "\n",
        "print(\"Vocabulary:\\n\")\n",
        "print(vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "bd32a8c9",
      "metadata": {
        "id": "bd32a8c9",
        "outputId": "8970240f-6a6b-44cb-ca90-9e759cfad97c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original label: ['cs.CV']\n",
            "Label-binarized representation: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# Choose a sample label\n",
        "sample_label = train_df[\"terms\"].iloc[1]\n",
        "print(f\"Original label: {sample_label}\")\n",
        "\n",
        "# Convert the sample label to a list of lists, similar to what was done before\n",
        "sample_label_list = [eval(sample_label)]\n",
        "\n",
        "# Binarize the sample label using the MultiLabelBinarizer\n",
        "label_binarized = mlb.transform(sample_label_list)\n",
        "print(f\"Label-binarized representation: {label_binarized}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "120e17de",
      "metadata": {
        "id": "120e17de"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the maximum sequence length and batch size\n",
        "max_seqlen = 150\n",
        "batch_size = 128\n",
        "\n",
        "# Define the function to create the dataset\n",
        "def make_dataset(dataframe, is_train):\n",
        "    # Convert the terms column to a list of lists\n",
        "    terms_list = dataframe['terms'].apply(lambda x: eval(x)).tolist()\n",
        "\n",
        "    # Initialize MultiLabelBinarizer\n",
        "    mlb = MultiLabelBinarizer()\n",
        "\n",
        "    # Fit MultiLabelBinarizer on the terms list to build the vocabulary\n",
        "    mlb.fit(terms_list)\n",
        "\n",
        "    # Transform the terms list into a multi-hot encoded representation\n",
        "    label_binarized = mlb.transform(terms_list)\n",
        "\n",
        "    # Convert the abstracts column to a TensorFlow dataset\n",
        "    abstracts_dataset = tf.data.Dataset.from_tensor_slices(dataframe['abstracts'].values)\n",
        "\n",
        "    # Create a dataset of labels using the binarized representation\n",
        "    labels_dataset = tf.data.Dataset.from_tensor_slices(label_binarized)\n",
        "\n",
        "    # Zip the abstracts and labels datasets together to create a single dataset of tuples\n",
        "    dataset = tf.data.Dataset.zip((abstracts_dataset, labels_dataset))\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    dataset = dataset.shuffle(buffer_size=len(dataframe)) if is_train else dataset\n",
        "\n",
        "    return dataset.batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "6c36ee52",
      "metadata": {
        "id": "6c36ee52"
      },
      "outputs": [],
      "source": [
        "train_dataset = make_dataset(train_df, is_train=True)\n",
        "validation_dataset = make_dataset(val_df, is_train=False)\n",
        "test_dataset = make_dataset(test_df, is_train=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for abstracts, labels in train_dataset.take(1):\n",
        "    print(\"Abstracts:\", abstracts)\n",
        "    print(\"Labels:\", labels)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHMaM9a_gp-V",
        "outputId": "2e24c716-5582-443b-c18e-242114aa96eb"
      },
      "id": "nHMaM9a_gp-V",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstracts: tf.Tensor(\n",
            "[b\"We present a protocol to construct your own depth validation dataset for\\nnavigation. This protocol, called RDC for Rigid Depth Constructor, aims at\\nbeing more accessible and cheaper than already existing techniques, requiring\\nonly a camera and a Lidar sensor to get started. We also develop a test suite\\nto get insightful information from the evaluated algorithm. Finally, we take\\nthe example of UAV videos, on which we test two depth algorithms that were\\ninitially tested on KITTI and show that the drone context is dramatically\\ndifferent from in-car videos. This shows that a single context benchmark should\\nnot be considered reliable, and when developing a depth estimation algorithm,\\none should benchmark it on a dataset that best fits one's particular needs,\\nwhich often means creating a brand new one. Along with this paper we provide\\nthe tool with an open source implementation and plan to make it as\\nuser-friendly as possible, to make depth dataset creation possible even for\\nsmall teams. Our key contributions are the following: We propose a complete,\\nopen-source and almost fully automatic software application for creating\\nvalidation datasets with densely annotated depth, adaptable to a wide variety\\nof image, video and range data. It includes selection tools to adapt the\\ndataset to specific validation needs, and conversion tools to other dataset\\nformats. Using this application, we propose two new real datasets, outdoor and\\nindoor, readily usable in UAV navigation context. Finally as examples, we show\\nan evaluation of two depth prediction algorithms, using a collection of\\ncomprehensive (e.g. distribution based) metrics.\"\n",
            " b'Applications in virtual and augmented reality create a demand for rapid\\ncreation and easy access to large sets of 3D models. An effective way to\\naddress this demand is to edit or deform existing 3D models based on a\\nreference, e.g., a 2D image which is very easy to acquire. Given such a source\\n3D model and a target which can be a 2D image, 3D model, or a point cloud\\nacquired as a depth scan, we introduce 3DN, an end-to-end network that deforms\\nthe source model to resemble the target. Our method infers per-vertex offset\\ndisplacements while keeping the mesh connectivity of the source model fixed. We\\npresent a training strategy which uses a novel differentiable operation, mesh\\nsampling operator, to generalize our method across source and target models\\nwith varying mesh densities. Mesh sampling operator can be seamlessly\\nintegrated into the network to handle meshes with different topologies.\\nQualitative and quantitative results show that our method generates higher\\nquality results compared to the state-of-the art learning-based methods for 3D\\nshape generation. Code is available at github.com/laughtervv/3DN.'\n",
            " b'We propose a novel neural network architecture for visual saliency\\ndetections, which utilizes neurophysiologically plausible mechanisms for\\nextraction of salient regions. The model has been significantly inspired by\\nrecent findings from neurophysiology and aimed to simulate the bottom-up\\nprocesses of human selective attention. Two types of features were analyzed:\\ncolor and direction of maximum variance. The mechanism we employ for processing\\nthose features is PCA, implemented by means of normalized Hebbian learning and\\nthe waves of spikes. To evaluate performance of our model we have conducted\\npsychological experiment. Comparison of simulation results with those of\\nexperiment indicates good performance of our model.'\n",
            " b'In the problem of domain transfer learning, we learn a model for the\\npredic-tion in a target domain from the data of both some source domains and\\nthe target domain, where the target domain is in lack of labels while the\\nsource domain has sufficient labels. Besides the instances of the data,\\nrecently the attributes of data shared across domains are also explored and\\nproven to be very helpful to leverage the information of different domains. In\\nthis paper, we propose a novel learning framework for domain-transfer learning\\nbased on both instances and attributes. We proposed to embed the attributes of\\ndif-ferent domains by a shared convolutional neural network (CNN), learn a\\ndomain-independent CNN model to represent the information shared by dif-ferent\\ndomains by matching across domains, and a domain-specific CNN model to\\nrepresent the information of each domain. The concatenation of the three CNN\\nmodel outputs is used to predict the class label. An iterative algo-rithm based\\non gradient descent method is developed to learn the parameters of the model.\\nThe experiments over benchmark datasets show the advantage of the proposed\\nmodel.'\n",
            " b'Classes in natural images tend to follow long tail distributions. This is\\nproblematic when there are insufficient training examples for rare classes.\\nThis effect is emphasized in compound classes, involving the conjunction of\\nseveral concepts, such as those appearing in action-recognition datasets. In\\nthis paper, we propose to address this issue by learning how to utilize common\\nvisual concepts which are readily available. We detect the presence of\\nprominent concepts in images and use them to infer the target labels instead of\\nusing visual features directly, combining tools from vision and\\nnatural-language processing. We validate our method on the recently introduced\\nHICO dataset reaching a mAP of 31.54\\\\% and on the Stanford-40 Actions dataset,\\nwhere the proposed method outperforms that obtained by direct visual features,\\nobtaining an accuracy 83.12\\\\%. Moreover, the method provides for each class a\\nsemantically meaningful list of keywords and relevant image regions relating it\\nto its constituent concepts.'\n",
            " b\"We present SKD, a novel keypoint detector that uses saliency to determine the\\nbest candidates from a point cloud for tasks such as registration and\\nreconstruction. The approach can be applied to any differentiable deep learning\\ndescriptor by using the gradients of that descriptor with respect to the 3D\\nposition of the input points as a measure of their saliency. The saliency is\\ncombined with the original descriptor and context information in a neural\\nnetwork, which is trained to learn robust keypoint candidates. The key\\nintuition behind this approach is that keypoints are not extracted solely as a\\nresult of the geometry surrounding a point, but also take into account the\\ndescriptor's response. The approach was evaluated on two large LIDAR datasets -\\nthe Oxford RobotCar dataset and the KITTI dataset, where we obtain up to 50%\\nimprovement over the state-of-the-art in both matchability and repeatability.\\nWhen performing sparse matching with the keypoints computed by our method we\\nachieve a higher inlier ratio and faster convergence.\"\n",
            " b'In both the fields of computer science and medicine there is very strong\\ninterest in developing personalized treatment policies for patients who have\\nvariable responses to treatments. In particular, I aim to find an optimal\\npersonalized treatment policy which is a non-deterministic function of the\\npatient specific covariate data that maximizes the expected survival time or\\nclinical outcome. I developed an algorithmic framework to solve multistage\\ndecision problem with a varying number of stages that are subject to censoring\\nin which the \"rewards\" are expected survival times. In specific, I developed a\\nnovel Q-learning algorithm that dynamically adjusts for these parameters.\\nFurthermore, I found finite upper bounds on the generalized error of the\\ntreatment paths constructed by this algorithm. I have also shown that when the\\noptimal Q-function is an element of the approximation space, the anticipated\\nsurvival times for the treatment regime constructed by the algorithm will\\nconverge to the optimal treatment path. I demonstrated the performance of the\\nproposed algorithmic framework via simulation studies and through the analysis\\nof chronic depression data and a hypothetical clinical trial. The censored\\nQ-learning algorithm I developed is more effective than the state of the art\\nclinical decision support systems and is able to operate in environments when\\nmany covariate parameters may be unobtainable or censored.'\n",
            " b'Understanding the content of videos is one of the core techniques for\\ndeveloping various helpful applications in the real world, such as recognizing\\nvarious human actions for surveillance systems or customer behavior analysis in\\nan autonomous shop. However, understanding the content or story of the video\\nstill remains a challenging problem due to its sheer amount of data and\\ntemporal structure. In this paper, we propose a multi-channel neural network\\nstructure that adopts a two-stream network structure, which has been shown high\\nperformance in human action recognition field, and use it as a spatiotemporal\\nvideo feature extractor for solving video question and answering task. We also\\nadopt a squeeze-and-excitation structure to two-stream network structure for\\nachieving a channel-wise attended spatiotemporal feature. For jointly modeling\\nthe spatiotemporal features from video and the textual features from the\\nquestion, we design a context matching module with a level adjusting layer to\\nremove the gap of information between visual and textual features by applying\\nattention mechanism on joint modeling. Finally, we adopt a scoring mechanism\\nand smoothed ranking loss objective function for selecting the correct answer\\nfrom answer candidates. We evaluate our model with TVQA dataset, and our\\napproach shows the improved result in textual only setting, but the result with\\nvisual feature shows the limitation and possibility of our approach.'\n",
            " b'Deep Gaussian processes (DGPs) have struggled for relevance in applications\\ndue to the challenges and cost associated with Bayesian inference. In this\\npaper we propose a sparse variational approximation for DGPs for which the\\napproximate posterior mean has the same mathematical structure as a Deep Neural\\nNetwork (DNN). We make the forward pass through a DGP equivalent to a ReLU DNN\\nby finding an interdomain transformation that represents the GP posterior mean\\nas a sum of ReLU basis functions. This unification enables the initialisation\\nand training of the DGP as a neural network, leveraging the well established\\npractice in the deep learning community, and so greatly aiding the inference\\ntask. The experiments demonstrate improved accuracy and faster training\\ncompared to current DGP methods, while retaining favourable predictive\\nuncertainties.'\n",
            " b'We present OctNet, a representation for deep learning with sparse 3D data. In\\ncontrast to existing models, our representation enables 3D convolutional\\nnetworks which are both deep and high resolution. Towards this goal, we exploit\\nthe sparsity in the input data to hierarchically partition the space using a\\nset of unbalanced octrees where each leaf node stores a pooled feature\\nrepresentation. This allows to focus memory allocation and computation to the\\nrelevant dense regions and enables deeper networks without compromising\\nresolution. We demonstrate the utility of our OctNet representation by\\nanalyzing the impact of resolution on several 3D tasks including 3D object\\nclassification, orientation estimation and point cloud labeling.'\n",
            " b'Heterogeneous face recognition (HFR) refers to matching face images acquired\\nfrom different domains with wide applications in security scenarios. This paper\\npresents a deep neural network approach namely Multi-Margin based Decorrelation\\nLearning (MMDL) to extract decorrelation representations in a hyperspherical\\nspace for cross-domain face images. The proposed framework can be divided into\\ntwo components: heterogeneous representation network and decorrelation\\nrepresentation learning. First, we employ a large scale of accessible visual\\nface images to train heterogeneous representation network. The decorrelation\\nlayer projects the output of the first component into decorrelation latent\\nsubspace and obtains decorrelation representation. In addition, we design a\\nmulti-margin loss (MML), which consists of quadruplet margin loss (QML) and\\nheterogeneous angular margin loss (HAML), to constrain the proposed framework.\\nExperimental results on two challenging heterogeneous face databases show that\\nour approach achieves superior performance on both verification and recognition\\ntasks, comparing with state-of-the-art methods.'\n",
            " b'Deep Learning models are getting more and more popular but constraints on\\nexplainability, adversarial robustness and fairness are often major concerns\\nfor production deployment. Although the open source ecosystem is abundant on\\naddressing those concerns, fully integrated, end to end systems are lacking in\\nopen source. Therefore we provide an entirely open source, reusable component\\nframework, visual editor and execution engine for production grade machine\\nlearning on top of Kubernetes, a joint effort between IBM and the University\\nHospital Basel. It uses Kubeflow Pipelines, the AI Explainability360 toolkit,\\nthe AI Fairness360 toolkit and the Adversarial Robustness Toolkit on top of\\nElyraAI, Kubeflow, Kubernetes and JupyterLab. Using the Elyra pipeline editor,\\nAI pipelines can be developed visually with a set of jupyter notebooks.'\n",
            " b'The ability to automatically learn task specific feature representations has\\nled to a huge success of deep learning methods. When large training data is\\nscarce, such as in medical imaging problems, transfer learning has been very\\neffective. In this paper, we systematically investigate the process of\\ntransferring a Convolutional Neural Network, trained on ImageNet images to\\nperform image classification, to kidney detection problem in ultrasound images.\\nWe study how the detection performance depends on the extent of transfer. We\\nshow that a transferred and tuned CNN can outperform a state-of-the-art feature\\nengineered pipeline and a hybridization of these two techniques achieves 20\\\\%\\nhigher performance. We also investigate how the evolution of intermediate\\nresponse images from our network. Finally, we compare these responses to\\nstate-of-the-art image processing filters in order to gain greater insight into\\nhow transfer learning is able to effectively manage widely varying imaging\\nregimes.'\n",
            " b'The effectiveness of Convolutional Neural Networks (CNNs) has been\\nsubstantially attributed to their built-in property of translation\\nequivariance. However, CNNs do not have embedded mechanisms to handle other\\ntypes of transformations. In this work, we pay attention to scale changes,\\nwhich regularly appear in various tasks due to the changing distances between\\nthe objects and the camera. First, we introduce the general theory for building\\nscale-equivariant convolutional networks with steerable filters. We develop\\nscale-convolution and generalize other common blocks to be scale-equivariant.\\nWe demonstrate the computational efficiency and numerical stability of the\\nproposed method. We compare the proposed models to the previously developed\\nmethods for scale equivariance and local scale invariance. We demonstrate\\nstate-of-the-art results on MNIST-scale dataset and on STL-10 dataset in the\\nsupervised learning setting.'\n",
            " b'Segmentation of structural parts of 3D models of plants is an important step\\nfor plant phenotyping, especially for monitoring architectural and\\nmorphological traits. This work introduces a benchmark for assessing the\\nperformance of 3D point-based deep learning methods on organ segmentation of 3D\\nplant models, specifically rosebush models. Six recent deep learning\\narchitectures that segment 3D point clouds into semantic parts were adapted and\\ncompared. The methods were tested on the ROSE-X data set, containing fully\\nannotated 3D models of real rosebush plants. The contribution of incorporating\\nsynthetic 3D models generated through Lindenmayer systems into training data\\nwas also investigated.'\n",
            " b'Autonomous driving applications use two types of sensor systems to identify\\nvehicles - depth sensing LiDAR and radiance sensing cameras. We compare the\\nperformance (average precision) of a ResNet for vehicle detection in complex,\\ndaytime, driving scenes when the input is a depth map (D = d(x,y)), a radiance\\nimage (L = r(x,y)), or both [D,L]. (1) When the spatial sampling resolution of\\nthe depth map and radiance image are equal to typical camera resolutions, a\\nResNet detects vehicles at higher average precision from depth than radiance.\\n(2) As the spatial sampling of the depth map declines to the range of current\\nLiDAR devices, the ResNet average precision is higher for radiance than depth.\\n(3) For a hybrid system that combines a depth map and radiance image, the\\naverage precision is higher than using depth or radiance alone. We established\\nthese observations in simulation and then confirmed them using realworld data.\\nThe advantage of combining depth and radiance can be explained by noting that\\nthe two type of information have complementary weaknesses. The radiance data\\nare limited by dynamic range and motion blur. The LiDAR data have relatively\\nlow spatial resolution. The ResNet combines the two data sources effectively to\\nimprove overall vehicle detection.'\n",
            " b'Active Inference is a theory of action arising from neuroscience which casts\\naction and planning as a bayesian inference problem to be solved by minimizing\\na single quantity - the variational free energy. Active Inference promises a\\nunifying account of action and perception coupled with a biologically plausible\\nprocess theory. Despite these potential advantages, current implementations of\\nActive Inference can only handle small, discrete policy and state-spaces and\\ntypically require the environmental dynamics to be known. In this paper we\\npropose a novel deep Active Inference algorithm which approximates key\\ndensities using deep neural networks as flexible function approximators, which\\nenables Active Inference to scale to significantly larger and more complex\\ntasks. We demonstrate our approach on a suite of OpenAIGym benchmark tasks and\\nobtain performance comparable with common reinforcement learning baselines.\\nMoreover, our algorithm shows similarities with maximum entropy reinforcement\\nlearning and the policy gradients algorithm, which reveals interesting\\nconnections between the Active Inference framework and reinforcement learning.'\n",
            " b'We present our latest experiment results of object recognition from 3D point\\ncloud data collected through moving car.'\n",
            " b'When labeled data is scarce for a specific target task, transfer learning\\noften offers an effective solution by utilizing data from a related source\\ntask. However, when transferring knowledge from a less related source, it may\\ninversely hurt the target performance, a phenomenon known as negative transfer.\\nDespite its pervasiveness, negative transfer is usually described in an\\ninformal manner, lacking rigorous definition, careful analysis, or systematic\\ntreatment. This paper proposes a formal definition of negative transfer and\\nanalyzes three important aspects thereof. Stemming from this analysis, a novel\\ntechnique is proposed to circumvent negative transfer by filtering out\\nunrelated source data. Based on adversarial networks, the technique is highly\\ngeneric and can be applied to a wide range of transfer learning algorithms. The\\nproposed approach is evaluated on six state-of-the-art deep transfer methods\\nvia experiments on four benchmark datasets with varying levels of difficulty.\\nEmpirically, the proposed method consistently improves the performance of all\\nbaseline methods and largely avoids negative transfer, even when the source\\ndata is degenerate.'\n",
            " b'Multi-domain image-to-image translation with conditional Generative\\nAdversarial Networks (GANs) can generate highly photo realistic images with\\ndesired target classes, yet these synthetic images have not always been helpful\\nto improve downstream supervised tasks such as image classification. Improving\\ndownstream tasks with synthetic examples requires generating images with high\\nfidelity to the unknown conditional distribution of the target class, which\\nmany labeled conditional GANs attempt to achieve by adding soft-max\\ncross-entropy loss based auxiliary classifier in the discriminator. As recent\\nstudies suggest that the soft-max loss in Euclidean space of deep feature does\\nnot leverage their intrinsic angular distribution, we propose to replace this\\nloss in auxiliary classifier with an additive angular margin (AAM) loss that\\ntakes benefit of the intrinsic angular distribution, and promotes intra-class\\ncompactness and inter-class separation to help generator synthesize high\\nfidelity images.\\n  We validate our method on RaFD and CIFAR-100, two challenging face expression\\nand natural image classification data set. Our method outperforms\\nstate-of-the-art methods in several different evaluation criteria including\\nrecently proposed GAN-train and GAN-test metrics designed to assess the impact\\nof synthetic data on downstream classification task, assessing the usefulness\\nin data augmentation for supervised tasks with prediction accuracy score and\\naverage confidence score, and the well known FID metric.'\n",
            " b'Many modern object detectors demonstrate outstanding performances by using\\nthe mechanism of looking and thinking twice. In this paper, we explore this\\nmechanism in the backbone design for object detection. At the macro level, we\\npropose Recursive Feature Pyramid, which incorporates extra feedback\\nconnections from Feature Pyramid Networks into the bottom-up backbone layers.\\nAt the micro level, we propose Switchable Atrous Convolution, which convolves\\nthe features with different atrous rates and gathers the results using switch\\nfunctions. Combining them results in DetectoRS, which significantly improves\\nthe performances of object detection. On COCO test-dev, DetectoRS achieves\\nstate-of-the-art 55.7% box AP for object detection, 48.5% mask AP for instance\\nsegmentation, and 50.0% PQ for panoptic segmentation. The code is made publicly\\navailable.'\n",
            " b'In this paper, we study the problem of recognizing compositional\\nattribute-object concepts within the zero-shot learning (ZSL) framework. We\\npropose an episode-based cross-attention (EpiCA) network which combines merits\\nof cross-attention mechanism and episode-based training strategy to recognize\\nnovel compositional concepts. Firstly, EpiCA bases on cross-attention to\\ncorrelate concept-visual information and utilizes the gated pooling layer to\\nbuild contextualized representations for both images and concepts. The updated\\nrepresentations are used for a more in-depth multi-modal relevance calculation\\nfor concept recognition. Secondly, a two-phase episode training strategy,\\nespecially the transductive phase, is adopted to utilize unlabeled test\\nexamples to alleviate the low-resource learning problem. Experiments on two\\nwidely-used zero-shot compositional learning (ZSCL) benchmarks have\\ndemonstrated the effectiveness of the model compared with recent approaches on\\nboth conventional and generalized ZSCL settings.'\n",
            " b\"Learning to adapt and make real-time informed decisions in a dynamic and\\ncomplex environment is a challenging problem. Monopoly is a popular strategic\\nboard game that requires players to make multiple decisions during the game.\\nDecision-making in Monopoly involves many real-world elements such as\\nstrategizing, luck, and modeling of opponent's policies. In this paper, we\\npresent novel representations for the state and action space for the full\\nversion of Monopoly and define an improved reward function. Using these, we\\nshow that our deep reinforcement learning agent can learn winning strategies\\nfor Monopoly against different fixed-policy agents. In Monopoly, players can\\ntake multiple actions even if it is not their turn to roll the dice. Some of\\nthese actions occur more frequently than others, resulting in a skewed\\ndistribution that adversely affects the performance of the learning agent. To\\ntackle the non-uniform distribution of actions, we propose a hybrid approach\\nthat combines deep reinforcement learning (for frequent but complex decisions)\\nwith a fixed policy approach (for infrequent but straightforward decisions).\\nExperimental results show that our hybrid agent outperforms a standard deep\\nreinforcement learning agent by 30% in the number of games won against\\nfixed-policy agents.\"\n",
            " b'In recent years the importance of finding a meaningful pattern from huge\\ndatasets has become more challenging. Data miners try to adopt innovative\\nmethods to face this problem by applying feature selection methods. In this\\npaper we propose a new hybrid method in which we use a combination of\\nresampling, filtering the sample domain and wrapper subset evaluation method\\nwith genetic search to reduce dimensions of Lung-Cancer dataset that we\\nreceived from UCI Repository of Machine Learning databases. Finally, we apply\\nsome well- known classification algorithms (Na\\\\\"ive Bayes, Logistic, Multilayer\\nPerceptron, Best First Decision Tree and JRIP) to the resulting dataset and\\ncompare the results and prediction rates before and after the application of\\nour feature selection method on that dataset. The results show a substantial\\nprogress in the average performance of five classification algorithms\\nsimultaneously and the classification error for these classifiers decreases\\nconsiderably. The experiments also show that this method outperforms other\\nfeature selection methods with a lower cost.'\n",
            " b'Event cameras are biologically-inspired sensors that gather the temporal\\nevolution of the scene. They capture pixel-wise brightness variations and\\noutput a corresponding stream of asynchronous events. Despite having multiple\\nadvantages with respect to traditional cameras, their use is partially\\nprevented by the limited applicability of traditional data processing and\\nvision algorithms. To this aim, we present a framework which exploits the\\noutput stream of event cameras to synthesize RGB frames, relying on an initial\\nor a periodic set of color key-frames and the sequence of intermediate events.\\nDifferently from existing work, we propose a deep learning-based frame\\nsynthesis method, consisting of an adversarial architecture combined with a\\nrecurrent module. Qualitative results and quantitative per-pixel, perceptual,\\nand semantic evaluation on four public datasets confirm the quality of the\\nsynthesized images.'\n",
            " b'Learning sentence vectors from an unlabeled corpus has attracted attention\\nbecause such vectors can represent sentences in a lower dimensional and\\ncontinuous space. Simple heuristics using pre-trained word vectors are widely\\napplied to machine learning tasks. However, they are not well understood from a\\ntheoretical perspective. We analyze learning sentence vectors from a transfer\\nlearning perspective by using a PAC-Bayes bound that enables us to understand\\nexisting heuristics. We show that simple heuristics such as averaging and\\ninverse document frequency weighted averaging are derived by our formulation.\\nMoreover, we propose novel sentence vector learning algorithms on the basis of\\nour PAC-Bayes analysis.'\n",
            " b'Generating novel pairs of image and text is a problem that combines computer\\nvision and natural language processing. In this paper, we present strategies\\nfor generating novel image and caption pairs based on existing captioning\\ndatasets. The model takes advantage of recent advances in generative\\nadversarial networks and sequence-to-sequence modeling. We make generalizations\\nto generate paired samples from multiple domains. Furthermore, we study cycles\\n-- generating from image to text then back to image and vise versa, as well as\\nits connection with autoencoders.'\n",
            " b'The success of convolutional neural networks (CNNs) in various applications\\nis accompanied by a significant increase in computation and parameter storage\\ncosts. Recent efforts to reduce these overheads involve pruning and compressing\\nthe weights of various layers while at the same time aiming to not sacrifice\\nperformance. In this paper, we propose a novel criterion for CNN pruning\\ninspired by neural network interpretability: The most relevant units, i.e.\\nweights or filters, are automatically found using their relevance scores\\nobtained from concepts of explainable AI (XAI). By exploring this idea, we\\nconnect the lines of interpretability and model compression research. We show\\nthat our proposed method can efficiently prune CNN models in transfer-learning\\nsetups in which networks pre-trained on large corpora are adapted to\\nspecialized tasks. The method is evaluated on a broad range of computer vision\\ndatasets. Notably, our novel criterion is not only competitive or better\\ncompared to state-of-the-art pruning criteria when successive retraining is\\nperformed, but clearly outperforms these previous criteria in the\\nresource-constrained application scenario in which the data of the task to be\\ntransferred to is very scarce and one chooses to refrain from fine-tuning. Our\\nmethod is able to compress the model iteratively while maintaining or even\\nimproving accuracy. At the same time, it has a computational cost in the order\\nof gradient computation and is comparatively simple to apply without the need\\nfor tuning hyperparameters for pruning.'\n",
            " b'Real-world applications of machine learning tools in high-stakes domains are\\noften regulated to be fair, in the sense that the predicted target should\\nsatisfy some quantitative notion of parity with respect to a protected\\nattribute. However, the exact tradeoff between fairness and accuracy with a\\nreal-valued target is not clear. In this paper, we characterize the inherent\\ntradeoff between statistical parity and accuracy in the regression setting by\\nproviding a lower bound on the error of any fair regressor. Our lower bound is\\nsharp, algorithm-independent, and admits a simple interpretation: when the\\nmoments of the target differ between groups, any fair algorithm has to make a\\nlarge error on at least one of the groups. We further extend this result to\\ngive a lower bound on the joint error of any (approximately) fair algorithm,\\nusing the Wasserstein distance to measure the quality of the approximation. On\\nthe upside, we establish the first connection between individual fairness,\\naccuracy parity, and the Wasserstein distance by showing that if a regressor is\\nindividually fair, it also approximately verifies the accuracy parity, where\\nthe gap is given by the Wasserstein distance between the two groups. Inspired\\nby our theoretical results, we develop a practical algorithm for fair\\nregression through the lens of representation learning, and conduct experiments\\non a real-world dataset to corroborate our findings.'\n",
            " b'Bayesian Networks (BNs) have become increasingly popular over the last few\\ndecades as a tool for reasoning under uncertainty in fields as diverse as\\nmedicine, biology, epidemiology, economics and the social sciences. This is\\nespecially true in real-world areas where we seek to answer complex questions\\nbased on hypothetical evidence to determine actions for intervention. However,\\ndetermining the graphical structure of a BN remains a major challenge,\\nespecially when modelling a problem under causal assumptions. Solutions to this\\nproblem include the automated discovery of BN graphs from data, constructing\\nthem based on expert knowledge, or a combination of the two. This paper\\nprovides a comprehensive review of combinatoric algorithms proposed for\\nlearning BN structure from data, describing 61 algorithms including\\nprototypical, well-established and state-of-the-art approaches. The basic\\napproach of each algorithm is described in consistent terms, and the\\nsimilarities and differences between them highlighted. Methods of evaluating\\nalgorithms and their comparative performance are discussed including the\\nconsistency of claims made in the literature. Approaches for dealing with data\\nnoise in real-world datasets and incorporating expert knowledge into the\\nlearning process are also covered.'\n",
            " b'Reinforcement learning is a powerful learning paradigm in which agents can\\nlearn to maximize sparse and delayed reward signals. Although RL has had many\\nimpressive successes in complex domains, learning can take hours, days, or even\\nyears of training data. A major challenge of contemporary RL research is to\\ndiscover how to learn with less data. Previous work has shown that domain\\ninformation can be successfully used to shape the reward; by adding additional\\nreward information, the agent can learn with much less data. Furthermore, if\\nthe reward is constructed from a potential function, the optimal policy is\\nguaranteed to be unaltered. While such potential-based reward shaping (PBRS)\\nholds promise, it is limited by the need for a well-defined potential function.\\nIdeally, we would like to be able to take arbitrary advice from a human or\\nother agent and improve performance without affecting the optimal policy. The\\nrecently introduced dynamic potential based advice (DPBA) method tackles this\\nchallenge by admitting arbitrary advice from a human or other agent and\\nimproves performance without affecting the optimal policy. The main\\ncontribution of this paper is to expose, theoretically and empirically, a flaw\\nin DPBA. Alternatively, to achieve the ideal goals, we present a simple method\\ncalled policy invariant explicit shaping (PIES) and show theoretically and\\nempirically that PIES succeeds where DPBA fails.'\n",
            " b'Recent years, transfer learning has attracted much attention in the community\\nof machine learning. In this paper, we mainly focus on the tasks of parameter\\ntransfer under the framework of extreme learning machine (ELM). Unlike the\\nexisting parameter transfer approaches, which incorporate the source model\\ninformation into the target by regularizing the di erence between the source\\nand target domain parameters, an intuitively appealing projective-model is\\nproposed to bridge the source and target model parameters. Specifically, we\\nformulate the parameter transfer in the ELM networks by the means of parameter\\nprojection, and train the model by optimizing the projection matrix and\\nclassifier parameters jointly. Further more, the `L2,1-norm structured sparsity\\npenalty is imposed on the source domain parameters, which encourages the joint\\nfeature selection and parameter transfer. To evaluate the e ectiveness of the\\nproposed method, comprehensive experiments on several commonly used domain\\nadaptation datasets are presented. The results show that the proposed method\\nsignificantly outperforms the non-transfer ELM networks and other classical\\ntransfer learning methods.'\n",
            " b\"Deep neural networks have been shown to be fooled rather easily using\\nadversarial attack algorithms. Practical methods such as adversarial patches\\nhave been shown to be extremely effective in causing misclassification.\\nHowever, these patches are highlighted using standard network interpretation\\nalgorithms, thus revealing the identity of the adversary. We show that it is\\npossible to create adversarial patches which not only fool the prediction, but\\nalso change what we interpret regarding the cause of the prediction. Moreover,\\nwe introduce our attack as a controlled setting to measure the accuracy of\\ninterpretation algorithms. We show this using extensive experiments for\\nGrad-CAM interpretation that transfers to occluding patch interpretation as\\nwell. We believe our algorithms can facilitate developing more robust network\\ninterpretation tools that truly explain the network's underlying decision\\nmaking process.\"\n",
            " b\"A significant challenge in object detection is accurate identification of an\\nobject's position in image space, whereas one algorithm with one set of\\nparameters is usually not enough, and the fusion of multiple algorithms and/or\\nparameters can lead to more robust results. Herein, a new computational\\nintelligence fusion approach based on the dynamic analysis of agreement among\\nobject detection outputs is proposed. Furthermore, we propose an online versus\\njust in training image augmentation strategy. Experiments comparing the results\\nboth with and without fusion are presented. We demonstrate that the augmented\\nand fused combination results are the best, with respect to higher accuracy\\nrates and reduction of outlier influences. The approach is demonstrated in the\\ncontext of cone, pedestrian and box detection for Advanced Driver Assistance\\nSystems (ADAS) applications.\"\n",
            " b'Graph embedding methods transform high-dimensional and complex graph contents\\ninto low-dimensional representations. They are useful for a wide range of graph\\nanalysis tasks including link prediction, node classification, recommendation\\nand visualization. Most existing approaches represent graph nodes as point\\nvectors in a low-dimensional embedding space, ignoring the uncertainty present\\nin the real-world graphs. Furthermore, many real-world graphs are large-scale\\nand rich in content (e.g. node attributes). In this work, we propose GLACE, a\\nnovel, scalable graph embedding method that preserves both graph structure and\\nnode attributes effectively and efficiently in an end-to-end manner. GLACE\\neffectively models uncertainty through Gaussian embeddings, and supports\\ninductive inference of new nodes based on their attributes. In our\\ncomprehensive experiments, we evaluate GLACE on real-world graphs, and the\\nresults demonstrate that GLACE significantly outperforms state-of-the-art\\nembedding methods on multiple graph analysis tasks.'\n",
            " b'Mutual information maximization provides an appealing formalism for learning\\nrepresentations of data. In the context of reinforcement learning (RL), such\\nrepresentations can accelerate learning by discarding irrelevant and redundant\\ninformation, while retaining the information necessary for control. Much of the\\nprior work on these methods has addressed the practical difficulties of\\nestimating mutual information from samples of high-dimensional observations,\\nwhile comparatively less is understood about which mutual information\\nobjectives yield representations that are sufficient for RL from a theoretical\\nperspective. In this paper, we formalize the sufficiency of a state\\nrepresentation for learning and representing the optimal policy, and study\\nseveral popular mutual-information based objectives through this lens.\\nSurprisingly, we find that two of these objectives can yield insufficient\\nrepresentations given mild and common assumptions on the structure of the MDP.\\nWe corroborate our theoretical results with empirical experiments on a\\nsimulated game environment with visual observations.'\n",
            " b'Objective: This work addresses two key problems of skin lesion\\nclassification. The first problem is the effective use of high-resolution\\nimages with pretrained standard architectures for image classification. The\\nsecond problem is the high class imbalance encountered in real-world\\nmulti-class datasets. Methods: To use high-resolution images, we propose a\\nnovel patch-based attention architecture that provides global context between\\nsmall, high-resolution patches. We modify three pretrained architectures and\\nstudy the performance of patch-based attention. To counter class imbalance\\nproblems, we compare oversampling, balanced batch sampling, and class-specific\\nloss weighting. Additionally, we propose a novel diagnosis-guided loss\\nweighting method which takes the method used for ground-truth annotation into\\naccount. Results: Our patch-based attention mechanism outperforms previous\\nmethods and improves the mean sensitivity by 7%. Class balancing significantly\\nimproves the mean sensitivity and we show that our diagnosis-guided loss\\nweighting method improves the mean sensitivity by 3% over normal loss\\nbalancing. Conclusion: The novel patch-based attention mechanism can be\\nintegrated into pretrained architectures and provides global context between\\nlocal patches while outperforming other patch-based methods. Hence, pretrained\\narchitectures can be readily used with high-resolution images without\\ndownsampling. The new diagnosis-guided loss weighting method outperforms other\\nmethods and allows for effective training when facing class imbalance.\\nSignificance: The proposed methods improve automatic skin lesion\\nclassification. They can be extended to other clinical applications where\\nhigh-resolution image data and class imbalance are relevant.'\n",
            " b'Transformers have shown impressive performance in various natural language\\nprocessing and computer vision tasks, due to the capability of modeling\\nlong-range dependencies. Recent progress has demonstrated to combine such\\ntransformers with CNN-based semantic image segmentation models is very\\npromising. However, it is not well studied yet on how well a pure transformer\\nbased approach can achieve for image segmentation. In this work, we explore a\\nnovel framework for semantic image segmentation, which is encoder-decoder based\\nFully Transformer Networks (FTN). Specifically, we first propose a Pyramid\\nGroup Transformer (PGT) as the encoder for progressively learning hierarchical\\nfeatures, while reducing the computation complexity of the standard visual\\ntransformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fuse\\nsemantic-level and spatial-level information from multiple levels of the PGT\\nencoder for semantic image segmentation. Surprisingly, this simple baseline can\\nachieve new state-of-the-art results on multiple challenging semantic\\nsegmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. The\\nsource code will be released upon the publication of this work.'\n",
            " b'Grouping has been commonly used in deep metric learning for computing diverse\\nfeatures. However, current methods are prone to overfitting and lack\\ninterpretability. In this work, we propose an improved and interpretable\\ngrouping method to be integrated flexibly with any metric learning framework.\\nOur method is based on the attention mechanism with a learnable query for each\\ngroup. The query is fully trainable and can capture group-specific information\\nwhen combined with the diversity loss. An appealing property of our method is\\nthat it naturally lends itself interpretability. The attention scores between\\nthe learnable query and each spatial position can be interpreted as the\\nimportance of that position. We formally show that our proposed grouping method\\nis invariant to spatial permutations of features. When used as a module in\\nconvolutional neural networks, our method leads to translational invariance. We\\nconduct comprehensive experiments to evaluate our method. Our quantitative\\nresults indicate that the proposed method outperforms prior methods\\nconsistently and significantly across different datasets, evaluation metrics,\\nbase models, and loss functions. For the first time to the best of our\\nknowledge, our interpretation results clearly demonstrate that the proposed\\nmethod enables the learning of distinct and diverse features across groups. The\\ncode is available on https://github.com/XinyiXuXD/DGML-master.'\n",
            " b'In this work, we address the problem of multi-domain image-to-image\\ntranslation with particular attention paid to computational cost. In\\nparticular, current state of the art models require a large and deep model in\\norder to handle the visual diversity of multiple domains. In a context of\\nlimited computational resources, increasing the network size may not be\\npossible. Therefore, we propose to increase the network capacity by using an\\nadaptive graph structure. At inference time, the network estimates its own\\ngraph by selecting specific sub-networks. Sub-network selection is implemented\\nusing Gumbel-Softmax in order to allow end-to-end training. This approach leads\\nto an adjustable increase in number of parameters while preserving an almost\\nconstant computational cost. Our evaluation on two publicly available datasets\\nof facial and painting images shows that our adaptive strategy generates better\\nimages with fewer artifacts than literature methods'\n",
            " b'Recent progress has shown that few-shot learning can be improved with access\\nto unlabelled data, known as semi-supervised few-shot learning(SS-FSL). We\\nintroduce an SS-FSL approach, dubbed as Prototypical Random Walk\\nNetworks(PRWN), built on top of Prototypical Networks (PN). We develop a random\\nwalk semi-supervised loss that enables the network to learn representations\\nthat are compact and well-separated. Our work is related to the very recent\\ndevelopment of graph-based approaches for few-shot learning. However, we show\\nthat compact and well-separated class representations can be achieved by\\nmodeling our prototypical random walk notion without needing additional\\ngraph-NN parameters or requiring a transductive setting where a collective test\\nset is provided. Our model outperforms baselines in most benchmarks with\\nsignificant improvements in some cases. Our model, trained with 40$\\\\%$ of the\\ndata as labeled, compares competitively against fully supervised prototypical\\nnetworks, trained on 100$\\\\%$ of the labels, even outperforming it in the 1-shot\\nmini-Imagenet case with 50.89$\\\\%$ to 49.4$\\\\%$ accuracy. We also show that our\\nloss is resistant to distractors, unlabeled data that does not belong to any of\\nthe training classes, and hence reflecting robustness to labeled/unlabeled\\nclass distribution mismatch. Associated GitHub page can be found at\\nhttps://prototypical-random-walk.github.io.'\n",
            " b'This paper aims to accelerate the test-time computation of convolutional\\nneural networks (CNNs), especially very deep CNNs that have substantially\\nimpacted the computer vision community. Unlike previous methods that are\\ndesigned for approximating linear filters or linear responses, our method takes\\nthe nonlinear units into account. We develop an effective solution to the\\nresulting nonlinear optimization problem without the need of stochastic\\ngradient descent (SGD). More importantly, while previous methods mainly focus\\non optimizing one or two layers, our nonlinear method enables an asymmetric\\nreconstruction that reduces the rapidly accumulated error when multiple (e.g.,\\n>=10) layers are approximated. For the widely used very deep VGG-16 model, our\\nmethod achieves a whole-model speedup of 4x with merely a 0.3% increase of\\ntop-5 error in ImageNet classification. Our 4x accelerated VGG-16 model also\\nshows a graceful accuracy degradation for object detection when plugged into\\nthe Fast R-CNN detector.'\n",
            " b'Since their introduction a year ago, distributional approaches to\\nreinforcement learning (distributional RL) have produced strong results\\nrelative to the standard approach which models expected values (expected RL).\\nHowever, aside from convergence guarantees, there have been few theoretical\\nresults investigating the reasons behind the improvements distributional RL\\nprovides. In this paper we begin the investigation into this fundamental\\nquestion by analyzing the differences in the tabular, linear approximation, and\\nnon-linear approximation settings. We prove that in many realizations of the\\ntabular and linear approximation settings, distributional RL behaves exactly\\nthe same as expected RL. In cases where the two methods behave differently,\\ndistributional RL can in fact hurt performance when it does not induce\\nidentical behaviour. We then continue with an empirical analysis comparing\\ndistributional and expected RL methods in control settings with non-linear\\napproximators to tease apart where the improvements from distributional RL\\nmethods are coming from.'\n",
            " b'Single image super-resolution task has witnessed great strides with the\\ndevelopment of deep learning. However, most existing studies focus on building\\na more complex neural network with a massive number of layers, bringing heavy\\ncomputational cost and memory storage. Recently, as Transformer yields\\nbrilliant results in NLP tasks, more and more researchers start to explore the\\napplication of Transformer in computer vision tasks. But with the heavy\\ncomputational cost and high GPU memory occupation of the vision Transformer,\\nthe network can not be designed too deep. To address this problem, we propose a\\nnovel Efficient Super-Resolution Transformer (ESRT) for fast and accurate image\\nsuper-resolution. ESRT is a hybrid Transformer where a CNN-based SR network is\\nfirst designed in the front to extract deep features. Specifically, there are\\ntwo backbones for formatting the ESRT: lightweight CNN backbone (LCB) and\\nlightweight Transformer backbone (LTB). Among them, LCB is a lightweight SR\\nnetwork to extract deep SR features at a low computational cost by dynamically\\nadjusting the size of the feature map. LTB is made up of an efficient\\nTransformer (ET) with a small GPU memory occupation, which benefited from the\\nnovel efficient multi-head attention (EMHA). In EMHA, a feature split module\\n(FSM) is proposed to split the long sequence into sub-segments and then these\\nsub-segments are applied by attention operation. This module can significantly\\ndecrease the GPU memory occupation. Extensive experiments show that our ESRT\\nachieves competitive results. Compared with the original Transformer which\\noccupies 16057M GPU memory, the proposed ET only occupies 4191M GPU memory with\\nbetter performance.'\n",
            " b'This paper studies learning the representations of whole graphs in both\\nunsupervised and semi-supervised scenarios. Graph-level representations are\\ncritical in a variety of real-world applications such as predicting the\\nproperties of molecules and community analysis in social networks. Traditional\\ngraph kernel based methods are simple, yet effective for obtaining fixed-length\\nrepresentations for graphs but they suffer from poor generalization due to\\nhand-crafted designs. There are also some recent methods based on language\\nmodels (e.g. graph2vec) but they tend to only consider certain substructures\\n(e.g. subtrees) as graph representatives. Inspired by recent progress of\\nunsupervised representation learning, in this paper we proposed a novel method\\ncalled InfoGraph for learning graph-level representations. We maximize the\\nmutual information between the graph-level representation and the\\nrepresentations of substructures of different scales (e.g., nodes, edges,\\ntriangles). By doing so, the graph-level representations encode aspects of the\\ndata that are shared across different scales of substructures. Furthermore, we\\nfurther propose InfoGraph*, an extension of InfoGraph for semi-supervised\\nscenarios. InfoGraph* maximizes the mutual information between unsupervised\\ngraph representations learned by InfoGraph and the representations learned by\\nexisting supervised methods. As a result, the supervised encoder learns from\\nunlabeled data while preserving the latent semantic space favored by the\\ncurrent supervised task. Experimental results on the tasks of graph\\nclassification and molecular property prediction show that InfoGraph is\\nsuperior to state-of-the-art baselines and InfoGraph* can achieve performance\\ncompetitive with state-of-the-art semi-supervised models.'\n",
            " b'Numerous methods for crafting adversarial examples were proposed recently\\nwith high success rate. Since most existing machine learning based classifiers\\nnormalize images into some continuous, real vector, domain firstly, attacks\\noften craft adversarial examples in such domain. However, \"adversarial\"\\nexamples may become benign after denormalizing them back into the discrete\\ninteger domain, known as the discretization problem. This problem was mentioned\\nin some work, but has received relatively little attention.\\n  In this work, we first conduct a comprehensive study of existing methods and\\ntools for crafting. We theoretically analyze 34 representative methods and\\nempirically study 20 representative open source tools for crafting adversarial\\nimages. Our study reveals that the discretization problem is far more serious\\nthan originally thought. This suggests that the discretization problem should\\nbe taken into account seriously when crafting adversarial examples and\\nmeasuring attack success rate. As a first step towards addressing this problem\\nin black-box scenario, we propose a black-box method which reduces the\\nadversarial example searching problem to a derivative-free optimization\\nproblem. Our method is able to craft adversarial images by derivative-free\\nsearch in the discrete integer domain. Experimental results show that our\\nmethod is comparable to recent white-box methods (e.g., FGSM, BIM and C\\\\&W) and\\nachieves significantly higher success rate in terms of adversarial examples in\\nthe discrete integer domain than recent black-box methods (e.g., ZOO, NES-PGD\\nand Bandits). Moreover, our method is able to handle models that is\\nnon-differentiable and successfully break the winner of NIPS 2017 competition\\non defense with 95\\\\% success rate. Our results suggest that discrete\\noptimization algorithms open up a promising area of research into effective\\nblack-box attacks.'\n",
            " b'Convolutional neural networks (CNNs) have achieved state-of-the-art results\\non many visual recognition tasks. However, current CNN models still exhibit a\\npoor ability to be invariant to spatial transformations of images. Intuitively,\\nwith sufficient layers and parameters, hierarchical combinations of convolution\\n(matrix multiplication and non-linear activation) and pooling operations should\\nbe able to learn a robust mapping from transformed input images to\\ntransform-invariant representations. In this paper, we propose randomly\\ntransforming (rotation, scale, and translation) feature maps of CNNs during the\\ntraining stage. This prevents complex dependencies of specific rotation, scale,\\nand translation levels of training images in CNN models. Rather, each\\nconvolutional kernel learns to detect a feature that is generally helpful for\\nproducing the transform-invariant answer given the combinatorially large\\nvariety of transform levels of its input feature maps. In this way, we do not\\nrequire any extra training supervision or modification to the optimization\\nprocess and training images. We show that random transformation provides\\nsignificant improvements of CNNs on many benchmark tasks, including small-scale\\nimage recognition, large-scale image recognition, and image retrieval. The code\\nis available at https://github.com/jasonustc/caffe-multigpu/tree/TICNN.'\n",
            " b'The goal of few-shot classification is to classify new categories with few\\nlabeled examples within each class. Nowadays, the excellent performance in\\nhandling few-shot classification problems is shown by metric-based\\nmeta-learning methods. However, it is very hard for previous methods to\\ndiscriminate the fine-grained sub-categories in the embedding space without\\nfine-grained labels. This may lead to unsatisfactory generalization to\\nfine-grained subcategories, and thus affects model interpretation. To tackle\\nthis problem, we introduce the contrastive loss into few-shot classification\\nfor learning latent fine-grained structure in the embedding space. Furthermore,\\nto overcome the drawbacks of random image transformation used in current\\ncontrastive learning in producing noisy and inaccurate image pairs (i.e.,\\nviews), we develop a learning-to-learn algorithm to automatically generate\\ndifferent views of the same image. Extensive experiments on standard few-shot\\nlearning benchmarks demonstrate the superiority of our method.'\n",
            " b'Learning with sparse rewards remains a significant challenge in reinforcement\\nlearning (RL), especially when the aim is to train a policy capable of\\nachieving multiple different goals. To date, the most successful approaches for\\ndealing with multi-goal, sparse reward environments have been model-free RL\\nalgorithms. In this work we propose PlanGAN, a model-based algorithm\\nspecifically designed for solving multi-goal tasks in environments with sparse\\nrewards. Our method builds on the fact that any trajectory of experience\\ncollected by an agent contains useful information about how to achieve the\\ngoals observed during that trajectory. We use this to train an ensemble of\\nconditional generative models (GANs) to generate plausible trajectories that\\nlead the agent from its current state towards a specified goal. We then combine\\nthese imagined trajectories into a novel planning algorithm in order to achieve\\nthe desired goal as efficiently as possible. The performance of PlanGAN has\\nbeen tested on a number of robotic navigation/manipulation tasks in comparison\\nwith a range of model-free reinforcement learning baselines, including\\nHindsight Experience Replay. Our studies indicate that PlanGAN can achieve\\ncomparable performance whilst being around 4-8 times more sample efficient.'\n",
            " b\"Decision trees and randomized forests are widely used in computer vision and\\nmachine learning. Standard algorithms for decision tree induction optimize the\\nsplit functions one node at a time according to some splitting criteria. This\\ngreedy procedure often leads to suboptimal trees. In this paper, we present an\\nalgorithm for optimizing the split functions at all levels of the tree jointly\\nwith the leaf parameters, based on a global objective. We show that the problem\\nof finding optimal linear-combination (oblique) splits for decision trees is\\nrelated to structured prediction with latent variables, and we formulate a\\nconvex-concave upper bound on the tree's empirical loss. The run-time of\\ncomputing the gradient of the proposed surrogate objective with respect to each\\ntraining exemplar is quadratic in the the tree depth, and thus training deep\\ntrees is feasible. The use of stochastic gradient descent for optimization\\nenables effective training with large datasets. Experiments on several\\nclassification benchmarks demonstrate that the resulting non-greedy decision\\ntrees outperform greedy decision tree baselines.\"\n",
            " b'Currently, instance segmentation is attracting more and more attention in\\nmachine learning region. However, there exists some defects on the information\\npropagation in previous Mask R-CNN and other network models. In this paper, we\\npropose supervised adaptive threshold network for instance segmentation.\\nSpecifically, we adopt the Mask R-CNN method based on adaptive threshold, and\\nby establishing a layered adaptive network structure, it performs adaptive\\nbinarization on the probability graph generated by Mask RCNN to obtain better\\nsegmentation effect and reduce the error rate. At the same time, an adaptive\\nfeature pool is designed to make the transmission between different layers of\\nthe network more accurate and effective, reduce the loss in the process of\\nfeature transmission, and further improve the mask method. Experiments on\\nbenchmark data sets indicate that the effectiveness of the proposed model'\n",
            " b'Many computer vision problems can be formulated as binary quadratic programs\\n(BQPs). Two classic relaxation methods are widely used for solving BQPs,\\nnamely, spectral methods and semidefinite programming (SDP), each with their\\nown advantages and disadvantages. Spectral relaxation is simple and easy to\\nimplement, but its bound is loose. Semidefinite relaxation has a tighter bound,\\nbut its computational complexity is high for large scale problems. We present a\\nnew SDP formulation for BQPs, with two desirable properties. First, it has a\\nsimilar relaxation bound to conventional SDP formulations. Second, compared\\nwith conventional SDP methods, the new SDP formulation leads to a significantly\\nmore efficient and scalable dual optimization approach, which has the same\\ndegree of complexity as spectral methods. Extensive experiments on various\\napplications including clustering, image segmentation, co-segmentation and\\nregistration demonstrate the usefulness of our SDP formulation for solving\\nlarge-scale BQPs.'\n",
            " b'We propose and address a novel few-shot RL problem, where a task is\\ncharacterized by a subtask graph which describes a set of subtasks and their\\ndependencies that are unknown to the agent. The agent needs to quickly adapt to\\nthe task over few episodes during adaptation phase to maximize the return in\\nthe test phase. Instead of directly learning a meta-policy, we develop a\\nMeta-learner with Subtask Graph Inference(MSGI), which infers the latent\\nparameter of the task by interacting with the environment and maximizes the\\nreturn given the latent parameter. To facilitate learning, we adopt an\\nintrinsic reward inspired by upper confidence bound (UCB) that encourages\\nefficient exploration. Our experiment results on two grid-world domains and\\nStarCraft II environments show that the proposed method is able to accurately\\ninfer the latent task parameter, and to adapt more efficiently than existing\\nmeta RL and hierarchical RL methods.'\n",
            " b'Generative adversarial networks (GANs) are successful deep generative models.\\nGANs are based on a two-player minimax game. However, the objective function\\nderived in the original motivation is changed to obtain stronger gradients when\\nlearning the generator. We propose a novel algorithm that repeats the density\\nratio estimation and f-divergence minimization. Our algorithm offers a new\\nperspective toward the understanding of GANs and is able to make use of\\nmultiple viewpoints obtained in the research of density ratio estimation, e.g.\\nwhat divergence is stable and relative density ratio is useful.'\n",
            " b'In this paper, we observe that most false positive images (i.e., different\\nidentities with query images) in the top ranking list usually have the similar\\ncolor information with the query image in person re-identification (Re-ID).\\nMeanwhile, when we use the greyscale images generated from RGB images to\\nconduct the person Re-ID task, some hard query images can obtain better\\nperformance compared with using RGB images. Therefore, RGB and greyscale images\\nseem to be complementary to each other for person Re-ID. In this paper, we aim\\nto utilize both RGB and greyscale images to improve the person Re-ID\\nperformance. To this end, we propose a novel two-stream deep neural network\\nwith RGB-grey information, which can effectively fuse RGB and greyscale feature\\nrepresentations to enhance the generalization ability of Re-ID. Firstly, we\\nconvert RGB images to greyscale images in each training batch. Based on these\\nRGB and greyscale images, we train the RGB and greyscale branches,\\nrespectively. Secondly, to build up connections between RGB and greyscale\\nbranches, we merge the RGB and greyscale branches into a new joint branch.\\nFinally, we concatenate the features of all three branches as the final feature\\nrepresentation for Re-ID. Moreover, in the training process, we adopt the joint\\nlearning scheme to simultaneously train each branch by the independent loss\\nfunction, which can enhance the generalization ability of each branch. Besides,\\na global loss function is utilized to further fine-tune the final concatenated\\nfeature. The extensive experiments on multiple benchmark datasets fully show\\nthat the proposed method can outperform the state-of-the-art person Re-ID\\nmethods. Furthermore, using greyscale images can indeed improve the person\\nRe-ID performance.'\n",
            " b\"Existing deep learning-based approaches for monocular 3D object detection in\\nautonomous driving often model the object as a rotated 3D cuboid while the\\nobject's geometric shape has been ignored. In this work, we propose an approach\\nfor incorporating the shape-aware 2D/3D constraints into the 3D detection\\nframework. Specifically, we employ the deep neural network to learn\\ndistinguished 2D keypoints in the 2D image domain and regress their\\ncorresponding 3D coordinates in the local 3D object coordinate first. Then the\\n2D/3D geometric constraints are built by these correspondences for each object\\nto boost the detection performance. For generating the ground truth of 2D/3D\\nkeypoints, an automatic model-fitting approach has been proposed by fitting the\\ndeformed 3D object model and the object mask in the 2D image. The proposed\\nframework has been verified on the public KITTI dataset and the experimental\\nresults demonstrate that by using additional geometrical constraints the\\ndetection performance has been significantly improved as compared to the\\nbaseline method. More importantly, the proposed framework achieves\\nstate-of-the-art performance with real time. Data and code will be available at\\nhttps://github.com/zongdai/AutoShape\"\n",
            " b'Model inference for dynamical systems aims to estimate the future behaviour\\nof a system from observations. Purely model-free statistical methods, such as\\nArtificial Neural Networks, tend to perform poorly for such tasks. They are\\ntherefore not well suited to many questions from applications, for example in\\nBayesian filtering and reliability estimation.\\n  This work introduces a parametric polynomial kernel method that can be used\\nfor inferring the future behaviour of Ordinary Differential Equation models,\\nincluding chaotic dynamical systems, from observations. Using numerical\\nintegration techniques, parametric representations of Ordinary Differential\\nEquations can be learnt using Backpropagation and Stochastic Gradient Descent.\\nThe polynomial technique presented here is based on a nonparametric method,\\nkernel ridge regression. However, the time complexity of nonparametric kernel\\nridge regression scales cubically with the number of training data points. Our\\nparametric polynomial method avoids this manifestation of the curse of\\ndimensionality, which becomes particularly relevant when working with large\\ntime series data sets.\\n  Two numerical demonstrations are presented. First, a simple regression test\\ncase is used to illustrate the method and to compare the performance with\\nstandard Artificial Neural Network techniques. Second, a more substantial test\\ncase is the inference of a chaotic spatio-temporal dynamical system, the\\nLorenz--Emanuel system, from observations. Our method was able to successfully\\ntrack the future behaviour of the system over time periods much larger than the\\ntraining data sampling rate. Finally, some limitations of the method are\\npresented, as well as proposed directions for future work to mitigate these\\nlimitations.'\n",
            " b'Clustering performs an essential role in many real world applications, such\\nas market research, pattern recognition, data analysis, and image processing.\\nHowever, due to the high dimensionality of the input feature values, the data\\nbeing fed to clustering algorithms usually contains noise and thus could lead\\nto in-accurate clustering results. While traditional dimension reduction and\\nfeature selection algorithms could be used to address this problem, the simple\\nheuristic rules used in those algorithms are based on some particular\\nassumptions. When those assumptions does not hold, these algorithms then might\\nnot work. In this paper, we propose DAC, Deep Autoencoder-based Clustering, a\\ngeneralized data-driven framework to learn clustering representations using\\ndeep neuron networks. Experiment results show that our approach could\\neffectively boost performance of the K-Means clustering algorithm on a variety\\ntypes of datasets.'\n",
            " b'Matching the performance of conditional Generative Adversarial Networks with\\nlittle supervision is an important task, especially in venturing into new\\ndomains. We design a new training algorithm, which is robust to missing or\\nambiguous labels. The main idea is to intentionally corrupt the labels of\\ngenerated examples to match the statistics of the real data, and have a\\ndiscriminator process the real and generated examples with corrupted labels. We\\nshowcase the robustness of this proposed approach both theoretically and\\nempirically. We show that minimizing the proposed loss is equivalent to\\nminimizing true divergence between real and generated data up to a\\nmultiplicative factor, and characterize this multiplicative factor as a\\nfunction of the statistics of the uncertain labels. Experiments on MNIST\\ndataset demonstrates that proposed architecture is able to achieve high\\naccuracy in generating examples faithful to the class even with only a few\\nexamples per class.'\n",
            " b'The objective of this work is to improve the accuracy of building demand\\nforecasting. This is a more challenging task than grid level forecasting. For\\nthe said purpose, we develop a new technique called recurrent transform\\nlearning (RTL). Two versions are proposed. The first one (RTL) is unsupervised;\\nthis is used as a feature extraction tool that is further fed into a regression\\nmodel. The second formulation embeds regression into the RTL framework leading\\nto regressing recurrent transform learning (R2TL). Forecasting experiments have\\nbeen carried out on three popular publicly available datasets. Both of our\\nproposed techniques yield results superior to the state-of-the-art like long\\nshort term memory network, echo state network and sparse coding regression.'\n",
            " b'For successful semantic editing of real images, it is critical for a GAN\\ninversion method to find an in-domain latent code that aligns with the domain\\nof a pre-trained GAN model. Unfortunately, such in-domain latent codes can be\\nfound only for in-range images that align with the training images of a GAN\\nmodel. In this paper, we propose BDInvert, a novel GAN inversion approach to\\nsemantic editing of out-of-range images that are geometrically unaligned with\\nthe training images of a GAN model. To find a latent code that is semantically\\neditable, BDInvert inverts an input out-of-range image into an alternative\\nlatent space than the original latent space. We also propose a regularized\\ninversion method to find a solution that supports semantic editing in the\\nalternative space. Our experiments show that BDInvert effectively supports\\nsemantic editing of out-of-range images with geometric transformations.'\n",
            " b'Human-object interaction detection is an important and relatively new class\\nof visual relationship detection tasks, essential for deeper scene\\nunderstanding. Most existing approaches decompose the problem into object\\nlocalization and interaction recognition. Despite showing progress, these\\napproaches only rely on the appearances of humans and objects and overlook the\\navailable context information, crucial for capturing subtle interactions\\nbetween them. We propose a contextual attention framework for human-object\\ninteraction detection. Our approach leverages context by learning\\ncontextually-aware appearance features for human and object instances. The\\nproposed attention module then adaptively selects relevant instance-centric\\ncontext information to highlight image regions likely to contain human-object\\ninteractions. Experiments are performed on three benchmarks: V-COCO, HICO-DET\\nand HCVRD. Our approach outperforms the state-of-the-art on all datasets. On\\nthe V-COCO dataset, our method achieves a relative gain of 4.4% in terms of\\nrole mean average precision ($mAP_{role}$), compared to the existing best\\napproach.'\n",
            " b'Reinforcement learning is commonly used with function approximation. However,\\nvery few positive results are known about the convergence of function\\napproximation based RL control algorithms. In this paper we show that TD(0) and\\nSarsa(0) with linear function approximation is convergent for a simple class of\\nproblems, where the system is linear and the costs are quadratic (the LQ\\ncontrol problem). Furthermore, we show that for systems with Gaussian noise and\\nnon-completely observable states (the LQG problem), the mentioned RL algorithms\\nare still convergent, if they are combined with Kalman filtering.'\n",
            " b'Transformers have attracted increasing interests in computer vision, but they\\nstill fall behind state-of-the-art convolutional networks. In this work, we\\nshow that while Transformers tend to have larger model capacity, their\\ngeneralization can be worse than convolutional networks due to the lack of the\\nright inductive bias. To effectively combine the strengths from both\\narchitectures, we present CoAtNets(pronounced \"coat\" nets), a family of hybrid\\nmodels built from two key insights: (1) depthwise Convolution and\\nself-Attention can be naturally unified via simple relative attention; (2)\\nvertically stacking convolution layers and attention layers in a principled way\\nis surprisingly effective in improving generalization, capacity and efficiency.\\nExperiments show that our CoAtNets achieve state-of-the-art performance under\\ndifferent resource constraints across various datasets: Without extra data,\\nCoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M\\nimages from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching\\nViT-huge pre-trained with 300M images from JFT-300M while using 23x less data;\\nNotably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1\\naccuracy on ImageNet, establishing a new state-of-the-art result.'\n",
            " b'The developmental process of embryos follows a monotonic order. An embryo can\\nprogressively cleave from one cell to multiple cells and finally transform to\\nmorula and blastocyst. For time-lapse videos of embryos, most existing\\ndevelopmental stage classification methods conduct per-frame predictions using\\nan image frame at each time step. However, classification using only images\\nsuffers from overlapping between cells and imbalance between stages. Temporal\\ninformation can be valuable in addressing this problem by capturing movements\\nbetween neighboring frames. In this work, we propose a two-stream model for\\ndevelopmental stage classification. Unlike previous methods, our two-stream\\nmodel accepts both temporal and image information. We develop a linear-chain\\nconditional random field (CRF) on top of neural network features extracted from\\nthe temporal and image streams to make use of both modalities. The linear-chain\\nCRF formulation enables tractable training of global sequential models over\\nmultiple frames while also making it possible to inject monotonic development\\norder constraints into the learning process explicitly. We demonstrate our\\nalgorithm on two time-lapse embryo video datasets: i) mouse and ii) human\\nembryo datasets. Our method achieves 98.1 % and 80.6 % for mouse and human\\nembryo stage classification, respectively. Our approach will enable more\\nprofound clinical and biological studies and suggests a new direction for\\ndevelopmental stage classification by utilizing temporal information.'\n",
            " b\"Recently, flow-based methods have achieved promising success in video frame\\ninterpolation. However, electron microscopic (EM) images suffer from unstable\\nimage quality, low PSNR, and disorderly deformation. Existing flow-based\\ninterpolation methods cannot precisely compute optical flow for EM images since\\nonly predicting each position's unique offset. To overcome these problems, we\\npropose a novel interpolation framework for EM images that progressively\\nsynthesizes interpolated features in a coarse-to-fine manner. First, we extract\\nmissing intermediate features by the proposed temporal spatial-adaptive (TSA)\\ninterpolation module. The TSA interpolation module aggregates temporal contexts\\nand then adaptively samples the spatial-related features with the proposed\\nresidual spatial adaptive block. Second, we introduce a stacked deformable\\nrefinement block (SDRB) further enhance the reconstruction quality, which is\\naware of the matching positions and relevant features from input frames with\\nthe feedback mechanism. Experimental results demonstrate the superior\\nperformance of our approach compared to previous works, both quantitatively and\\nqualitatively.\"\n",
            " b'We introduce CuLE (CUDA Learning Environment), a CUDA port of the Atari\\nLearning Environment (ALE) which is used for the development of deep\\nreinforcement algorithms. CuLE overcomes many limitations of existing CPU-based\\nemulators and scales naturally to multiple GPUs. It leverages GPU\\nparallelization to run thousands of games simultaneously and it renders frames\\ndirectly on the GPU, to avoid the bottleneck arising from the limited CPU-GPU\\ncommunication bandwidth. CuLE generates up to 155M frames per hour on a single\\nGPU, a finding previously achieved only through a cluster of CPUs. Beyond\\nhighlighting the differences between CPU and GPU emulators in the context of\\nreinforcement learning, we show how to leverage the high throughput of CuLE by\\neffective batching of the training data, and show accelerated convergence for\\nA2C+V-trace. CuLE is available at https://github.com/NVLabs/cule .'\n",
            " b'Instance segmentation in point clouds is one of the most fine-grained ways to\\nunderstand the 3D scene. Due to its close relationship to semantic\\nsegmentation, many works approach these two tasks simultaneously and leverage\\nthe benefits of multi-task learning. However, most of them only considered\\nsimple strategies such as element-wise feature fusion, which may not lead to\\nmutual promotion. In this work, we build a Bi-Directional Attention module on\\nbackbone neural networks for 3D point cloud perception, which uses similarity\\nmatrix measured from features for one task to help aggregate non-local\\ninformation for the other task, avoiding the potential feature exclusion and\\ntask conflict. From comprehensive experiments and ablation studies on the S3DIS\\ndataset and the PartNet dataset, the superiority of our method is verified.\\nMoreover, the mechanism of how bi-directional attention module helps joint\\ninstance and semantic segmentation is also analyzed.'\n",
            " b'A remarkable characteristic of overparameterized deep neural networks (DNNs)\\nis that their accuracy does not degrade when the network\\'s width is increased.\\nRecent evidence suggests that developing compressible representations is key\\nfor adjusting the complexity of large networks to the learning task at hand.\\nHowever, these compressible representations are poorly understood. A promising\\nstrand of research inspired from biology is understanding representations at\\nthe unit level as it offers a more granular and intuitive interpretation of the\\nneural mechanisms. In order to better understand what facilitates increases in\\nwidth without decreases in accuracy, we ask: Are there mechanisms at the unit\\nlevel by which networks control their effective complexity as their width is\\nincreased? If so, how do these depend on the architecture, dataset, and\\ntraining parameters? We identify two distinct types of \"frivolous\" units that\\nproliferate when the network\\'s width is increased: prunable units which can be\\ndropped out of the network without significant change to the output and\\nredundant units whose activities can be expressed as a linear combination of\\nothers. These units imply complexity constraints as the function the network\\nrepresents could be expressed by a network without them. We also identify how\\nthe development of these units can be influenced by architecture and a number\\nof training factors. Together, these results help to explain why the accuracy\\nof DNNs does not degrade when width is increased and highlight the importance\\nof frivolous units toward understanding implicit regularization in DNNs.'\n",
            " b'This study delves into semi-supervised object detection (SSOD) to improve\\ndetector performance with additional unlabeled data. State-of-the-art SSOD\\nperformance has been achieved recently by self-training, in which training\\nsupervision consists of ground truths and pseudo-labels. In current studies, we\\nobserve that class imbalance in SSOD severely impedes the effectiveness of\\nself-training. To address the class imbalance, we propose adaptive\\nclass-rebalancing self-training (ACRST) with a novel memory module called\\nCropBank. ACRST adaptively rebalances the training data with foreground\\ninstances extracted from the CropBank, thereby alleviating the class imbalance.\\nOwing to the high complexity of detection tasks, we observe that both\\nself-training and data-rebalancing suffer from noisy pseudo-labels in SSOD.\\nTherefore, we propose a novel two-stage filtering algorithm to generate\\naccurate pseudo-labels. Our method achieves satisfactory improvements on\\nMS-COCO and VOC benchmarks. When using only 1\\\\% labeled data in MS-COCO, our\\nmethod achieves 17.02 mAP improvement over supervised baselines, and 5.32 mAP\\nimprovement compared with state-of-the-art methods.'\n",
            " b'Point cloud completion aims to infer the complete geometries for missing\\nregions of 3D objects from incomplete ones. Previous methods usually predict\\nthe complete point cloud based on the global shape representation extracted\\nfrom the incomplete input. However, the global representation often suffers\\nfrom the information loss of structure details on local regions of incomplete\\npoint cloud. To address this problem, we propose Skip-Attention Network\\n(SA-Net) for 3D point cloud completion. Our main contributions lie in the\\nfollowing two-folds. First, we propose a skip-attention mechanism to\\neffectively exploit the local structure details of incomplete point clouds\\nduring the inference of missing parts. The skip-attention mechanism selectively\\nconveys geometric information from the local regions of incomplete point clouds\\nfor the generation of complete ones at different resolutions, where the\\nskip-attention reveals the completion process in an interpretable way. Second,\\nin order to fully utilize the selected geometric information encoded by\\nskip-attention mechanism at different resolutions, we propose a novel\\nstructure-preserving decoder with hierarchical folding for complete shape\\ngeneration. The hierarchical folding preserves the structure of complete point\\ncloud generated in upper layer by progressively detailing the local regions,\\nusing the skip-attentioned geometry at the same resolution. We conduct\\ncomprehensive experiments on ShapeNet and KITTI datasets, which demonstrate\\nthat the proposed SA-Net outperforms the state-of-the-art point cloud\\ncompletion methods.'\n",
            " b'Data augmentation techniques have become standard practice in deep learning,\\nas it has been shown to greatly improve the generalisation abilities of models.\\nThese techniques rely on different ideas such as invariance-preserving\\ntransformations (e.g, expert-defined augmentation), statistical heuristics\\n(e.g, Mixup), and learning the data distribution (e.g, GANs). However, in the\\nadversarial settings it remains unclear under what conditions such data\\naugmentation methods reduce or even worsen the misclassification risk. In this\\npaper, we therefore analyse the effect of different data augmentation\\ntechniques on the adversarial risk by three measures: (a) the well-known risk\\nunder adversarial attacks, (b) a new measure of prediction-change stress based\\non the Laplacian operator, and (c) the influence of training examples on\\nprediction. The results of our empirical analysis disprove the hypothesis that\\nan improvement in the classification performance induced by a data augmentation\\nis always accompanied by an improvement in the risk under adversarial attack.\\nFurther, our results reveal that the augmented data has more influence than the\\nnon-augmented data, on the resulting models. Taken together, our results\\nsuggest that general-purpose data augmentations that do not take into the\\naccount the characteristics of the data and the task, must be applied with\\ncare.'\n",
            " b'Recently, the attention mechanism has been successfully applied in\\nconvolutional neural networks (CNNs), significantly boosting the performance of\\nmany computer vision tasks. Unfortunately, few medical image recognition\\napproaches incorporate the attention mechanism in the CNNs. In particular,\\nthere exists high redundancy in fundus images for glaucoma detection, such that\\nthe attention mechanism has potential in improving the performance of CNN-based\\nglaucoma detection. This paper proposes an attention-based CNN for glaucoma\\ndetection (AG-CNN). Specifically, we first establish a large-scale attention\\nbased glaucoma (LAG) database, which includes 5,824 fundus images labeled with\\neither positive glaucoma (2,392) or negative glaucoma (3,432). The attention\\nmaps of the ophthalmologists are also collected in LAG database through a\\nsimulated eye-tracking experiment. Then, a new structure of AG-CNN is designed,\\nincluding an attention prediction subnet, a pathological area localization\\nsubnet and a glaucoma classification subnet. Different from other\\nattention-based CNN methods, the features are also visualized as the localized\\npathological area, which can advance the performance of glaucoma detection.\\nFinally, the experiment results show that the proposed AG-CNN approach\\nsignificantly advances state-of-the-art glaucoma detection.'\n",
            " b'Multi-horizon forecasting problems often contain a complex mix of inputs --\\nincluding static (i.e. time-invariant) covariates, known future inputs, and\\nother exogenous time series that are only observed historically -- without any\\nprior information on how they interact with the target. While several deep\\nlearning models have been proposed for multi-step prediction, they typically\\ncomprise black-box models which do not account for the full range of inputs\\npresent in common scenarios. In this paper, we introduce the Temporal Fusion\\nTransformer (TFT) -- a novel attention-based architecture which combines\\nhigh-performance multi-horizon forecasting with interpretable insights into\\ntemporal dynamics. To learn temporal relationships at different scales, the TFT\\nutilizes recurrent layers for local processing and interpretable self-attention\\nlayers for learning long-term dependencies. The TFT also uses specialized\\ncomponents for the judicious selection of relevant features and a series of\\ngating layers to suppress unnecessary components, enabling high performance in\\na wide range of regimes. On a variety of real-world datasets, we demonstrate\\nsignificant performance improvements over existing benchmarks, and showcase\\nthree practical interpretability use-cases of TFT.'\n",
            " b'Tensor ring (TR) decomposition is a powerful tool for exploiting the low-rank\\nnature of multiway data and has demonstrated great potential in a variety of\\nimportant applications. In this paper, nonnegative tensor ring (NTR)\\ndecomposition and graph regularized NTR (GNTR) decomposition are proposed,\\nwhere the former equips TR decomposition with local feature extraction by\\nimposing nonnegativity on the core tensors and the latter is additionally able\\nto capture manifold geometry information of tensor data, both significantly\\nextend the applications of TR decomposition for nonnegative multiway\\nrepresentation learning. Accelerated proximal gradient based methods are\\nderived for NTR and GNTR. The experimental result demonstrate that the proposed\\nalgorithms can extract parts-based basis with rich colors and rich lines from\\ntensor objects that provide more interpretable and meaningful representation,\\nand hence yield better performance than the state-of-the-art tensor based\\nmethods in clustering and classification tasks.'\n",
            " b'Computer vision is increasingly effective at segmenting objects in images and\\nvideos; however, scene effects related to the objects---shadows, reflections,\\ngenerated smoke, etc---are typically overlooked. Identifying such scene effects\\nand associating them with the objects producing them is important for improving\\nour fundamental understanding of visual scenes, and can also assist a variety\\nof applications such as removing, duplicating, or enhancing objects in video.\\nIn this work, we take a step towards solving this novel problem of\\nautomatically associating objects with their effects in video. Given an\\nordinary video and a rough segmentation mask over time of one or more subjects\\nof interest, we estimate an omnimatte for each subject---an alpha matte and\\ncolor image that includes the subject along with all its related time-varying\\nscene elements. Our model is trained only on the input video in a\\nself-supervised manner, without any manual labels, and is generic---it produces\\nomnimattes automatically for arbitrary objects and a variety of effects. We\\nshow results on real-world videos containing interactions between different\\ntypes of subjects (cars, animals, people) and complex effects, ranging from\\nsemi-transparent elements such as smoke and reflections, to fully opaque\\neffects such as objects attached to the subject.'\n",
            " b'In this work, we address the challenging task of few-shot segmentation.\\nPrevious few-shot segmentation methods mainly employ the information of support\\nimages as guidance for query image segmentation. Although some works propose to\\nbuild cross-reference between support and query images, their extraction of\\nquery information still depends on the support images. We here propose to\\nextract the information from the query itself independently to benefit the\\nfew-shot segmentation task. To this end, we first propose a prior extractor to\\nlearn the query information from the unlabeled images with our proposed\\nglobal-local contrastive learning. Then, we extract a set of predetermined\\npriors via this prior extractor. With the obtained priors, we generate the\\nprior region maps for query images, which locate the objects, as guidance to\\nperform cross interaction with support features. In such a way, the extraction\\nof query information is detached from the support branch, overcoming the\\nlimitation by support, and could obtain more informative query clues to achieve\\nbetter interaction. Without bells and whistles, the proposed approach achieves\\nnew state-of-the-art performance for the few-shot segmentation task on\\nPASCAL-5$^{i}$ and COCO datasets.'\n",
            " b'Molecular property prediction plays a fundamental role in drug discovery to\\ndiscover candidate molecules with target properties. However, molecular\\nproperty prediction is essentially a few-shot problem which makes it hard to\\nobtain regular models. In this paper, we propose a property-aware adaptive\\nrelation networks (PAR) for the few-shot molecular property prediction problem.\\nIn comparison to existing works, we leverage the facts that both substructures\\nand relationships among molecules are different considering various molecular\\nproperties. Our PAR is compatible with existing graph-based molecular encoders,\\nand are further equipped with the ability to obtain property-aware molecular\\nembedding and model molecular relation graph adaptively. The resultant relation\\ngraph also facilitates effective label propagation within each task. Extensive\\nexperiments on benchmark molecular property prediction datasets show that our\\nmethod consistently outperforms state-of-the-art methods and is able to obtain\\nproperty-aware molecular embedding and model molecular relation graph properly.'\n",
            " b'In order to achieve high efficiency of classification in intrusion detection,\\na compressed model is proposed in this paper which combines horizontal\\ncompression with vertical compression. OneR is utilized as horizontal\\ncom-pression for attribute reduction, and affinity propagation is employed as\\nvertical compression to select small representative exemplars from large\\ntraining data. As to be able to computationally compress the larger volume of\\ntraining data with scalability, MapReduce based parallelization approach is\\nthen implemented and evaluated for each step of the model compression process\\nabovementioned, on which common but efficient classification methods can be\\ndirectly used. Experimental application study on two publicly available\\ndatasets of intrusion detection, KDD99 and CMDC2012, demonstrates that the\\nclassification using the compressed model proposed can effectively speed up the\\ndetection procedure at up to 184 times, most importantly at the cost of a\\nminimal accuracy difference with less than 1% on average.'\n",
            " b'Disentangled representation learning has been proposed as an approach to\\nlearning general representations. This can be done in the absence of, or with\\nlimited, annotations. A good general representation can be readily fine-tuned\\nfor new target tasks using modest amounts of data, or even be used directly in\\nunseen domains achieving remarkable performance in the corresponding task. This\\nalleviation of the data and annotation requirements offers tantalising\\nprospects for tractable and affordable applications in computer vision and\\nhealthcare. Finally, disentangled representations can offer model\\nexplainability and can help us understand the underlying causal relations of\\nthe factors of variation, increasing their suitability for real-world\\ndeployment. In this tutorial paper, we will offer an overview of the\\ndisentangled representation learning, its building blocks and criteria, and\\ndiscuss applications in computer vision and medical imaging. We conclude our\\ntutorial by presenting the identified opportunities for the integration of\\nrecent machine learning advances into disentanglement, as well as the remaining\\nchallenges.'\n",
            " b'Traditionally, federated learning (FL) aims to train a single global model\\nwhile collaboratively using multiple clients and a server. Two natural\\nchallenges that FL algorithms face are heterogeneity in data across clients and\\ncollaboration of clients with {\\\\em diverse resources}. In this work, we\\nintroduce a \\\\textit{quantized} and \\\\textit{personalized} FL algorithm QuPeD\\nthat facilitates collective (personalized model compression) training via\\n\\\\textit{knowledge distillation} (KD) among clients who have access to\\nheterogeneous data and resources. For personalization, we allow clients to\\nlearn \\\\textit{compressed personalized models} with different quantization\\nparameters and model dimensions/structures. Towards this, first we propose an\\nalgorithm for learning quantized models through a relaxed optimization problem,\\nwhere quantization values are also optimized over. When each client\\nparticipating in the (federated) learning process has different requirements\\nfor the compressed model (both in model dimension and precision), we formulate\\na compressed personalization framework by introducing knowledge distillation\\nloss for local client objectives collaborating through a global model. We\\ndevelop an alternating proximal gradient update for solving this compressed\\npersonalization problem, and analyze its convergence properties. Numerically,\\nwe validate that QuPeD outperforms competing personalized FL methods, FedAvg,\\nand local training of clients in various heterogeneous settings.'\n",
            " b'The ability to perceive the environments in different ways is essential to\\nrobotic research. This involves the analysis of both 2D and 3D data sources. We\\npresent a large scale urban scene dataset associated with a handy simulator\\nbased on Unreal Engine 4 and AirSim, which consists of both man-made and\\nreal-world reconstruction scenes in different scales, referred to as\\nUrbanScene3D. Unlike previous works that purely based on 2D information or\\nman-made 3D CAD models, UrbanScene3D contains both compact man-made models and\\ndetailed real-world models reconstructed by aerial images. Each building has\\nbeen manually extracted from the entire scene model and then has been assigned\\nwith a unique label, forming an instance segmentation map. The provided 3D\\nground-truth textured models with instance segmentation labels in UrbanScene3D\\nallow users to obtain all kinds of data they would like to have: instance\\nsegmentation map, depth map in arbitrary resolution, 3D point cloud/mesh in\\nboth visible and invisible places, etc. In addition, with the help of AirSim,\\nusers can also simulate the robots (cars/drones)to test a variety of autonomous\\ntasks in the proposed city environment. Please refer to our paper and\\nwebsite(https://vcc.tech/UrbanScene3D/) for further details and applications.'\n",
            " b'Neural Architecture Search (NAS) has shown great potentials in automatically\\ndesigning scalable network architectures for dense image predictions. However,\\nexisting NAS algorithms usually compromise on restricted search space and\\nsearch on proxy task to meet the achievable computational demands. To allow as\\nwide as possible network architectures and avoid the gap between target and\\nproxy dataset, we propose a Densely Connected NAS (DCNAS) framework, which\\ndirectly searches the optimal network structures for the multi-scale\\nrepresentations of visual information, over a large-scale target dataset.\\nSpecifically, by connecting cells with each other using learnable weights, we\\nintroduce a densely connected search space to cover an abundance of mainstream\\nnetwork designs. Moreover, by combining both path-level and channel-level\\nsampling strategies, we design a fusion module to reduce the memory consumption\\nof ample search space. We demonstrate that the architecture obtained from our\\nDCNAS algorithm achieves state-of-the-art performances on public semantic image\\nsegmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC\\n2012. We also retain leading performances when evaluating the architecture on\\nthe more challenging ADE20K and Pascal Context dataset.'\n",
            " b'Deep metric learning is essential for visual recognition. The widely used\\npair-wise (or triplet) based loss objectives cannot make full use of semantical\\ninformation in training samples or give enough attention to those hard samples\\nduring optimization. Thus, they often suffer from a slow convergence rate and\\ninferior performance. In this paper, we show how to learn an importance-driven\\ndistance metric via optimal transport programming from batches of samples. It\\ncan automatically emphasize hard examples and lead to significant improvements\\nin convergence. We propose a new batch-wise optimal transport loss and combine\\nit in an end-to-end deep metric learning manner. We use it to learn the\\ndistance metric and deep feature representation jointly for recognition.\\nEmpirical results on visual retrieval and classification tasks with six\\nbenchmark datasets, i.e., MNIST, CIFAR10, SHREC13, SHREC14, ModelNet10, and\\nModelNet40, demonstrate the superiority of the proposed method. It can\\naccelerate the convergence rate significantly while achieving a\\nstate-of-the-art recognition performance. For example, in 3D shape recognition\\nexperiments, we show that our method can achieve better recognition performance\\nwithin only 5 epochs than what can be obtained by mainstream 3D shape\\nrecognition approaches after 200 epochs.'\n",
            " b\"The success and generalisation of deep learning algorithms heavily depend on\\nlearning good feature representations. In medical imaging this entails\\nrepresenting anatomical information, as well as properties related to the\\nspecific imaging setting. Anatomical information is required to perform further\\nanalysis, whereas imaging information is key to disentangle scanner variability\\nand potential artefacts. The ability to factorise these would allow for\\ntraining algorithms only on the relevant information according to the task. To\\ndate, such factorisation has not been attempted. In this paper, we propose a\\nmethodology of latent space factorisation relying on the cycle-consistency\\nprinciple. As an example application, we consider cardiac MR segmentation,\\nwhere we separate information related to the myocardium from other features\\nrelated to imaging and surrounding substructures. We demonstrate the proposed\\nmethod's utility in a semi-supervised setting: we use very few labelled images\\ntogether with many unlabelled images to train a myocardium segmentation neural\\nnetwork. Specifically, we achieve comparable performance to fully supervised\\nnetworks using a fraction of labelled images in experiments on ACDC and a\\ndataset from Edinburgh Imaging Facility QMRI. Code will be made available at\\nhttps://github.com/agis85/spatial_factorisation.\"\n",
            " b'Channel pruning and tensor decomposition have received extensive attention in\\nconvolutional neural network compression. However, these two techniques are\\ntraditionally deployed in an isolated manner, leading to significant accuracy\\ndrop when pursuing high compression rates. In this paper, we propose a\\nCollaborative Compression (CC) scheme, which joints channel pruning and tensor\\ndecomposition to compress CNN models by simultaneously learning the model\\nsparsity and low-rankness. Specifically, we first investigate the compression\\nsensitivity of each layer in the network, and then propose a Global Compression\\nRate Optimization that transforms the decision problem of compression rate into\\nan optimization problem. After that, we propose multi-step heuristic\\ncompression to remove redundant compression units step-by-step, which fully\\nconsiders the effect of the remaining compression space (i.e., unremoved\\ncompression units). Our method demonstrates superior performance gains over\\nprevious ones on various datasets and backbone architectures. For example, we\\nachieve 52.9% FLOPs reduction by removing 48.4% parameters on ResNet-50 with\\nonly a Top-1 accuracy drop of 0.56% on ImageNet 2012.'\n",
            " b'Meta-learning has been proposed as a framework to address the challenging\\nfew-shot learning setting. The key idea is to leverage a large number of\\nsimilar few-shot tasks in order to learn how to adapt a base-learner to a new\\ntask for which only a few labeled samples are available. As deep neural\\nnetworks (DNNs) tend to overfit using a few samples only, typical meta-learning\\nmodels use shallow neural networks, thus limiting its effectiveness. In order\\nto achieve top performance, some recent works tried to use the DNNs pre-trained\\non large-scale datasets but mostly in straight-forward manners, e.g., (1)\\ntaking their weights as a warm start of meta-training, and (2) freezing their\\nconvolutional layers as the feature extractor of base-learners. In this paper,\\nwe propose a novel approach called meta-transfer learning (MTL) which learns to\\ntransfer the weights of a deep NN for few-shot learning tasks. Specifically,\\nmeta refers to training multiple tasks, and transfer is achieved by learning\\nscaling and shifting functions of DNN weights for each task. In addition, we\\nintroduce the hard task (HT) meta-batch scheme as an effective learning\\ncurriculum that further boosts the learning efficiency of MTL. We conduct\\nfew-shot learning experiments and report top performance for five-class\\nfew-shot recognition tasks on three challenging benchmarks: miniImageNet,\\ntieredImageNet and Fewshot-CIFAR100 (FC100). Extensive comparisons to related\\nworks validate that our MTL approach trained with the proposed HT meta-batch\\nscheme achieves top performance. An ablation study also shows that both\\ncomponents contribute to fast convergence and high accuracy.'\n",
            " b'Binary neural networks have attracted numerous attention in recent years.\\nHowever, mainly due to the information loss stemming from the biased\\nbinarization, how to preserve the accuracy of networks still remains a critical\\nissue. In this paper, we attempt to maintain the information propagated in the\\nforward process and propose a Balanced Binary Neural Networks with Gated\\nResidual (BBG for short). First, a weight balanced binarization is introduced\\nto maximize information entropy of binary weights, and thus the informative\\nbinary weights can capture more information contained in the activations.\\nSecond, for binary activations, a gated residual is further appended to\\ncompensate their information loss during the forward process, with a slight\\noverhead. Both techniques can be wrapped as a generic network module that\\nsupports various network architectures for different tasks including\\nclassification and detection. We evaluate our BBG on image classification tasks\\nover CIFAR-10/100 and ImageNet and on detection task over Pascal VOC. The\\nexperimental results show that BBG-Net performs remarkably well across various\\nnetwork architectures such as VGG, ResNet and SSD with the superior performance\\nover state-of-the-art methods in terms of memory consumption, inference speed\\nand accuracy.'\n",
            " b'Knowledge graph (KG) embedding aims at embedding entities and relations in a\\nKG into a lowdimensional latent representation space. Existing KG embedding\\napproaches model entities andrelations in a KG by utilizing real-valued ,\\ncomplex-valued, or hypercomplex-valued (Quaternionor Octonion) representations,\\nall of which are subsumed into a geometric algebra. In this work,we introduce a\\nnovel geometric algebra-based KG embedding framework, GeomE, which uti-lizes\\nmultivector representations and the geometric product to model entities and\\nrelations. Ourframework subsumes several state-of-the-art KG embedding\\napproaches and is advantageouswith its ability of modeling various key relation\\npatterns, including (anti-)symmetry, inversionand composition, rich\\nexpressiveness with higher degree of freedom as well as good general-ization\\ncapacity. Experimental results on multiple benchmark knowledge graphs show that\\ntheproposed approach outperforms existing state-of-the-art models for link\\nprediction.'\n",
            " b'In this work, we propose a novel straightforward method for medical volume\\nand sequence segmentation with limited annotations. To avert laborious\\nannotating, the recent success of self-supervised learning(SSL) motivates the\\npre-training on unlabeled data. Despite its success, it is still challenging to\\nadapt typical SSL methods to volume/sequence segmentation, due to their lack of\\nmining on local semantic discrimination and rare exploitation on volume and\\nsequence structures. Based on the continuity between slices/frames and the\\ncommon spatial layout of organs across volumes/sequences, we introduced a novel\\nbootstrap self-supervised representation learning method by leveraging the\\npredictable possibility of neighboring slices. At the core of our method is a\\nsimple and straightforward dense self-supervision on the predictions of local\\nrepresentations and a strategy of predicting locals based on global context,\\nwhich enables stable and reliable supervision for both global and local\\nrepresentation mining among volumes. Specifically, we first proposed an\\nasymmetric network with an attention-guided predictor to enforce\\ndistance-specific prediction and supervision on slices within and across\\nvolumes/sequences. Secondly, we introduced a novel prototype-based\\nforeground-background calibration module to enhance representation consistency.\\nThe two parts are trained jointly on labeled and unlabeled data. When evaluated\\non three benchmark datasets of medical volumes and sequences, our model\\noutperforms existing methods with a large margin of 4.5\\\\% DSC on ACDC, 1.7\\\\% on\\nProstate, and 2.3\\\\% on CAMUS. Intensive evaluations reveals the effectiveness\\nand superiority of our method.'\n",
            " b'We present an unsupervised learning approach to recover 3D human pose from 2D\\nskeletal joints extracted from a single image. Our method does not require any\\nmulti-view image data, 3D skeletons, correspondences between 2D-3D points, or\\nuse previously learned 3D priors during training. A lifting network accepts 2D\\nlandmarks as inputs and generates a corresponding 3D skeleton estimate. During\\ntraining, the recovered 3D skeleton is reprojected on random camera viewpoints\\nto generate new \"synthetic\" 2D poses. By lifting the synthetic 2D poses back to\\n3D and re-projecting them in the original camera view, we can define\\nself-consistency loss both in 3D and in 2D. The training can thus be self\\nsupervised by exploiting the geometric self-consistency of the\\nlift-reproject-lift process. We show that self-consistency alone is not\\nsufficient to generate realistic skeletons, however adding a 2D pose\\ndiscriminator enables the lifter to output valid 3D poses. Additionally, to\\nlearn from 2D poses \"in the wild\", we train an unsupervised 2D domain adapter\\nnetwork to allow for an expansion of 2D data. This improves results and\\ndemonstrates the usefulness of 2D pose data for unsupervised 3D lifting.\\nResults on Human3.6M dataset for 3D human pose estimation demonstrate that our\\napproach improves upon the previous unsupervised methods by 30% and outperforms\\nmany weakly supervised approaches that explicitly use 3D data.'\n",
            " b\"In this work we present Discrete Attend Infer Repeat (Discrete-AIR), a\\nRecurrent Auto-Encoder with structured latent distributions containing discrete\\ncategorical distributions, continuous attribute distributions, and factorised\\nspatial attention. While inspired by the original AIR model andretaining AIR\\nmodel's capability in identifying objects in an image, Discrete-AIR provides\\ndirect interpretability of the latent codes. We show that for Multi-MNIST and a\\nmultiple-objects version of dSprites dataset, the Discrete-AIR model needs just\\none categorical latent variable, one attribute variable (for Multi-MNIST only),\\ntogether with spatial attention variables, for efficient inference. We perform\\nanalysis to show that the learnt categorical distributions effectively capture\\nthe categories of objects in the scene for Multi-MNIST and for Multi-Sprites.\"\n",
            " b'Images captured under low-light conditions often suffer from (partially) poor\\nvisibility. Besides unsatisfactory lightings, multiple types of degradations,\\nsuch as noise and color distortion due to the limited quality of cameras, hide\\nin the dark. In other words, solely turning up the brightness of dark regions\\nwill inevitably amplify hidden artifacts. This work builds a simple yet\\neffective network for \\\\textbf{Kin}dling the \\\\textbf{D}arkness (denoted as\\nKinD), which, inspired by Retinex theory, decomposes images into two\\ncomponents. One component (illumination) is responsible for light adjustment,\\nwhile the other (reflectance) for degradation removal. In such a way, the\\noriginal space is decoupled into two smaller subspaces, expecting to be better\\nregularized/learned. It is worth to note that our network is trained with\\npaired images shot under different exposure conditions, instead of using any\\nground-truth reflectance and illumination information. Extensive experiments\\nare conducted to demonstrate the efficacy of our design and its superiority\\nover state-of-the-art alternatives. Our KinD is robust against severe visual\\ndefects, and user-friendly to arbitrarily adjust light levels. In addition, our\\nmodel spends less than 50ms to process an image in VGA resolution on a 2080Ti\\nGPU. All the above merits make our KinD attractive for practical use.'\n",
            " b'Single-frame infrared small target detection remains a challenge not only due\\nto the scarcity of intrinsic target characteristics but also because of lacking\\na public dataset. In this paper, we first contribute an open dataset with\\nhigh-quality annotations to advance the research in this field. We also propose\\nan asymmetric contextual modulation module specially designed for detecting\\ninfrared small targets. To better highlight small targets, besides a top-down\\nglobal contextual feedback, we supplement a bottom-up modulation pathway based\\non point-wise channel attention for exchanging high-level semantics and subtle\\nlow-level details. We report ablation studies and comparisons to\\nstate-of-the-art methods, where we find that our approach performs\\nsignificantly better. Our dataset and code are available online.'\n",
            " b'Many User interactive systems are proposed all methods are trying to\\nimplement as a user friendly and various approaches proposed but most of the\\nsystems not reached to the use specifications like user friendly systems with\\nuser interest, all proposed method implemented basic techniques some are\\nimproved methods also propose but not reaching to the user specifications. In\\nthis proposed paper we concentrated on image retrieval system with in early\\ndays many user interactive systems performed with basic concepts but such\\nsystems are not reaching to the user specifications and not attracted to the\\nuser so a lot of research interest in recent years with new specifications,\\nrecent approaches have user is interested in friendly interacted methods are\\nexpecting, many are concentrated for improvement in all methods. In this\\nproposed system we focus on the retrieval of images within a large image\\ncollection based on color projections and different mathematical approaches are\\nintroduced and applied for retrieval of images. before Appling proposed methods\\nimages are sub grouping using threshold values, in this paper R G B color\\ncombinations considered for retrieval of images, in proposed methods are\\nimplemented and results are included, through results it is observed that we\\nobtaining efficient results comparatively previous and existing.'\n",
            " b'We consider the problem of learning fair policies in (deep) cooperative\\nmulti-agent reinforcement learning (MARL). We formalize it in a principled way\\nas the problem of optimizing a welfare function that explicitly encodes two\\nimportant aspects of fairness: efficiency and equity. As a solution method, we\\npropose a novel neural network architecture, which is composed of two\\nsub-networks specifically designed for taking into account the two aspects of\\nfairness. In experiments, we demonstrate the importance of the two sub-networks\\nfor fair optimization. Our overall approach is general as it can accommodate\\nany (sub)differentiable welfare function. Therefore, it is compatible with\\nvarious notions of fairness that have been proposed in the literature (e.g.,\\nlexicographic maximin, generalized Gini social welfare function, proportional\\nfairness). Our solution method is generic and can be implemented in various\\nMARL settings: centralized training and decentralized execution, or fully\\ndecentralized. Finally, we experimentally validate our approach in various\\ndomains and show that it can perform much better than previous methods.'\n",
            " b'In this paper, we propose a monocular 3D object detection framework in the\\ndomain of autonomous driving. Unlike previous image-based methods which focus\\non RGB feature extracted from 2D images, our method solves this problem in the\\nreconstructed 3D space in order to exploit 3D contexts explicitly. To this end,\\nwe first leverage a stand-alone module to transform the input data from 2D\\nimage plane to 3D point clouds space for a better input representation, then we\\nperform the 3D detection using PointNet backbone net to obtain objects 3D\\nlocations, dimensions and orientations. To enhance the discriminative\\ncapability of point clouds, we propose a multi-modal feature fusion module to\\nembed the complementary RGB cue into the generated point clouds representation.\\nWe argue that it is more effective to infer the 3D bounding boxes from the\\ngenerated 3D scene space (i.e., X,Y, Z space) compared to the image plane\\n(i.e., R,G,B image plane). Evaluation on the challenging KITTI dataset shows\\nthat our approach boosts the performance of state-of-the-art monocular approach\\nby a large margin.'\n",
            " b\"3D Point cloud registration is still a very challenging topic due to the\\ndifficulty in finding the rigid transformation between two point clouds with\\npartial correspondences, and it's even harder in the absence of any initial\\nestimation information. In this paper, we present an end-to-end deep-learning\\nbased approach to resolve the point cloud registration problem. Firstly, the\\nrevised LPD-Net is introduced to extract features and aggregate them with the\\ngraph network. Secondly, the self-attention mechanism is utilized to enhance\\nthe structure information in the point cloud and the cross-attention mechanism\\nis designed to enhance the corresponding information between the two input\\npoint clouds. Based on which, the virtual corresponding points can be generated\\nby a soft pointer based method, and finally, the point cloud registration\\nproblem can be solved by implementing the SVD method. Comparison results in\\nModelNet40 dataset validate that the proposed approach reaches the\\nstate-of-the-art in point cloud registration tasks and experiment resutls in\\nKITTI dataset validate the effectiveness of the proposed approach in real\\napplications.Our source code is available at\\n\\\\url{https://github.com/qiaozhijian/VCR-Net.git}\"\n",
            " b'Through multi-agent competition, the simple objective of hide-and-seek, and\\nstandard reinforcement learning algorithms at scale, we find that agents create\\na self-supervised autocurriculum inducing multiple distinct rounds of emergent\\nstrategy, many of which require sophisticated tool use and coordination. We\\nfind clear evidence of six emergent phases in agent strategy in our\\nenvironment, each of which creates a new pressure for the opposing team to\\nadapt; for instance, agents learn to build multi-object shelters using moveable\\nboxes which in turn leads to agents discovering that they can overcome\\nobstacles using ramps. We further provide evidence that multi-agent competition\\nmay scale better with increasing environment complexity and leads to behavior\\nthat centers around far more human-relevant skills than other self-supervised\\nreinforcement learning methods such as intrinsic motivation. Finally, we\\npropose transfer and fine-tuning as a way to quantitatively evaluate targeted\\ncapabilities, and we compare hide-and-seek agents to both intrinsic motivation\\nand random initialization baselines in a suite of domain-specific intelligence\\ntests.'\n",
            " b'Detecting objects and estimating their pose remains as one of the major\\nchallenges of the computer vision research community. There exists a compromise\\nbetween localizing the objects and estimating their viewpoints. The detector\\nideally needs to be view-invariant, while the pose estimation process should be\\nable to generalize towards the category-level. This work is an exploration of\\nusing deep learning models for solving both problems simultaneously. For doing\\nso, we propose three novel deep learning architectures, which are able to\\nperform a joint detection and pose estimation, where we gradually decouple the\\ntwo tasks. We also investigate whether the pose estimation problem should be\\nsolved as a classification or regression problem, being this still an open\\nquestion in the computer vision community. We detail a comparative analysis of\\nall our solutions and the methods that currently define the state of the art\\nfor this problem. We use PASCAL3D+ and ObjectNet3D datasets to present the\\nthorough experimental evaluation and main results. With the proposed models we\\nachieve the state-of-the-art performance in both datasets.'\n",
            " b'Plant phenotyping focuses on the measurement of plant characteristics\\nthroughout the growing season, typically with the goal of evaluating genotypes\\nfor plant breeding. Estimating plant location is important for identifying\\ngenotypes which have low emergence, which is also related to the environment\\nand management practices such as fertilizer applications. The goal of this\\npaper is to investigate methods that estimate plant locations for a field-based\\ncrop using RGB aerial images captured using Unmanned Aerial Vehicles (UAVs).\\nDeep learning approaches provide promising capability for locating plants\\nobserved in RGB images, but they require large quantities of labeled data\\n(ground truth) for training. Using a deep learning architecture fine-tuned on a\\nsingle field or a single type of crop on fields in other geographic areas or\\nwith other crops may not have good results. The problem of generating ground\\ntruth for each new field is labor-intensive and tedious. In this paper, we\\npropose a method for estimating plant centers by transferring an existing model\\nto a new scenario using limited ground truth data. We describe the use of\\ntransfer learning using a model fine-tuned for a single field or a single type\\nof plant on a varied set of similar crops and fields. We show that transfer\\nlearning provides promising results for detecting plant locations.'\n",
            " b\"We examine the problem of learning and planning on high-dimensional domains\\nwith long horizons and sparse rewards. Recent approaches have shown great\\nsuccesses in many Atari 2600 domains. However, domains with long horizons and\\nsparse rewards, such as Montezuma's Revenge and Venture, remain challenging for\\nexisting methods. Methods using abstraction (Dietterich 2000; Sutton, Precup,\\nand Singh 1999) have shown to be useful in tackling long-horizon problems. We\\ncombine recent techniques of deep reinforcement learning with existing\\nmodel-based approaches using an expert-provided state abstraction. We construct\\ntoy domains that elucidate the problem of long horizons, sparse rewards and\\nhigh-dimensional inputs, and show that our algorithm significantly outperforms\\nprevious methods on these domains. Our abstraction-based approach outperforms\\nDeep Q-Networks (Mnih et al. 2015) on Montezuma's Revenge and Venture, and\\nexhibits backtracking behavior that is absent from previous methods.\"\n",
            " b\"A core element in decision-making under uncertainty is the feedback on the\\nquality of the performed actions. However, in many applications, such feedback\\nis restricted. For example, in recommendation systems, repeatedly asking the\\nuser to provide feedback on the quality of recommendations will annoy them. In\\nthis work, we formalize decision-making problems with querying budget, where\\nthere is a (possibly time-dependent) hard limit on the number of reward queries\\nallowed. Specifically, we consider multi-armed bandits, linear bandits, and\\nreinforcement learning problems. We start by analyzing the performance of\\n`greedy' algorithms that query a reward whenever they can. We show that in\\nfully stochastic settings, doing so performs surprisingly well, but in the\\npresence of any adversity, this might lead to linear regret. To overcome this\\nissue, we propose the Confidence-Budget Matching (CBM) principle that queries\\nrewards when the confidence intervals are wider than the inverse square root of\\nthe available budget. We analyze the performance of CBM based algorithms in\\ndifferent settings and show that they perform well in the presence of adversity\\nin the contexts, initial states, and budgets.\"\n",
            " b'Graph-based Semi-Supervised Learning (SSL) aims to transfer the labels of a\\nhandful of labeled data to the remaining massive unlabeled data via a graph. As\\none of the most popular graph-based SSL approaches, the recently proposed Graph\\nConvolutional Networks (GCNs) have gained remarkable progress by combining the\\nsound expressiveness of neural networks with graph structure. Nevertheless, the\\nexisting graph-based methods do not directly address the core problem of SSL,\\ni.e., the shortage of supervision, and thus their performances are still very\\nlimited. To accommodate this issue, a novel GCN-based SSL algorithm is\\npresented in this paper to enrich the supervision signals by utilizing both\\ndata similarities and graph structure. Firstly, by designing a semi-supervised\\ncontrastive loss, improved node representations can be generated via maximizing\\nthe agreement between different views of the same data or the data from the\\nsame class. Therefore, the rich unlabeled data and the scarce yet valuable\\nlabeled data can jointly provide abundant supervision information for learning\\ndiscriminative node representations, which helps improve the subsequent\\nclassification result. Secondly, the underlying determinative relationship\\nbetween the data features and input graph topology is extracted as\\nsupplementary supervision signals for SSL via using a graph generative loss\\nrelated to the input features. Intensive experimental results on a variety of\\nreal-world datasets firmly verify the effectiveness of our algorithm compared\\nwith other state-of-the-art methods.'\n",
            " b\"Fashion is the way we present ourselves to the world and has become one of\\nthe world's largest industries. Fashion, mainly conveyed by vision, has thus\\nattracted much attention from computer vision researchers in recent years.\\nGiven the rapid development, this paper provides a comprehensive survey of more\\nthan 200 major fashion-related works covering four main aspects for enabling\\nintelligent fashion: (1) Fashion detection includes landmark detection, fashion\\nparsing, and item retrieval, (2) Fashion analysis contains attribute\\nrecognition, style learning, and popularity prediction, (3) Fashion synthesis\\ninvolves style transfer, pose transformation, and physical simulation, and (4)\\nFashion recommendation comprises fashion compatibility, outfit matching, and\\nhairstyle suggestion. For each task, the benchmark datasets and the evaluation\\nprotocols are summarized. Furthermore, we highlight promising directions for\\nfuture research.\"\n",
            " b\"Deep neural networks (DNNs) have achieved impressive predictive performance\\ndue to their ability to learn complex, non-linear relationships between\\nvariables. However, the inability to effectively visualize these relationships\\nhas led to DNNs being characterized as black boxes and consequently limited\\ntheir applications. To ameliorate this problem, we introduce the use of\\nhierarchical interpretations to explain DNN predictions through our proposed\\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\\ntrained DNN, ACD produces a hierarchical clustering of the input features,\\nalong with the contribution of each cluster to the final prediction. This\\nhierarchy is optimized to identify clusters of features that the DNN learned\\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\\nwe show that ACD is effective at diagnosing incorrect predictions and\\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\\nenables users both to identify the more accurate of two DNNs and to better\\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\\nadversarial perturbations, implying that it captures fundamental aspects of the\\ninput and ignores spurious noise.\"\n",
            " b\"Human-centric explainability of AI-based Decision Support Systems (DSS) using\\nvisual input modalities is directly related to reliability and practicality of\\nsuch algorithms. An otherwise accurate and robust DSS might not enjoy trust of\\nexperts in critical application areas if it is not able to provide reasonable\\njustification of its predictions. This paper introduces Concept Localization\\nMaps (CLMs), which is a novel approach towards explainable image classifiers\\nemployed as DSS. CLMs extend Concept Activation Vectors (CAVs) by locating\\nsignificant regions corresponding to a learned concept in the latent space of a\\ntrained image classifier. They provide qualitative and quantitative assurance\\nof a classifier's ability to learn and focus on similar concepts important for\\nhumans during image recognition. To better understand the effectiveness of the\\nproposed method, we generated a new synthetic dataset called Simple Concept\\nDataBase (SCDB) that includes annotations for 10 distinguishable concepts, and\\nmade it publicly available. We evaluated our proposed method on SCDB as well as\\na real-world dataset called CelebA. We achieved localization recall of above\\n80% for most relevant concepts and average recall above 60% for all concepts\\nusing SE-ResNeXt-50 on SCDB. Our results on both datasets show great promise of\\nCLMs for easing acceptance of DSS in practice.\"\n",
            " b'Almost all of the current top-performing object detection networks employ\\nregion proposals to guide the search for object instances. State-of-the-art\\nregion proposal methods usually need several thousand proposals to get high\\nrecall, thus hurting the detection efficiency. Although the latest Region\\nProposal Network method gets promising detection accuracy with several hundred\\nproposals, it still struggles in small-size object detection and precise\\nlocalization (e.g., large IoU thresholds), mainly due to the coarseness of its\\nfeature maps. In this paper, we present a deep hierarchical network, namely\\nHyperNet, for handling region proposal generation and object detection jointly.\\nOur HyperNet is primarily based on an elaborately designed Hyper Feature which\\naggregates hierarchical feature maps first and then compresses them into a\\nuniform space. The Hyper Features well incorporate deep but highly semantic,\\nintermediate but really complementary, and shallow but naturally\\nhigh-resolution features of the image, thus enabling us to construct HyperNet\\nby sharing them both in generating proposals and detecting objects via an\\nend-to-end joint training strategy. For the deep VGG16 model, our method\\nachieves completely leading recall and state-of-the-art object detection\\naccuracy on PASCAL VOC 2007 and 2012 using only 100 proposals per image. It\\nruns with a speed of 5 fps (including all steps) on a GPU, thus having the\\npotential for real-time processing.'\n",
            " b'Verification and regression are two general methodologies for prediction in\\nneural networks. Each has its own strengths: verification can be easier to\\ninfer accurately, and regression is more efficient and applicable to continuous\\ntarget variables. Hence, it is often beneficial to carefully combine them to\\ntake advantage of their benefits. In this paper, we take this philosophy to\\nimprove state-of-the-art object detection, specifically by RepPoints. Though\\nRepPoints provides high performance, we find that its heavy reliance on\\nregression for object localization leaves room for improvement. We introduce\\nverification tasks into the localization prediction of RepPoints, producing\\nRepPoints v2, which provides consistent improvements of about 2.0 mAP over the\\noriginal RepPoints on the COCO object detection benchmark using different\\nbackbones and training methods. RepPoints v2 also achieves 52.1 mAP on COCO\\n\\\\texttt{test-dev} by a single model. Moreover, we show that the proposed\\napproach can more generally elevate other object detection frameworks as well\\nas applications such as instance segmentation. The code is available at\\nhttps://github.com/Scalsol/RepPointsV2.'\n",
            " b'Reward function specification, which requires considerable human effort and\\niteration, remains a major impediment for learning behaviors through deep\\nreinforcement learning. In contrast, providing visual demonstrations of desired\\nbehaviors often presents an easier and more natural way to teach agents. We\\nconsider a setting where an agent is provided a fixed dataset of visual\\ndemonstrations illustrating how to perform a task, and must learn to solve the\\ntask using the provided demonstrations and unsupervised environment\\ninteractions. This setting presents a number of challenges including\\nrepresentation learning for visual observations, sample complexity due to high\\ndimensional spaces, and learning instability due to the lack of a fixed reward\\nor learning signal. Towards addressing these challenges, we develop a\\nvariational model-based adversarial imitation learning (V-MAIL) algorithm. The\\nmodel-based approach provides a strong signal for representation learning,\\nenables sample efficiency, and improves the stability of adversarial training\\nby enabling on-policy learning. Through experiments involving several\\nvision-based locomotion and manipulation tasks, we find that V-MAIL learns\\nsuccessful visuomotor policies in a sample-efficient manner, has better\\nstability compared to prior work, and also achieves higher asymptotic\\nperformance. We further find that by transferring the learned models, V-MAIL\\ncan learn new tasks from visual demonstrations without any additional\\nenvironment interactions. All results including videos can be found online at\\n\\\\url{https://sites.google.com/view/variational-mail}.'\n",
            " b\"The cerebellar grey matter morphology is an important feature to study\\nneurodegenerative diseases such as Alzheimer's disease or Down's syndrome. Its\\nvolume or thickness is commonly used as a surrogate imaging biomarker for such\\ndiseases. Most studies about grey matter thickness estimation focused on the\\ncortex, and little attention has been drawn on the morphology of the\\ncerebellum. Using ex vivo high-resolution MRI, it is now possible to visualise\\nthe different cell layers in the mouse cerebellum. In this work, we introduce a\\nframework to extract the Purkinje layer within the grey matter, enabling the\\nestimation of the thickness of the cerebellar grey matter, the granular layer\\nand molecular layer from gadolinium-enhanced ex vivo mouse brain MRI.\\nApplication to mouse model of Down's syndrome found reduced cortical and layer\\nthicknesses in the transchromosomic group.\"\n",
            " b'We propose a nonparametric model for time series with missing data based on\\nlow-rank matrix factorization. The model expresses each instance in a set of\\ntime series as a linear combination of a small number of shared basis\\nfunctions. Constraining the functions and the corresponding coefficients to be\\nnonnegative yields an interpretable low-dimensional representation of the data.\\nA time-smoothing regularization term ensures that the model captures meaningful\\ntrends in the data, instead of overfitting short-term fluctuations. The\\nlow-dimensional representation makes it possible to detect outliers and cluster\\nthe time series according to the interpretable features extracted by the model,\\nand also to perform forecasting via kernel regression. We apply our methodology\\nto a large real-world dataset of infant-sleep data gathered by caregivers with\\na mobile-phone app. Our analysis automatically extracts daily-sleep patterns\\nconsistent with the existing literature. This allows us to compute\\nsleep-development trends for the cohort, which characterize the emergence of\\ncircadian sleep and different napping habits. We apply our methodology to\\ndetect anomalous individuals, to cluster the cohort into groups with different\\nsleeping tendencies, and to obtain improved predictions of future sleep\\nbehavior.'\n",
            " b'This research strives for natural language moment retrieval in long,\\nuntrimmed video streams. The problem is not trivial especially when a video\\ncontains multiple moments of interests and the language describes complex\\ntemporal dependencies, which often happens in real scenarios. We identify two\\ncrucial challenges: semantic misalignment and structural misalignment. However,\\nexisting approaches treat different moments separately and do not explicitly\\nmodel complex moment-wise temporal relations. In this paper, we present Moment\\nAlignment Network (MAN), a novel framework that unifies the candidate moment\\nencoding and temporal structural reasoning in a single-shot feed-forward\\nnetwork. MAN naturally assigns candidate moment representations aligned with\\nlanguage semantics over different temporal locations and scales. Most\\nimportantly, we propose to explicitly model moment-wise temporal relations as a\\nstructured graph and devise an iterative graph adjustment network to jointly\\nlearn the best structure in an end-to-end manner. We evaluate the proposed\\napproach on two challenging public benchmarks DiDeMo and Charades-STA, where\\nour MAN significantly outperforms the state-of-the-art by a large margin.'\n",
            " b'Vehicle location prediction or vehicle tracking is a significant topic within\\nconnected vehicles. This task, however, is difficult if only a single modal\\ndata is available, probably causing bias and impeding the accuracy. With the\\ndevelopment of sensor networks in connected vehicles, multimodal data are\\nbecoming accessible. Therefore, we propose a framework for vehicle tracking\\nwith multimodal data fusion. Specifically, we fuse the results of two\\nmodalities, images and velocity, in our vehicle-tracking task. Images, being\\nprocessed in the module of vehicle detection, provide direct information about\\nthe features of vehicles, whereas velocity estimation can further evaluate the\\npossible location of the target vehicles, which reduces the number of features\\nbeing compared, and decreases the time consumption and computational cost.\\nVehicle detection is designed with a color-faster R-CNN, which takes both the\\nshape and color of the vehicles into consideration. Meanwhile, velocity\\nestimation is through the Kalman filter, which is a classical method for\\ntracking. Finally, a multimodal data fusion method is applied to integrate\\nthese outcomes so that vehicle-tracking tasks can be achieved. Experimental\\nresults suggest the efficiency of our methods, which can track vehicles using a\\nseries of surveillance cameras in urban areas.'\n",
            " b'When applying machine learning to sensitive data, one has to find a balance\\nbetween accuracy, information security, and computational-complexity. Recent\\nstudies combined Homomorphic Encryption with neural networks to make inferences\\nwhile protecting against information leakage. However, these methods are\\nlimited by the width and depth of neural networks that can be used (and hence\\nthe accuracy) and exhibit high latency even for relatively simple networks. In\\nthis study we provide two solutions that address these limitations. In the\\nfirst solution, we present more than $10\\\\times$ improvement in latency and\\nenable inference on wider networks compared to prior attempts with the same\\nlevel of security. The improved performance is achieved by novel methods to\\nrepresent the data during the computation. In the second solution, we apply the\\nmethod of transfer learning to provide private inference services using deep\\nnetworks with latency of $\\\\sim0.16$ seconds. We demonstrate the efficacy of our\\nmethods on several computer vision tasks.'\n",
            " b'Video representation learning has recently attracted attention in computer\\nvision due to its applications for activity and scene forecasting or\\nvision-based planning and control. Video prediction models often learn a latent\\nrepresentation of video which is encoded from input frames and decoded back\\ninto images. Even when conditioned on actions, purely deep learning based\\narchitectures typically lack a physically interpretable latent space. In this\\nstudy, we use a differentiable physics engine within an action-conditional\\nvideo representation network to learn a physical latent representation. We\\npropose supervised and self-supervised learning methods to train our network\\nand identify physical properties. The latter uses spatial transformers to\\ndecode physical states back into images. The simulation scenarios in our\\nexperiments comprise pushing, sliding and colliding objects, for which we also\\nanalyze the observability of the physical properties. In experiments we\\ndemonstrate that our network can learn to encode images and identify physical\\nproperties like mass and friction from videos and action sequences in the\\nsimulated scenarios. We evaluate the accuracy of our supervised and\\nself-supervised methods and compare it with a system identification baseline\\nwhich directly learns from state trajectories. We also demonstrate the ability\\nof our method to predict future video frames from input images and actions.'\n",
            " b'The clinical time-series setting poses a unique combination of challenges to\\ndata modeling and sharing. Due to the high dimensionality of clinical time\\nseries, adequate de-identification to preserve privacy while retaining data\\nutility is difficult to achieve using common de-identification techniques. An\\ninnovative approach to this problem is synthetic data generation. From a\\ntechnical perspective, a good generative model for time-series data should\\npreserve temporal dynamics, in the sense that new sequences respect the\\noriginal relationships between high-dimensional variables across time. From the\\nprivacy perspective, the model should prevent patient re-identification by\\nlimiting vulnerability to membership inference attacks. The NeurIPS 2020\\nHide-and-Seek Privacy Challenge is a novel two-tracked competition to\\nsimultaneously accelerate progress in tackling both problems. In our\\nhead-to-head format, participants in the synthetic data generation track (i.e.\\n\"hiders\") and the patient re-identification track (i.e. \"seekers\") are directly\\npitted against each other by way of a new, high-quality intensive care\\ntime-series dataset: the AmsterdamUMCdb dataset. Ultimately, we seek to advance\\ngenerative techniques for dense and high-dimensional temporal data streams that\\nare (1) clinically meaningful in terms of fidelity and predictivity, as well as\\n(2) capable of minimizing membership privacy risks in terms of the concrete\\nnotion of patient re-identification.'\n",
            " b'Deep CCA is a recently proposed deep neural network extension to the\\ntraditional canonical correlation analysis (CCA), and has been successful for\\nmulti-view representation learning in several domains. However, stochastic\\noptimization of the deep CCA objective is not straightforward, because it does\\nnot decouple over training examples. Previous optimizers for deep CCA are\\neither batch-based algorithms or stochastic optimization using large\\nminibatches, which can have high memory consumption. In this paper, we tackle\\nthe problem of stochastic optimization for deep CCA with small minibatches,\\nbased on an iterative solution to the CCA objective, and show that we can\\nachieve as good performance as previous optimizers and thus alleviate the\\nmemory requirement.'\n",
            " b'In this paper, a robust vehicle local position estimation with the help of\\nsingle camera sensor and GPS is presented. A modified Inverse Perspective\\nMapping, illuminant Invariant techniques and object detection based approach is\\nused to localize the vehicle in the road. Vehicles current lane, its position\\nfrom road boundary and other cars are used to define its local position. For\\nthis purpose Lane markings are detected using a Laplacian edge feature, robust\\nto shadowing. Effect of shadowing and extra sun light are removed using Lab\\ncolor space and illuminant invariant techniques. Lanes are assumed to be as\\nparabolic model and fitted using robust RANSAC. This method can reliably detect\\nall lanes of the road, estimate lane departure angle and local position of\\nvehicle relative to lanes, road boundary and other cars. Different type of\\nobstacle like pedestrians, vehicles are detected using HOG feature based\\ndeformable part model.'\n",
            " b'In this paper, we study the problem of learning vision-based dynamic\\nmanipulation skills using a scalable reinforcement learning approach. We study\\nthis problem in the context of grasping, a longstanding challenge in robotic\\nmanipulation. In contrast to static learning behaviors that choose a grasp\\npoint and then execute the desired grasp, our method enables closed-loop\\nvision-based control, whereby the robot continuously updates its grasp strategy\\nbased on the most recent observations to optimize long-horizon grasp success.\\nTo that end, we introduce QT-Opt, a scalable self-supervised vision-based\\nreinforcement learning framework that can leverage over 580k real-world grasp\\nattempts to train a deep neural network Q-function with over 1.2M parameters to\\nperform closed-loop, real-world grasping that generalizes to 96% grasp success\\non unseen objects. Aside from attaining a very high success rate, our method\\nexhibits behaviors that are quite distinct from more standard grasping systems:\\nusing only RGB vision-based perception from an over-the-shoulder camera, our\\nmethod automatically learns regrasping strategies, probes objects to find the\\nmost effective grasps, learns to reposition objects and perform other\\nnon-prehensile pre-grasp manipulations, and responds dynamically to\\ndisturbances and perturbations.'\n",
            " b'In this paper, we propose a novel approach to tackle the multiple instance\\nregression (MIR) problem. This problem arises when the data is a collection of\\nbags, where each bag is made of multiple instances corresponding to the same\\nunique real-valued label. Our goal is to train a regression model which maps\\nthe instances of an unseen bag to its unique label. This MIR setting is common\\nto remote sensing applications where there is high variability in the\\nmeasurements and low geographical variability in the quantity being estimated.\\nOur approach, in contrast to most competing methods, does not make the\\nassumption that there exists a prime instance responsible for the label in each\\nbag. Instead, we treat each bag as a set (i.e, an unordered sequence) of\\ninstances and learn to map each bag to its unique label by using all the\\ninstances in each bag. This is done by implementing an order-invariant\\noperation characterized by a particular type of attention mechanism. This\\nmethod is very flexible as it does not require domain knowledge nor does it\\nmake any assumptions about the distribution of the instances within each bag.\\nWe test our algorithm on five real world datasets and outperform previous\\nstate-of-the-art on three of the datasets. In addition, we augment our feature\\nspace by adding the moments of each feature for each bag, as extra features,\\nand show that while the first moments lead to higher accuracy, there is a\\ndiminishing return.'\n",
            " b'Applying machine learning to molecules is challenging because of their\\nnatural representation as graphs rather than vectors.Several architectures have\\nbeen recently proposed for deep learning from molecular graphs, but they suffer\\nfrom informationbottlenecks because they only pass information from a graph\\nnode to its direct neighbors. Here, we introduce a more expressiveroute-based\\nmulti-attention mechanism that incorporates features from routes between node\\npairs. We call the resulting methodGraph Informer. A single network layer can\\ntherefore attend to nodes several steps away. We show empirically that the\\nproposedmethod compares favorably against existing approaches in two prediction\\ntasks: (1) 13C Nuclear Magnetic Resonance (NMR)spectra, improving the\\nstate-of-the-art with an MAE of 1.35 ppm and (2) predicting drug bioactivity\\nand toxicity. Additionally, wedevelop a variant called injective Graph Informer\\nthat isprovablyas powerful as the Weisfeiler-Lehman test for graph\\nisomorphism.Furthermore, we demonstrate that the route information allows the\\nmethod to be informed about thenonlocal topologyof the graphand, thus, even go\\nbeyond the capabilities of the Weisfeiler-Lehman test.'\n",
            " b'We explore encoding brain symmetry into a neural network for a brain tumor\\nsegmentation task. A healthy human brain is symmetric at a high level of\\nabstraction, and the high-level asymmetric parts are more likely to be tumor\\nregions. Paying more attention to asymmetries has the potential to boost the\\nperformance in brain tumor segmentation. We propose a method to encode brain\\nsymmetry into existing neural networks and apply the method to a\\nstate-of-the-art neural network for medical imaging segmentation. We evaluate\\nour symmetry-encoded network on the dataset from a brain tumor segmentation\\nchallenge and verify that the new model extracts information in the training\\nimages more efficiently than the original model.'\n",
            " b'The deep convolutional neural networks have achieved significant improvements\\nin accuracy and speed for single image super-resolution. However, as the depth\\nof network grows, the information flow is weakened and the training becomes\\nharder and harder. On the other hand, most of the models adopt a single-stream\\nstructure with which integrating complementary contextual information under\\ndifferent receptive fields is difficult. To improve information flow and to\\ncapture sufficient knowledge for reconstructing the high-frequency details, we\\npropose a cascaded multi-scale cross network (CMSC) in which a sequence of\\nsubnetworks is cascaded to infer high resolution features in a coarse-to-fine\\nmanner. In each cascaded subnetwork, we stack multiple multi-scale cross (MSC)\\nmodules to fuse complementary multi-scale information in an efficient way as\\nwell as to improve information flow across the layers. Meanwhile, by\\nintroducing residual-features learning in each stage, the relative information\\nbetween high-resolution and low-resolution features is fully utilized to\\nfurther boost reconstruction performance. We train the proposed network with\\ncascaded-supervision and then assemble the intermediate predictions of the\\ncascade to achieve high quality image reconstruction. Extensive quantitative\\nand qualitative evaluations on benchmark datasets illustrate the superiority of\\nour proposed method over state-of-the-art super-resolution methods.'\n",
            " b'Variational inference has had great success in scaling approximate Bayesian\\ninference to big data by exploiting mini-batch training. To date, however, this\\nstrategy has been most applicable to models of independent data. We propose an\\nextension to state space models of time series data based on a novel generative\\nmodel for latent temporal states: the neural moving average model. This permits\\na subsequence to be sampled without drawing from the entire distribution,\\nenabling training iterations to use mini-batches of the time series at low\\ncomputational cost. We illustrate our method on autoregressive, Lotka-Volterra,\\nFitzHugh-Nagumo and stochastic volatility models, achieving accurate parameter\\nestimation in a short time.'\n",
            " b'Once an academic venture, autonomous driving has received unparalleled\\ncorporate funding in the last decade. Still, the operating conditions of\\ncurrent autonomous cars are mostly restricted to ideal scenarios. This means\\nthat driving in challenging illumination conditions such as night, sunrise, and\\nsunset remains an open problem. In these cases, standard cameras are being\\npushed to their limits in terms of low light and high dynamic range\\nperformance. To address these challenges, we propose, DSEC, a new dataset that\\ncontains such demanding illumination conditions and provides a rich set of\\nsensory data. DSEC offers data from a wide-baseline stereo setup of two color\\nframe cameras and two high-resolution monochrome event cameras. In addition, we\\ncollect lidar data and RTK GPS measurements, both hardware synchronized with\\nall camera data. One of the distinctive features of this dataset is the\\ninclusion of high-resolution event cameras. Event cameras have received\\nincreasing attention for their high temporal resolution and high dynamic range\\nperformance. However, due to their novelty, event camera datasets in driving\\nscenarios are rare. This work presents the first high-resolution, large-scale\\nstereo dataset with event cameras. The dataset contains 53 sequences collected\\nby driving in a variety of illumination conditions and provides ground truth\\ndisparity for the development and evaluation of event-based stereo algorithms.'\n",
            " b\"Deep learning (DL) 3D dose prediction has recently gained a lot of attention.\\nHowever, the variability of plan quality in the training dataset, generated\\nmanually by planners with wide range of expertise, can dramatically effect the\\nquality of the final predictions. Moreover, any changes in the clinical\\ncriteria requires a new set of manually generated plans by planners to build a\\nnew prediction model. In this work, we instead use consistent plans generated\\nby our in-house automated planning system (named ``ECHO'') to train the DL\\nmodel. ECHO (expedited constrained hierarchical optimization) generates\\nconsistent/unbiased plans by solving large-scale constrained optimization\\nproblems sequentially. If the clinical criteria changes, a new training data\\nset can be easily generated offline using ECHO, with no or limited human\\nintervention, making the DL-based prediction model easily adaptable to the\\nchanges in the clinical practice. We used 120 conventional lung patients (100\\nfor training, 20 for testing) with different beam configurations and trained\\nour DL-model using manually-generated as well as automated ECHO plans. We\\nevaluated different inputs: (1) CT+(PTV/OAR)contours, and (2) CT+contours+beam\\nconfigurations, and different loss functions: (1) MAE (mean absolute error),\\nand (2) MAE+DVH (dose volume histograms). The quality of the predictions was\\ncompared using different DVH metrics as well as dose-score and DVH-score,\\nrecently introduced by the AAPM knowledge-based planning grand challenge. The\\nbest results were obtained using automated ECHO plans and CT+contours+beam as\\ntraining inputs and MAE+DVH as loss function.\"\n",
            " b'Unsupervised feature learning has shown impressive results for a wide range\\nof input modalities, in particular for object classification tasks in computer\\nvision. Using a large amount of unlabeled data, unsupervised feature learning\\nmethods are utilized to construct high-level representations that are\\ndiscriminative enough for subsequently trained supervised classification\\nalgorithms. However, it has never been \\\\emph{quantitatively} investigated yet\\nhow well unsupervised learning methods can find \\\\emph{low-level\\nrepresentations} for image patches without any additional supervision. In this\\npaper we examine the performance of pure unsupervised methods on a low-level\\ncorrespondence task, a problem that is central to many Computer Vision\\napplications. We find that a special type of Restricted Boltzmann Machines\\n(RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple\\nbinarization scheme produces compact representations that perform better than\\nseveral state-of-the-art descriptors.'], shape=(128,), dtype=string)\n",
            "Labels: tf.Tensor(\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]], shape=(128, 164), dtype=int64)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "d409c7bf",
      "metadata": {
        "id": "d409c7bf",
        "outputId": "8d2499b6-3f62-4300-b46c-0aa65fe7f151",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract: b\"Tracking the full skeletal pose of the hands and fingers is a challenging\\nproblem that has a plethora of applications for user interaction. Existing\\ntechniques either require wearable hardware, add restrictions to user pose, or\\nrequire significant computation resources. This research explores a new\\napproach to tracking hands, or any articulated model, by using an augmented\\nrigid body simulation. This allows us to phrase 3D object tracking as a linear\\ncomplementarity problem with a well-defined solution. Based on a depth sensor's\\nsamples, the system generates constraints that limit motion orthogonal to the\\nrigid body model's surface. These constraints, along with prior motion,\\ncollision/contact constraints, and joint mechanics, are resolved with a\\nprojected Gauss-Seidel solver. Due to camera noise properties and attachment\\nerrors, the numerous surface constraints are impulse capped to avoid\\noverpowering mechanical constraints. To improve tracking accuracy, multiple\\nsimulations are spawned at each frame and fed a variety of heuristics,\\nconstraints and poses. A 3D error metric selects the best-fit simulation,\\nhelping the system handle challenging hand motions. Such an approach enables\\nreal-time, robust, and accurate 3D skeletal tracking of a user's hand on a\\nvariety of depth cameras, while only utilizing a single x86 CPU core for\\nprocessing.\"\n",
            "Label(s): ['I.3.7', 'cs.CV', 'cs.GR']\n",
            " \n",
            "Abstract: b'The recent improvements of graphics processing units (GPU) offer to the\\ncomputer vision community a powerful processing platform. Indeed, a lot of\\nhighly-parallelizable computer vision problems can be significantly accelerated\\nusing GPU architecture. Among these algorithms, the k nearest neighbor search\\n(KNN) is a well-known problem linked with many applications such as\\nclassification, estimation of statistical properties, etc. The main drawback of\\nthis task lies in its computation burden, as it grows polynomially with the\\ndata size. In this paper, we show that the use of the NVIDIA CUDA API\\naccelerates the search for the KNN up to a factor of 120.'\n",
            "Label(s): ['cs.CV', 'cs.DC']\n",
            " \n",
            "Abstract: b'In this work, we propose a novel approach that predicts the relationships\\nbetween various entities in an image in a weakly supervised manner by relying\\non image captions and object bounding box annotations as the sole source of\\nsupervision. Our proposed approach uses a top-down attention mechanism to align\\nentities in captions to objects in the image, and then leverage the syntactic\\nstructure of the captions to align the relations. We use these alignments to\\ntrain a relation classification network, thereby obtaining both grounded\\ncaptions and dense relationships. We demonstrate the effectiveness of our model\\non the Visual Genome dataset by achieving a recall@50 of 15% and recall@100 of\\n25% on the relationships present in the image. We also show that the model\\nsuccessfully predicts relations that are not present in the corresponding\\ncaptions.'\n",
            "Label(s): ['cs.CL', 'cs.CV', 'cs.LG']\n",
            " \n",
            "Abstract: b'The bias-variance trade-off is a central concept in supervised learning. In\\nclassical statistics, increasing the complexity of a model (e.g., number of\\nparameters) reduces bias but also increases variance. Until recently, it was\\ncommonly believed that optimal performance is achieved at intermediate model\\ncomplexities which strike a balance between bias and variance. Modern Deep\\nLearning methods flout this dogma, achieving state-of-the-art performance using\\n\"over-parameterized models\" where the number of fit parameters is large enough\\nto perfectly fit the training data. As a result, understanding bias and\\nvariance in over-parameterized models has emerged as a fundamental problem in\\nmachine learning. Here, we use methods from statistical physics to derive\\nanalytic expressions for bias and variance in two minimal models of\\nover-parameterization (linear regression and two-layer neural networks with\\nnonlinear data distributions), allowing us to disentangle properties stemming\\nfrom the model architecture and random sampling of data. In both models,\\nincreasing the number of fit parameters leads to a phase transition where the\\ntraining error goes to zero and the test error diverges as a result of the\\nvariance (while the bias remains finite). Beyond this threshold in the\\ninterpolation regime, the training error remains zero while the test error\\ndecreases. We also show that in contrast with classical intuition,\\nover-parameterized models can overfit even in the absence of noise and exhibit\\nbias even if the student and teacher models match. We synthesize these results\\nto construct a holistic understanding of generalization error and the\\nbias-variance trade-off in over-parameterized models and relate our results to\\nrandom matrix theory.'\n",
            "Label(s): ['cond-mat.dis-nn', 'cs.LG', 'stat.ML']\n",
            " \n",
            "Abstract: b'Sensor technologies are becoming increasingly prevalent in the biomedical\\nfield, with applications ranging from telemonitoring of people at risk, to\\nusing sensor derived information as objective endpoints in clinical trials. To\\nfully utilize sensor information, signals from distinct sensors often have to\\nbe temporally aligned. However, due to imperfect oscillators and significant\\nnoise, commonly encountered with biomedical signals, temporal alignment of raw\\nsignals is an all but trivial problem, with, to-date, no generally applicable\\nsolution. In this work, we present Deep Canonical Correlation Alignment (DCCA),\\na novel, generally applicable solution for the temporal alignment of raw\\n(biomedical) sensor signals. DCCA allows practitioners to directly align raw\\nsignals, from distinct sensors, without requiring deep domain knowledge. On a\\nselection of artificial and real datasets, we demonstrate the performance and\\nutility of DCCA under a variety of conditions. We compare the DCCA algorithm to\\nother warping based methods, DCCA outperforms dynamic time warping and cross\\ncorrelation based methods by an order of magnitude in terms of alignment error.\\nDCCA performs especially well on almost periodic biomedical signals such as\\nheart-beats and breathing patterns. In comparison to existing approaches, that\\nare not tailored towards raw sensor data, DCCA is not only fast enough to work\\non signals with billions of data points but also provides automatic filtering\\nand transformation functionalities, allowing it to deal with very noisy and\\neven morphologically distinct signals.'\n",
            "Label(s): ['cs.LG']\n",
            " \n"
          ]
        }
      ],
      "source": [
        "# Define a function to invert the multi-hot encoded labels\n",
        "def invert_multi_hot(label):\n",
        "    return [vocab[i] for i, val in enumerate(label) if val == 1]\n",
        "\n",
        "# Iterate through batches of the training dataset and print the abstract text along with the corresponding labels\n",
        "text_batch, label_batch = next(iter(train_dataset))\n",
        "for i, text in enumerate(text_batch[:5]):\n",
        "    label = label_batch[i].numpy()\n",
        "    print(f\"Abstract: {text}\")\n",
        "    print(f\"Label(s): {invert_multi_hot(label)}\")\n",
        "    print(\" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "3881b07a",
      "metadata": {
        "id": "3881b07a",
        "outputId": "16dd78a3-b64b-49b4-e83b-4195c041aaa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 158943\n"
          ]
        }
      ],
      "source": [
        "# Creating vocabulary with unique words\n",
        "vocabulary = set()\n",
        "train_df['abstracts'].apply(lambda x: vocabulary.update(x.lower().split()))\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(\"Vocabulary size:\", vocabulary_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WiP2bJ-Bfgur"
      },
      "id": "WiP2bJ-Bfgur",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4f8c291e",
      "metadata": {
        "id": "4f8c291e"
      },
      "source": [
        "# Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "13305e60",
      "metadata": {
        "id": "13305e60"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Initializing a TextVectorization layer\n",
        "text_vectorizer = TextVectorization(max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf-idf\")\n",
        "\n",
        "# TextVectorization layer needs to be adapted with the vocabulary from our training set.\n",
        "text_vectorizer.adapt(train_df[\"abstracts\"].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping Vectorization to Datasets\n",
        "train_dataset = train_dataset.map(lambda abstracts, terms: (text_vectorizer(abstracts), terms), num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation_dataset = validation_dataset.map(lambda abstracts, terms: (text_vectorizer(abstracts), terms), num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(lambda abstracts, terms: (text_vectorizer(abstracts), terms), num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "p1b7h8eppF38"
      },
      "id": "p1b7h8eppF38",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assuming you have a train_dataset TensorFlow dataset object\n",
        "\n",
        "# Define the number of elements to inspect\n",
        "num_elements = 5  # You can change this to inspect more or fewer elements\n",
        "\n",
        "# Take the first num_elements elements from the dataset\n",
        "sample_data = train_dataset.take(num_elements)\n",
        "\n",
        "# Iterate over the sample_data and print each element\n",
        "for sample in sample_data:\n",
        "    # Assuming the sample contains input features and labels\n",
        "    input_features, labels = sample\n",
        "    print(\"Input Features:\")\n",
        "    print(input_features)\n",
        "    print(\"Labels:\")\n",
        "    print(labels)\n",
        "    print(\"=\"*50)  # Separating each sample with '=' characters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BipE69L64efk",
        "outputId": "aa0b0cbb-7f5a-479b-924b-62a31ee8f2e6"
      },
      "id": "BipE69L64efk",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[298.32437      5.564707     3.5081503  ...   0.           0.\n",
            "    0.        ]\n",
            " [298.32437      7.651472     2.8065202  ...   0.           0.\n",
            "    0.        ]\n",
            " [133.4609       2.086765     0.70163006 ...   0.           0.\n",
            "    0.        ]\n",
            " ...\n",
            " [423.93463     11.129414     5.6130404  ...   0.           0.\n",
            "    0.        ]\n",
            " [282.62308      2.086765     4.20978    ...   0.           0.\n",
            "    0.        ]\n",
            " [345.42822      4.8691187    2.8065202  ...   0.           0.\n",
            "    0.        ]], shape=(128, 158943), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n",
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[423.93463     5.564707    2.8065202 ...   0.          0.\n",
            "    0.       ]\n",
            " [243.36987     5.564707    2.8065202 ...   0.          0.\n",
            "    0.       ]\n",
            " [463.18784     6.9558835   7.0163007 ...   0.          0.\n",
            "    0.       ]\n",
            " ...\n",
            " [400.3827      6.9558835   3.5081503 ...   0.          0.\n",
            "    0.       ]\n",
            " [455.3372     12.52059     4.9114103 ...   0.          0.\n",
            "    0.       ]\n",
            " [321.87628    13.911767    9.822821  ...   0.          0.\n",
            "    0.       ]], shape=(128, 158943), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n",
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[282.62308      2.086765     0.70163006 ...   0.           0.\n",
            "    0.        ]\n",
            " [447.48654      7.651472     3.5081503  ...   0.           0.\n",
            "    0.        ]\n",
            " [306.17502      6.9558835    5.6130404  ...   0.           0.\n",
            "    0.        ]\n",
            " ...\n",
            " [384.68143      4.8691187    3.5081503  ...   0.           0.\n",
            "    0.        ]\n",
            " [266.9218       5.564707     2.8065202  ...   0.           0.\n",
            "    0.        ]\n",
            " [392.53207      4.8691187    4.9114103  ...   0.           0.\n",
            "    0.        ]], shape=(128, 158943), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n",
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[266.9218     11.129414    5.6130404 ...   0.          0.\n",
            "    0.       ]\n",
            " [282.62308     6.260295    3.5081503 ...   0.          0.\n",
            "    0.       ]\n",
            " [361.1295      2.7823534   4.9114103 ...   0.          0.\n",
            "    0.       ]\n",
            " ...\n",
            " [329.72693     4.8691187   4.20978   ...   0.          0.\n",
            "    0.       ]\n",
            " [ 70.65577     2.7823534   2.10489   ...   0.          0.\n",
            "    0.       ]\n",
            " [494.5904     11.129414    2.8065202 ...   0.          0.\n",
            "    0.       ]], shape=(128, 158943), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n",
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[259.07117     6.260295    2.10489   ...   0.          0.\n",
            "    0.       ]\n",
            " [337.57758     5.564707    4.20978   ...   0.          0.\n",
            "    0.       ]\n",
            " [455.3372     13.216179    6.3146706 ...   0.          0.\n",
            "    0.       ]\n",
            " ...\n",
            " [518.14233     6.260295    1.4032601 ...   0.          0.\n",
            "    0.       ]\n",
            " [164.86346     5.564707    3.5081503 ...   0.          0.\n",
            "    0.       ]\n",
            " [164.86346     2.7823534   2.10489   ...   0.          0.\n",
            "    0.       ]], shape=(128, 158943), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "6b82682c",
      "metadata": {
        "id": "6b82682c"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import tensorflow as tf\n",
        "\n",
        "# # Flatten the lists in the \"terms\" column into strings\n",
        "# train_df['terms'] = train_df['terms'].apply(lambda x: ' '.join(x))\n",
        "# val_df['terms'] = val_df['terms'].apply(lambda x: ' '.join(x))\n",
        "# test_df['terms'] = test_df['terms'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# # Now train_df, val_df, and test_df have the \"terms\" column with strings instead of lists\n",
        "\n",
        "# # Now you can proceed with creating TensorFlow datasets\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((train_df['abstracts'].values, train_df['terms'].values, train_df['titles'].values))\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices((val_df['abstracts'].values, val_df['terms'].values, val_df['titles'].values))\n",
        "# test_dataset = tf.data.Dataset.from_tensor_slices((test_df['abstracts'].values, test_df['terms'].values, test_df['titles'].values))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85485df2",
      "metadata": {
        "id": "85485df2"
      },
      "source": [
        "# model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "1595fa1d",
      "metadata": {
        "scrolled": true,
        "id": "1595fa1d",
        "outputId": "53239e7a-4353-47a8-e8ce-4851d4a97fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "272/272 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.6029"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1917, in test_step\n        y_pred = self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_6' (type Sequential).\n    \n    Input 0 of layer \"dense_18\" is incompatible with the layer: expected min_ndim=2, found ndim=0. Full shape received: ()\n    \n    Call arguments received by layer 'sequential_6' (type Sequential):\n      • inputs=tf.Tensor(shape=(), dtype=string)\n      • training=False\n      • mask=None\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-b5dbfe94224a>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Evaluate the model on the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1917, in test_step\n        y_pred = self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_6' (type Sequential).\n    \n    Input 0 of layer \"dense_18\" is incompatible with the layer: expected min_ndim=2, found ndim=0. Full shape received: ()\n    \n    Call arguments received by layer 'sequential_6' (type Sequential):\n      • inputs=tf.Tensor(shape=(), dtype=string)\n      • training=False\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "\n",
        "# Create shallow MLP model for multi-label classification\n",
        "model = models.Sequential([\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(164, activation='sigmoid')  # Output layer with 164 neurons for multi-label classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccc3be2",
      "metadata": {
        "scrolled": true,
        "id": "1ccc3be2",
        "outputId": "dfdc6c05-4988-4149-ab75-ae1efb9a0d15"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plotting loss\n",
        "def plot_result(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_result(\"loss\")\n",
        "plot_result(\"binary_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9864d7ce",
      "metadata": {
        "id": "9864d7ce"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cebc1c9c",
      "metadata": {
        "id": "cebc1c9c",
        "outputId": "d1f03771-463d-4dcf-b1c6-bd2649c19175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 9s 553ms/step - loss: 0.0182 - binary_accuracy: 0.9946\n",
            "16/16 [==============================] - 9s 559ms/step - loss: 0.0183 - binary_accuracy: 0.9945\n",
            "Categorical accuracy on the test set: 99.46%.\n",
            "Categorical accuracy on the validation set: 99.45%.\n"
          ]
        }
      ],
      "source": [
        "# model evaltuation on test and val dataset\n",
        "_, binary_acc1 = model.evaluate(test_dataset)\n",
        "_, binary_acc2 = model.evaluate(validation_dataset)\n",
        "\n",
        "print(f\"Categorical accuracy on the test set: {round(binary_acc1 * 100, 2)}%.\")\n",
        "print(f\"Categorical accuracy on the validation set: {round(binary_acc2 * 100, 2)}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f447f1",
      "metadata": {
        "id": "16f447f1"
      },
      "source": [
        "# Save Model and Text Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4434b752",
      "metadata": {
        "id": "4434b752"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model1.save(\"models/model.h5\")\n",
        "\n",
        "# Save the configuration of the text vectorizer\n",
        "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
        "with open(\"models/text_vectorizer_config.pkl\", \"wb\") as f:\n",
        "    pickle.dump(saved_text_vectorizer_config, f)\n",
        "\n",
        "\n",
        "# Save the vocabulary\n",
        "with open(\"models/vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fceee66a",
      "metadata": {
        "id": "fceee66a"
      },
      "source": [
        "# Load Model and Text Vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34e7d7f6",
      "metadata": {
        "id": "34e7d7f6"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import pickle\n",
        "\n",
        "# Load the model\n",
        "loaded_model = keras.models.load_model(\"models/model.h5\")\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Load the configuration of the text vectorizer\n",
        "with open(\"models/text_vectorizer_config.pkl\", \"rb\") as f:\n",
        "    saved_text_vectorizer_config = pickle.load(f)\n",
        "\n",
        "# Create a new TextVectorization layer with the saved configuration\n",
        "loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
        "\n",
        "# Load the saved weights into the new TextVectorization layer\n",
        "with open(\"models/text_vectorizer_weights.pkl\", \"rb\") as f:\n",
        "    weights = pickle.load(f)\n",
        "    loaded_text_vectorizer.set_weights(weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e3fe39",
      "metadata": {
        "id": "27e3fe39"
      },
      "outputs": [],
      "source": [
        "# Load the vocabulary\n",
        "with open(\"models/vocab.pkl\", \"rb\") as f:\n",
        "    loaded_vocab = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "506f9a17",
      "metadata": {
        "id": "506f9a17"
      },
      "source": [
        "# Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2985e7",
      "metadata": {
        "id": "8e2985e7"
      },
      "outputs": [],
      "source": [
        "def invert_multi_hot(encoded_labels):\n",
        "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
        "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
        "    return np.take(loaded_vocab, hot_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f5b374",
      "metadata": {
        "id": "76f5b374"
      },
      "outputs": [],
      "source": [
        "def predict_category(abstract, model, vectorizer, label_lookup):\n",
        "    # Preprocess the abstract using the loaded text vectorizer\n",
        "    preprocessed_abstract = vectorizer([abstract])\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    predictions = model.predict(preprocessed_abstract)\n",
        "\n",
        "    # Convert predictions to human-readable labels\n",
        "    predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
        "\n",
        "    return predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f91a6e8",
      "metadata": {
        "id": "6f91a6e8",
        "outputId": "780fba18-868f-49e2-f43d-171f1ab1eb22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Predicted Categories: ['cs.LG']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "new_abstract = \"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\"\n",
        "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
        "print(\"Predicted Categories:\", predicted_categories)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13ef48a",
      "metadata": {
        "id": "e13ef48a",
        "outputId": "ce7dbf05-d1ed-4e5b-8c18-b9b6678ff57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 459ms/step\n",
            "Predicted Categories: ['cs.LG' 'cs.AI']\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "new_abstract = 'Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.'\n",
        "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
        "print(\"Predicted Categories:\", predicted_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77b90c6e",
      "metadata": {
        "id": "77b90c6e"
      },
      "outputs": [],
      "source": [
        "# great resutls..................................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5c90e37",
      "metadata": {
        "id": "b5c90e37"
      },
      "source": [
        "# =======Section 2========"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cecfe2d1",
      "metadata": {
        "id": "cecfe2d1"
      },
      "source": [
        "# 2 Recommendation System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "08b0ebec",
      "metadata": {
        "id": "08b0ebec"
      },
      "outputs": [],
      "source": [
        "arxiv_data.drop(columns = [\"terms\",\"abstracts\"], inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c94e6f2f",
      "metadata": {
        "id": "c94e6f2f"
      },
      "outputs": [],
      "source": [
        "arxiv_data.drop_duplicates(inplace= True)\n",
        "arxiv_data.reset_index(drop= True,inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8ca5e456",
      "metadata": {
        "id": "8ca5e456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "11db85d1-7f5a-4357-d3eb-3ce27148a439"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                 titles\n",
              "0      Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
              "1           Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
              "2                                                       Power up! Robust Graph Convolutional Network via Graph Powering\n",
              "3                                                  Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
              "4                                   Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
              "...                                                                                                                 ...\n",
              "41100              An experimental study of graph-based semi-supervised classification with additional node information\n",
              "41101                                                          Bayesian Differential Privacy through Posterior Sampling\n",
              "41102                                       Mining Spatio-temporal Data on Industrialization from Historical Registries\n",
              "41103                                                 Wav2Letter: an End-to-End ConvNet-based Speech Recognition System\n",
              "41104                                                                                       Generalized Low Rank Models\n",
              "\n",
              "[41105 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21b1c17e-c405-48a4-907c-524211c05bd6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Power up! Robust Graph Convolutional Network via Graph Powering</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Releasing Graph Neural Networks with Differential Privacy Guarantees</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41100</th>\n",
              "      <td>An experimental study of graph-based semi-supervised classification with additional node information</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41101</th>\n",
              "      <td>Bayesian Differential Privacy through Posterior Sampling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41102</th>\n",
              "      <td>Mining Spatio-temporal Data on Industrialization from Historical Registries</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41103</th>\n",
              "      <td>Wav2Letter: an End-to-End ConvNet-based Speech Recognition System</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41104</th>\n",
              "      <td>Generalized Low Rank Models</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41105 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21b1c17e-c405-48a4-907c-524211c05bd6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21b1c17e-c405-48a4-907c-524211c05bd6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21b1c17e-c405-48a4-907c-524211c05bd6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-af615d60-0746-41b2-be50-8fefe8bc0a2b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-af615d60-0746-41b2-be50-8fefe8bc0a2b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-af615d60-0746-41b2-be50-8fefe8bc0a2b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f752777e-9eec-471c-bd15-9a2e15c2c485\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('arxiv_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f752777e-9eec-471c-bd15-9a2e15c2c485 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('arxiv_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "arxiv_data",
              "summary": "{\n  \"name\": \"arxiv_data\",\n  \"rows\": 41105,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41105,\n        \"samples\": [\n          \"Semi-supervised Federated Learning for Activity Recognition\",\n          \"SATR-DL: Improving Surgical Skill Assessment and Task Recognition in Robot-assisted Surgery with Deep Neural Networks\",\n          \"A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "arxiv_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b71915",
      "metadata": {
        "id": "95b71915"
      },
      "source": [
        "# Sentence Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5712474f",
      "metadata": {
        "id": "5712474f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "514a5734",
      "metadata": {
        "id": "514a5734",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b3ec65-cac4-4e73-bea2-ab17b8047eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "import pickle\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Example sentences (assuming `arxiv_data` is defined)\n",
        "sentences = arxiv_data[\"titles\"]\n",
        "\n",
        "# Tokenization and normalization\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "normalized_sentences = [[word for word in sentence if word.isalnum() and word not in stop_words] for sentence in tokenized_sentences]\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(normalized_sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# Saving the Word2Vec model\n",
        "with open('word2vec_model.pkl', 'wb') as f:\n",
        "    pickle.dump(word2vec_model, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c502c4e0",
      "metadata": {
        "id": "c502c4e0"
      },
      "outputs": [],
      "source": [
        "# Function to generate sentence embeddings by averaging word embeddings\n",
        "def sentence_embedding(sentence, model):\n",
        "    vectors = []\n",
        "    for word in sentence:\n",
        "        if word in model.wv:\n",
        "            vectors.append(model.wv[word])\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Generate sentence embeddings\n",
        "all_sentence_embeddings = [sentence_embedding(sentence, word2vec_model) for sentence in normalized_sentences]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db125c7d",
      "metadata": {
        "id": "db125c7d"
      },
      "source": [
        "# Why select all-MiniLM-L6-v2?\n",
        "\n",
        "All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs. Source\n",
        "\n",
        "Its small in size 80 MB with good performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "355ffc70",
      "metadata": {
        "id": "355ffc70"
      },
      "source": [
        "# Print the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "0af0618d",
      "metadata": {
        "id": "0af0618d"
      },
      "outputs": [],
      "source": [
        "# c = 0\n",
        "# # Iterate over pairs of sentences and their corresponding embeddings\n",
        "# for sentence, normalized_sentence in zip(sentences, normalized_sentences):\n",
        "#     # Generate sentence embedding\n",
        "#     vectors = []\n",
        "#     for word in normalized_sentence:\n",
        "#         if word in word2vec_model.wv:\n",
        "#             vectors.append(word2vec_model.wv[word])\n",
        "#     if vectors:\n",
        "#         sentence_embedding = np.mean(vectors, axis=0)\n",
        "#     else:\n",
        "#         sentence_embedding = np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "#     # Print information\n",
        "#     print(\"Sentence:\", sentence)\n",
        "#     print(\"Embedding length:\", len(sentence_embedding)) # list of floats\n",
        "#     print(\"\")\n",
        "\n",
        "#     # Break out of the loop after printing information for the first 5 sentences\n",
        "#     if c >= 5:\n",
        "#         break\n",
        "#     c += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5cdb484",
      "metadata": {
        "id": "f5cdb484"
      },
      "source": [
        "# Save files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "a802f9f1",
      "metadata": {
        "id": "a802f9f1"
      },
      "outputs": [],
      "source": [
        "# Saving sentences and corresponding embeddings\n",
        "with open('embeddings.pkl', 'wb') as f:\n",
        "    pickle.dump(all_sentence_embeddings, f)\n",
        "\n",
        "with open('sentences.pkl', 'wb') as f:\n",
        "    pickle.dump(sentences, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dcea57f",
      "metadata": {
        "id": "1dcea57f"
      },
      "source": [
        "# Recommendation for similar papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "dcb1a450",
      "metadata": {
        "id": "dcb1a450"
      },
      "outputs": [],
      "source": [
        "# Load saved files\n",
        "sentences = pickle.load(open('sentences.pkl','rb'))\n",
        "word2vec_model = pickle.load(open('word2vec_model.pkl','rb'))\n",
        "all_sentence_embeddings = pickle.load(open('embeddings.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "84ffe218",
      "metadata": {
        "id": "84ffe218"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def recommendation(input_paper, embeddings, sentences):\n",
        "    # Calculate embeddings for the input paper\n",
        "    input_embedding = sentence_embedding(input_paper, word2vec_model)\n",
        "\n",
        "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
        "    cosine_scores = cosine_similarity([input_embedding], embeddings)[0]\n",
        "\n",
        "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
        "    top_similar_papers_indices = np.argsort(cosine_scores)[::-1][:5]\n",
        "\n",
        "    # Retrieve the titles of the top similar papers.\n",
        "    papers_list = [sentences[i] for i in top_similar_papers_indices]\n",
        "\n",
        "    return papers_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "12c90388",
      "metadata": {
        "id": "12c90388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704b8bb9-f146-48ec-cc74-61b79c1e2910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Recommended Papers:\n",
            "1. A Neural Network for Semigroups\n",
            "2. SGAD: Soft-Guided Adaptively-Dropped Neural Network\n",
            "3. Neural Network Branch-and-Bound for Neural Network Verification\n",
            "4. MGIC: Multigrid-in-Channels Neural Network Architectures\n",
            "5. Which Minimizer Does My Neural Network Converge To?\n"
          ]
        }
      ],
      "source": [
        "input_paper = \"Neural Network\"\n",
        "input_paper_tokens = word_tokenize(input_paper.lower())\n",
        "recommendations = recommendation(input_paper_tokens, all_sentence_embeddings, sentences)\n",
        "print(\"Top 5 Recommended Papers:\")\n",
        "for i, paper in enumerate(recommendations, 1):\n",
        "    print(f\"{i}. {paper}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "026a1eb1",
      "metadata": {
        "id": "026a1eb1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "813491da",
      "metadata": {
        "id": "813491da"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://arxiv.org/search/?query={}&searchtype=all&abstracts=show&order=-announced_date_first&size=50\"\n",
        "\n",
        "final_urls = []\n",
        "titles = []\n",
        "abstracts = []\n",
        "for paper in recommendations:\n",
        "    query = '+'.join(paper.split())\n",
        "    url = base_url.format(query)\n",
        "    web = requests.get(url)\n",
        "    soup = BeautifulSoup(web.content,\"html.parser\")\n",
        "    links = soup.find_all(\"p\", attrs={'class':'list-title is-inline-block'})\n",
        "    for i in range(len(links)):\n",
        "      final_urls.append(links[i].find_all(\"a\")[0].get('href'))\n",
        "\n",
        "for i in range(len(final_urls)):\n",
        "  web2 = requests.get(final_urls[i])\n",
        "  soup2 = BeautifulSoup(web2.content,\"html.parser\")\n",
        "  title = soup2.find(\"h1\",attrs={'class':'title mathjax'}).text\n",
        "  title=title.strip(\"Title\").strip(':')\n",
        "  titles.append(title)\n",
        "  abstract = soup2.find('blockquote',attrs={'class':'abstract mathjax'})\n",
        "  abstract=abstract.text.strip('\\nAbstract:')\n",
        "  abstracts.append(abstract)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(titles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0pxkpFRqKil",
        "outputId": "2f58f04b-9528-4f1a-ef7e-41a091bc3211"
      },
      "id": "i0pxkpFRqKil",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abstracts[-7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "PA_hGt8GqQQz",
        "outputId": "4612d05c-a184-4a80-8e1b-67c5a674db56"
      },
      "id": "PA_hGt8GqQQz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Motif discovery is gaining increasing attention in the domain of functional data analysis. Functional motifs are typical \"shapes\" or \"patterns\" that recur multiple times in different portions of a single curve and/or in misaligned portions of multiple curves. In this paper, we define functional motifs using an additive model and we propose funBIalign for their discovery and evaluation. Inspired by clustering and biclustering techniques, funBIalign is a multi-step procedure which uses agglomerative hierarchical clustering with complete linkage and a functional distance based on mean squared residue scores to discover functional motifs, both in a single curve (e.g., time series) and in a set of curves. We assess its performance and compare it to other recent methods through extensive simulations. Moreover, we use funBIalign for discovering motifs in two real-data case studies; one on food price inflation and one on temperature changes.\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_urls[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FQA5rvqbqX75",
        "outputId": "3767e2ec-18fb-4b5e-d10c-8f3d2775ccb6"
      },
      "id": "FQA5rvqbqX75",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://arxiv.org/abs/2403.08011'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4jsXBoRqZvK"
      },
      "id": "r4jsXBoRqZvK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4AGoqN4qbES"
      },
      "id": "T4AGoqN4qbES",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86GG6OXNqk2m"
      },
      "id": "86GG6OXNqk2m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}