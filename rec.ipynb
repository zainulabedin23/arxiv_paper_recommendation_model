{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxrdKrWzSYRZVCXEWzWoxc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panik-79/arxiv_paper_recommendation_model/blob/main/rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "FietXq627-fu"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from ast import literal_eval\n",
        "# is used for safely evaluating strings containing Python literals or container displays\n",
        "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv_data = pd.read_csv(\"arxiv_data_210930-054931.csv\")"
      ],
      "metadata": {
        "id": "ax7C1je18Cvu"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove duplicate entries based on the \"titles\" (terms) column\n",
        "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
        "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
        "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
        "# There are some terms with occurrence as low as 1.\n",
        "print(sum(arxiv_data['terms'].value_counts()==1))\n",
        "# how many unique terms\n",
        "print(arxiv_data['terms'].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Iv60ewBlOM",
        "outputId": "6badacb1-b7b6-4cfb-85ea-c57148b06995"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 41105 rows in the deduplicated dataset.\n",
            "2503\n",
            "3401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting unique labels\n",
        "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
        "labels = labels_column.explode().unique()\n",
        "print(\"labels :\",labels)\n",
        "print(\"lenght :\",len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j_CX2KjBnpj",
        "outputId": "8bf0a658-fa98-4751-cd3b-ead094899927"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels : ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
            " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
            "lenght : 1177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
        "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
        "arxiv_data_filtered.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiWO1b_1BsML",
        "outputId": "3ea5bba8-50b9-4bab-be91-67371bcf53f6"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38602, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
        "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
        "arxiv_data_filtered['terms'].values[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9l6wVAlBvYn",
        "outputId": "1d15678c-1303-4e8d-87ae-02e12f6ad3e5"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
              "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_split = 0.25\n",
        "\n",
        "# Initial train and test split.\n",
        "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
        "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
        "\n",
        "# Splitting the test set further into validation\n",
        "# and new test sets.\n",
        "val_df = test_df.sample(frac=0.5)\n",
        "test_df.drop(val_df.index, inplace=True)\n",
        "\n",
        "print(f\"Number of rows in training set: {len(train_df)}\")\n",
        "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
        "print(f\"Number of rows in test set: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qni_RUsUBxCa",
        "outputId": "bb09fb64-e435-4360-dab4-89862f8b09d0"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in training set: 28951\n",
            "Number of rows in validation set: 4826\n",
            "Number of rows in test set: 4825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Convert the terms column to a list of lists\n",
        "terms_list = train_df['terms'].tolist()\n",
        "\n",
        "# Initialize MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Fit MultiLabelBinarizer on the terms list to build the vocabulary\n",
        "mlb.fit(terms_list)\n",
        "\n",
        "# Transform the terms list into a multi-hot encoded representation\n",
        "multi_hot_encoded = mlb.transform(terms_list)\n",
        "\n",
        "# Get the vocabulary (terms)\n",
        "vocab = mlb.classes_\n",
        "\n",
        "print(\"Vocabulary:\\n\")\n",
        "print(vocab)\n",
        "len(vocab)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HfUu05kB-tw",
        "outputId": "3847fc84-1f76-46e1-a67b-797265fb7bc7"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary:\n",
            "\n",
            "['14J60 (Primary) 14F05, 14J26 (Secondary)' '60L10, 60L20' '62H30' '62H35'\n",
            " '62H99' '65D19' '68' '68Q32' '68T01' '68T05' '68T07' '68T10' '68T30'\n",
            " '68T45' '68T99' '68Txx' '68U01' '68U10'\n",
            " 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3' 'F.2.2; I.2.7' 'G.3'\n",
            " 'H.3.1; H.3.3; I.2.6; I.2.7' 'H.3.1; I.2.6; I.2.7' 'I.2' 'I.2.0; I.2.6'\n",
            " 'I.2.1' 'I.2.10' 'I.2.10; I.2.6' 'I.2.10; I.4.8' 'I.2.10; I.4.8; I.5.4'\n",
            " 'I.2.10; I.4; I.5' 'I.2.10; I.5.1; I.4.8' 'I.2.1; J.3' 'I.2.6'\n",
            " 'I.2.6, I.5.4' 'I.2.6; I.2.10' 'I.2.6; I.2.7'\n",
            " 'I.2.6; I.2.7; H.3.1; H.3.3' 'I.2.6; I.2.8' 'I.2.6; I.2.9' 'I.2.6; I.5.1'\n",
            " 'I.2.6; I.5.4' 'I.2.7' 'I.2.8' 'I.2; I.2.6; I.2.7' 'I.2; I.4; I.5'\n",
            " 'I.2; I.5' 'I.2; J.2' 'I.3.7' 'I.4' 'I.4.0' 'I.4.1' 'I.4.3' 'I.4.4'\n",
            " 'I.4.5' 'I.4.6' 'I.4.6; I.4.8' 'I.4.8' 'I.4.9' 'I.4.9; I.5.4' 'I.4; I.5'\n",
            " 'I.5.2' 'I.5.4' 'K.3.2' 'astro-ph.IM' 'cond-mat.dis-nn'\n",
            " 'cond-mat.mtrl-sci' 'cond-mat.soft' 'cond-mat.stat-mech' 'cs.AI' 'cs.AR'\n",
            " 'cs.CC' 'cs.CE' 'cs.CG' 'cs.CL' 'cs.CR' 'cs.CV' 'cs.CY' 'cs.DB' 'cs.DC'\n",
            " 'cs.DL' 'cs.DM' 'cs.DS' 'cs.ET' 'cs.FL' 'cs.GR' 'cs.GT' 'cs.HC' 'cs.IR'\n",
            " 'cs.IT' 'cs.LG' 'cs.LO' 'cs.MA' 'cs.MM' 'cs.MS' 'cs.NA' 'cs.NE' 'cs.NI'\n",
            " 'cs.PF' 'cs.PL' 'cs.RO' 'cs.SC' 'cs.SD' 'cs.SE' 'cs.SI' 'cs.SY' 'econ.EM'\n",
            " 'econ.GN' 'eess.AS' 'eess.IV' 'eess.SP' 'eess.SY' 'hep-ex' 'hep-ph'\n",
            " 'math-ph' 'math.AP' 'math.AT' 'math.CO' 'math.DS' 'math.FA' 'math.IT'\n",
            " 'math.LO' 'math.MP' 'math.NA' 'math.OC' 'math.PR' 'math.SP' 'math.ST'\n",
            " 'nlin.AO' 'nlin.CD' 'physics.ao-ph' 'physics.app-ph' 'physics.bio-ph'\n",
            " 'physics.chem-ph' 'physics.class-ph' 'physics.comp-ph' 'physics.data-an'\n",
            " 'physics.flu-dyn' 'physics.geo-ph' 'physics.med-ph' 'physics.optics'\n",
            " 'physics.plasm-ph' 'physics.soc-ph' 'q-bio.BM' 'q-bio.GN' 'q-bio.MN'\n",
            " 'q-bio.NC' 'q-bio.OT' 'q-bio.PE' 'q-bio.QM' 'q-bio.TO' 'q-fin.CP'\n",
            " 'q-fin.EC' 'q-fin.GN' 'q-fin.PM' 'q-fin.RM' 'q-fin.ST' 'q-fin.TR'\n",
            " 'quant-ph' 'stat.AP' 'stat.CO' 'stat.ME' 'stat.ML' 'stat.TH']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuaX2Wf6Cncv",
        "outputId": "6f3e6d3e-2d53-465e-a660-8fd36309e8dc"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "len(arxiv_data_filtered)\n",
        "arxiv_data_filtered['abstracts'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "8apDQjqjCveU",
        "outputId": "6b38c193-ad85-4117-f96f-84eb6297b72e"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 259
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "corpus_train = []\n",
        "\n",
        "for abstract in train_df['abstracts']:\n",
        "    # Remove newline characters and lowercasing\n",
        "    abstract = re.sub('\\n', ' ', abstract).lower()\n",
        "    abstract = re.sub('[^a-zA-Z]', ' ', abstract)\n",
        "    # Tokenize the abstract\n",
        "    words = abstract.split()\n",
        "    # Stemming and removing stopwords\n",
        "    stemmed_abstract = [ps.stem(word) for word in words if word not in stop_words]\n",
        "    # Join the stemmed words back into a sentence\n",
        "    stemmed_abstract = ' '.join(stemmed_abstract)\n",
        "    corpus_train.append(stemmed_abstract)"
      ],
      "metadata": {
        "id": "a4Bo4TrtCylL"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nGPetl7E-S1x"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binarize all labels using the MultiLabelBinarizer\n",
        "train_labels_binarized = mlb.transform(train_df[\"terms\"])\n",
        "test_labels_binarized = mlb.transform(test_df[\"terms\"])\n",
        "val_labels_binarized = mlb.transform(val_df[\"terms\"])"
      ],
      "metadata": {
        "id": "FWKWHpFhC1H6"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7ANSvlL7hhu"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"abstracts\"] = corpus_train"
      ],
      "metadata": {
        "id": "2GtnWwuREySR"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "kle3KW05FRXQ",
        "outputId": "307e1f84-b949-4ef1-a59a-613114870e6b"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  terms  \\\n",
              "15787                           [cs.CV]   \n",
              "55156                    [cs.CV, cs.LG]   \n",
              "21925                           [cs.LG]   \n",
              "16723  [stat.ML, cs.LG, cs.NE, stat.ME]   \n",
              "37562                           [cs.CV]   \n",
              "\n",
              "                                                  titles  \\\n",
              "15787  Fingerprint Presentation Attack Detection: A S...   \n",
              "55156  Making Better Mistakes: Leveraging Class Hiera...   \n",
              "21925  A Generative Model to Synthesize EEG Data for ...   \n",
              "16723  Scalable Out-of-Sample Extension of Graph Embe...   \n",
              "37562  Depth Based Semantic Scene Completion with Pos...   \n",
              "\n",
              "                                               abstracts  \n",
              "15787  vulner autom fingerprint recognit system prese...  \n",
              "55156  deep neural network improv imag classif dramat...  \n",
              "21925  predict seizur occur vital bring normalci live...  \n",
              "16723  sever popular graph embed techniqu represent l...  \n",
              "37562  semant scene complet ssc refer task infer sema...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-895bbc5f-a393-48b3-9365-1bb68437db60\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>terms</th>\n",
              "      <th>titles</th>\n",
              "      <th>abstracts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15787</th>\n",
              "      <td>[cs.CV]</td>\n",
              "      <td>Fingerprint Presentation Attack Detection: A S...</td>\n",
              "      <td>vulner autom fingerprint recognit system prese...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55156</th>\n",
              "      <td>[cs.CV, cs.LG]</td>\n",
              "      <td>Making Better Mistakes: Leveraging Class Hiera...</td>\n",
              "      <td>deep neural network improv imag classif dramat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21925</th>\n",
              "      <td>[cs.LG]</td>\n",
              "      <td>A Generative Model to Synthesize EEG Data for ...</td>\n",
              "      <td>predict seizur occur vital bring normalci live...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16723</th>\n",
              "      <td>[stat.ML, cs.LG, cs.NE, stat.ME]</td>\n",
              "      <td>Scalable Out-of-Sample Extension of Graph Embe...</td>\n",
              "      <td>sever popular graph embed techniqu represent l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37562</th>\n",
              "      <td>[cs.CV]</td>\n",
              "      <td>Depth Based Semantic Scene Completion with Pos...</td>\n",
              "      <td>semant scene complet ssc refer task infer sema...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-895bbc5f-a393-48b3-9365-1bb68437db60')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-895bbc5f-a393-48b3-9365-1bb68437db60 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-895bbc5f-a393-48b3-9365-1bb68437db60');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-09f14206-e665-4c47-b753-7284ac632f0c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-09f14206-e665-4c47-b753-7284ac632f0c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-09f14206-e665-4c47-b753-7284ac632f0c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 28951,\n  \"fields\": [\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28951,\n        \"samples\": [\n          \"Diversified Mutual Learning for Deep Metric Learning\",\n          \"On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift\",\n          \"Supervised and Unsupervised Learning of Parameterized Color Enhancement\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstracts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 28948,\n        \"samples\": [\n          \"object detector usual train larg amount label data expens labor intens pre train detector appli unlabel dataset alway suffer differ dataset distribut also call domain shift domain adapt object detect tri adapt detector label dataset unlabel one better perform paper first reveal region propos network rpn region propos classifi rpc endem two stage detector e g faster rcnn demonstr significantli differ transfer face larg domain gap region classifi show prefer perform limit without rpn high qualiti propos simpl align backbon network effect enough rpn adapt delv consist differ rpn rpc treat individu leverag high confid output one mutual guidanc train moreov sampl low confid use discrep calcul rpn rpc minimax optim extens experiment result variou scenario demonstr effect propos method domain adapt region propos gener object detect code avail http github com ganlongzhao cst da detect\",\n          \"hyperspectr imag hsi classif improv convolut neural network cnn recent year differ rgb dataset differ hsi dataset gener captur variou remot sensor differ spectral configur moreov hsi dataset contain limit train sampl thu prone overfit use deep cnn paper first deliv asymmetr incept network ainet overcom overfit problem emphasi spectral signatur spatial context hsi data ainet convey classifi featur effect addit propos data fusion transfer learn strategi benefici boost classif perform extens experi show propos approach beat state art method sever hsi benchmark includ pavia univers indian pine kennedi space center ksc code found http github com unilaux ainet\",\n          \"correspond imag fundament problem comput vision varieti graphic applic paper present novel method spars cross domain correspond method design pair imag main object interest may belong differ semant categori differ drastic shape appear yet still contain semant relat geometr similar part approach oper hierarchi deep featur extract input imag pre train cnn specif start coarsest layer hierarchi search neural best buddi nbb pair neuron mutual nearest neighbor key idea percol nbb hierarchi narrow search region level retain nbb signific activ furthermor order overcom differ appear pair search region transform common appear evalu method via user studi addit comparison altern correspond approach use method demonstr use varieti graphic applic includ cross domain imag align creation hybrid imag automat imag morph\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the maximum sequence length and batch size\n",
        "max_seqlen = 150\n",
        "batch_size = 128\n",
        "\n",
        "# Define the function to create the dataset\n",
        "def make_dataset(dataframe, labels, is_train):\n",
        "\n",
        "    # Convert the abstracts column to a TensorFlow dataset\n",
        "    abstracts_dataset = tf.data.Dataset.from_tensor_slices(dataframe['abstracts'].values)\n",
        "\n",
        "    # Create a dataset of labels using the binarized representation\n",
        "    labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n",
        "\n",
        "    # Zip the abstracts and labels datasets together to create a single dataset of tuples\n",
        "    dataset = tf.data.Dataset.zip((abstracts_dataset, labels_dataset))\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    dataset = dataset.shuffle(buffer_size=len(dataframe)) if is_train else dataset\n",
        "\n",
        "    return dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "id": "tEEIXuqNGf2T"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = make_dataset(train_df, train_labels_binarized, is_train=True)\n",
        "validation_dataset = make_dataset(val_df, train_labels_binarized, is_train=False)\n",
        "test_dataset = make_dataset(test_df, train_labels_binarized, is_train=False)"
      ],
      "metadata": {
        "id": "whWF_o2bHXd9"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujvO5JVuHsJg"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to invert the multi-hot encoded labels\n",
        "def invert_multi_hot(label):\n",
        "    return [vocab[i] for i, val in enumerate(label) if val == 1]\n",
        "\n",
        "# Iterate through batches of the training dataset and print the abstract text along with the corresponding labels\n",
        "text_batch, label_batch = next(iter(train_dataset))\n",
        "for i, text in enumerate(text_batch[:5]):\n",
        "    label = label_batch[i].numpy()\n",
        "    print(f\"Abstract: {text}\")\n",
        "    print(f\"Label(s): {invert_multi_hot(label)}\")\n",
        "    print(\" \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mfFt6uXHuu_",
        "outputId": "b1b7da53-f4a7-4834-d7c9-be6d36faddbf"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract: b'incorpor multi scale featur fulli convolut neural network fcn key element achiev state art perform semant imag segment one common way extract multi scale featur feed multipl resiz input imag share deep network merg result featur pixelwis classif work propos attent mechan learn softli weight multi scale featur pixel locat adapt state art semant imag segment model jointli train multi scale input imag attent model propos attent model outperform averag max pool allow us diagnost visual import featur differ posit scale moreov show ad extra supervis output scale essenti achiev excel perform merg multi scale featur demonstr effect model extens experi three challeng dataset includ pascal person part pascal voc subset ms coco'\n",
            "Label(s): ['cs.CV']\n",
            " \n",
            "Abstract: b'recent cnn base method imag derain achiev excel perform term reconstruct error well visual qualiti howev method limit sens train fulli label data due variou challeng obtain real world fulli label imag derain dataset exist method train synthet gener data henc gener poorli real world imag use real world data train imag derain network rel less explor literatur propos gaussian process base semi supervis learn framework enabl network learn derain use synthet dataset gener better use unlabel real world imag extens experi ablat sever challeng dataset rain rain h ddn sirr show propos method train limit label data achiev par perform fulli label train addit demonstr use unlabel real world imag propos gp base framework result superior perform compar exist method code avail http github com rajeevyasarla syn real'\n",
            "Label(s): ['cs.CV']\n",
            " \n",
            "Abstract: b'one challeng aspect real world reinforc learn rl multitud unpredict ever chang distract could divert agent task train environ agent could learn reward signal ignor complex real world make reward hard acquir best extrem spars recent class self supervis method shown promis reward free adapt challeng distract possibl howev previou work focus short one episod adapt set paper consid long term adapt setup akin specif real world propos geometr perspect self supervis adapt empir describ process take place embed space adapt process reveal undesir effect perform show elimin moreov theoret studi actor base actor free agent generalis target environ manipul geometri manifold describ actor critic function'\n",
            "Label(s): ['cs.AI', 'cs.LG']\n",
            " \n",
            "Abstract: b'multi task learn mtl effici way improv perform relat task share knowledg howev exist mtl network run singl end suitabl collabor intellig ci scenario work propos mtl network deep joint sourc channel code jscc framework allow oper ci scenario first propos featur fusion base mtl network ffmnet joint object detect semant segment compar mtl network ffmnet get higher perform fewer paramet ffmnet split two part run mobil devic edg server respect featur gener mobil devic transmit wireless channel edg server reduc transmiss overhead intermedi featur deep jscc network design combin two network togeth whole model achiev x compress intermedi featur perform loss within task last train nois ffmnet jscc robust variou channel condit outperform separ sourc channel code scheme'\n",
            "Label(s): ['cs.AI', 'cs.CV']\n",
            " \n",
            "Abstract: b'fair awar machin learn multipl protect tribut refer multi fair hereaft receiv increas attent tradit singl protect attribut approach cannot en sure fair w r protect attribut exist method ever still ignor fact dataset domain often imbalanc lead unfair decis toward minor class thu solut need achiev multi fair accur predict perform overal balanc perform across differ class end introduc new fair notion multi max mistreat mmm measur unfair consid multi attribut protect group class membership instanc learn mmm fair classifi propos multi object problem formul solv problem use boost approach train incorpor multi fair treatment distribut updat post train find multipl pareto optim solut use pseudo weight base decis make select optim solut among accur balanc multi attribut fair solut'\n",
            "Label(s): ['cs.CY', 'cs.LG']\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set()\n",
        "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73wjCrqnH0oH",
        "outputId": "e3729186-5c8d-446c-f82e-c8db3f60982a"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializes a TextVectorization layer\n",
        "text_vectorizer = layers.TextVectorization(max_tokens=vocabulary_size,ngrams=2,output_mode=\"tf_idf\")\n",
        "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "jtG9s7A5IEZY"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
        "with open(\"text_vectorizer_config.pkl\", \"wb\") as f:\n",
        "    pickle.dump(saved_text_vectorizer_config, f)\n",
        "\n",
        "weights = text_vectorizer.get_weights()\n",
        "# Save the weights to a pickle file\n",
        "with open(\"text_vectorizer_weights.pkl\", \"wb\") as f:\n",
        "    pickle.dump(weights, f)\n"
      ],
      "metadata": {
        "id": "PrsMy3yWw-lM"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = train_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
        "validation_dataset = validation_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
        "test_dataset = test_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n"
      ],
      "metadata": {
        "id": "NhHzfeAyIJ8Q"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assuming you have a train_dataset TensorFlow dataset object\n",
        "\n",
        "# Define the number of elements to inspect\n",
        "num_elements = 5  # You can change this to inspect more or fewer elements\n",
        "\n",
        "# Take the first num_elements elements from the dataset\n",
        "sample_data = train_dataset.take(num_elements)\n",
        "\n",
        "# Iterate over the sample_data and print each element\n",
        "for sample in sample_data:\n",
        "    # Assuming the sample contains input features and labels\n",
        "    input_features, labels = sample\n",
        "    print(\"Input Features:\")\n",
        "    print(input_features)\n",
        "    print(\"Labels:\")\n",
        "    print(labels)\n",
        "    print(\"=\"*50)  # Separating each sample with '=' characters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPAXmK1qIPDk",
        "outputId": "7b0039f7-d144-4889-cb3d-0eba25a35175"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[468.37527      0.93528265   2.0206993  ...   0.           0.\n",
            "    0.        ]\n",
            " [535.286        0.93528265   0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [535.286        3.7411306    1.0103496  ...   0.           0.\n",
            "    0.        ]\n",
            " ...\n",
            " [194.04118      0.93528265   0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [588.81464      4.676413     0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [368.00916      5.611696     0.         ...   0.           0.\n",
            "    0.        ]], shape=(128, 32945), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 1 1]\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 1 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n",
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[615.5789       6.5469785    4.0413985  ...   0.           0.\n",
            "    0.        ]\n",
            " [528.595        0.93528265   0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [461.6842       0.           4.0413985  ...   0.           0.\n",
            "    0.        ]\n",
            " ...\n",
            " [555.35925      0.93528265   1.0103496  ...   0.           0.\n",
            "    0.        ]\n",
            " [314.48053      0.93528265   2.0206993  ...   0.           0.\n",
            "    0.        ]\n",
            " [428.22882      2.805848     4.0413985  ...   0.           0.\n",
            "    0.        ]], shape=(128, 32945), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n",
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[381.3913       0.           3.0310488  ...   0.           0.\n",
            "    0.        ]\n",
            " [254.26086      0.           0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [374.70023      1.8705653    2.0206993  ...   0.           0.\n",
            "    0.        ]\n",
            " ...\n",
            " [260.95193      3.7411306    3.0310488  ...   0.           0.\n",
            "    0.        ]\n",
            " [374.70023      0.93528265   3.0310488  ...   0.           0.\n",
            "    0.        ]\n",
            " [441.61096      0.           0.         ...   0.           0.\n",
            "    0.        ]], shape=(128, 32945), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n",
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[240.87871      3.7411306    2.0206993  ...   0.           0.\n",
            "    0.        ]\n",
            " [488.4485       3.7411306    0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [742.70935      0.           0.         ...   0.           0.\n",
            "    0.        ]\n",
            " ...\n",
            " [528.595        1.8705653    0.         ...   0.           0.\n",
            "    0.        ]\n",
            " [468.37527      0.           1.0103496  ...   0.           0.\n",
            "    0.        ]\n",
            " [401.4645       0.93528265   0.         ...   0.           0.\n",
            "    0.        ]], shape=(128, 32945), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 1 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n",
            "Input Features:\n",
            "tf.Tensor(\n",
            "[[408.1556      3.7411306   3.0310488 ...   0.          0.\n",
            "    0.       ]\n",
            " [227.49657     2.805848    1.0103496 ...   0.          0.\n",
            "    0.       ]\n",
            " [689.1808      0.         11.113846  ...   0.          0.\n",
            "    0.       ]\n",
            " ...\n",
            " [281.02518     0.          1.0103496 ...   0.          0.\n",
            "    0.       ]\n",
            " [307.78946     1.8705653   2.0206993 ...   0.          0.\n",
            "    0.       ]\n",
            " [441.61096     1.8705653   0.        ...   0.          0.\n",
            "    0.       ]], shape=(128, 32945), dtype=float32)\n",
            "Labels:\n",
            "tf.Tensor(\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]], shape=(128, 164), dtype=int64)\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(mlb.classes_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKdskp_4Igui",
        "outputId": "6d648e45-8657-47e7-9ab2-57185b5c113e"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Creating shallow_mlp_model (MLP) with dropout layers\n",
        "model1 = keras.Sequential([\n",
        "    # First hidden layer: 512 neurons, ReLU activation function, with dropout.\n",
        "    layers.Dense(256, activation=\"relu\"),\n",
        "    layers.Dropout(0.6),\n",
        "\n",
        "    # Second hidden layer: 256 neurons, ReLU activation function, with dropout.\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.6),\n",
        "\n",
        "    # Output layer: The number of neurons equals the vocabulary size (output vocabulary of the StringLookup layer), with a sigmoid activation function.\n",
        "    layers.Dense(len(vocab), activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
        "\n",
        "# Add early stopping\n",
        "# Number of epochs with no improvement after which training will be stopped.\n",
        "# Restore weights from the epoch with the best value of the monitored quantity.\n",
        "early_stopping = EarlyStopping(patience=4,restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "# Add early stopping callback.verbose=1\n",
        "history = model1.fit(train_dataset,validation_data=validation_dataset,epochs=20,callbacks=[early_stopping])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZusuZerIS06",
        "outputId": "26ae4fcc-08fd-4b8c-fb91-d0f2d1f8cf1d"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "227/227 [==============================] - 70s 302ms/step - loss: 0.1202 - binary_accuracy: 0.9540 - val_loss: 0.0465 - val_binary_accuracy: 0.9889\n",
            "Epoch 2/20\n",
            "227/227 [==============================] - 66s 290ms/step - loss: 0.0260 - binary_accuracy: 0.9935 - val_loss: 0.0420 - val_binary_accuracy: 0.9895\n",
            "Epoch 3/20\n",
            "227/227 [==============================] - 67s 293ms/step - loss: 0.0226 - binary_accuracy: 0.9942 - val_loss: 0.0456 - val_binary_accuracy: 0.9892\n",
            "Epoch 4/20\n",
            "227/227 [==============================] - 67s 293ms/step - loss: 0.0210 - binary_accuracy: 0.9944 - val_loss: 0.0429 - val_binary_accuracy: 0.9896\n",
            "Epoch 5/20\n",
            "227/227 [==============================] - 66s 291ms/step - loss: 0.0199 - binary_accuracy: 0.9946 - val_loss: 0.0476 - val_binary_accuracy: 0.9892\n",
            "Epoch 6/20\n",
            "227/227 [==============================] - 68s 299ms/step - loss: 0.0190 - binary_accuracy: 0.9947 - val_loss: 0.0446 - val_binary_accuracy: 0.9892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaltuation on test and val dataset\n",
        "_, binary_acc1 = model1.evaluate(test_dataset)\n",
        "_, binary_acc2 = model1.evaluate(validation_dataset)\n",
        "\n",
        "print(f\"Categorical accuracy on the test set: {round(binary_acc1 * 100, 2)}%.\")\n",
        "print(f\"Categorical accuracy on the validation set: {round(binary_acc2 * 100, 2)}%.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY19delNJRzr",
        "outputId": "9ef377cb-9112-45d1-9496-59547abce206"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 2s 60ms/step - loss: 0.0424 - binary_accuracy: 0.9894\n",
            "38/38 [==============================] - 3s 65ms/step - loss: 0.0420 - binary_accuracy: 0.9895\n",
            "Categorical accuracy on the test set: 98.94%.\n",
            "Categorical accuracy on the validation set: 98.95%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model, text_vectorizer, and vocab as pickle files.\n",
        "\n",
        "with open('model1.pkl', 'wb') as f:\n",
        "  pickle.dump(model1, f)\n",
        "\n",
        "with open('vocab.pkl', 'wb') as f:\n",
        "  pickle.dump(vocab, f)"
      ],
      "metadata": {
        "id": "YoZU2-UXTkeE"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import pickle\n",
        "\n",
        "# Load the model\n",
        "with open(\"model1.pkl\", \"rb\") as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "# Load the configuration of the text vectorizer\n",
        "with open(\"text_vectorizer_config.pkl\", \"rb\") as f:\n",
        "    saved_text_vectorizer_config = pickle.load(f)\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "# Create a new TextVectorization layer with the saved configuration\n",
        "loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)"
      ],
      "metadata": {
        "id": "uoevUzFIf1-G"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the vocabulary\n",
        "with open(\"vocab.pkl\", \"rb\") as f:\n",
        "  loaded_vocab = pickle.load(f)"
      ],
      "metadata": {
        "id": "imO4h0o6rtn1"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to invert the multi-hot encoded labels\n",
        "def invert_multi_hot(label):\n",
        "    return [vocab[i] for i, val in enumerate(label) if val == 1]\n"
      ],
      "metadata": {
        "id": "fAXY2tmJsZmm"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-oafZrS2tBoN"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate predict_category function as per my implementation by modifying above code\n",
        "\n",
        "def predict_category(abstract, model):\n",
        "    # Preprocess the abstract using the loaded text vectorizer\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    predictions = model.predict(abstract)\n",
        "\n",
        "    # # Convert predictions to human-readable labels\n",
        "    # predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
        "\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "TnJvJj1QuB3z"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6tG1U_Sucbu"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Load the configuration of the text vectorizer\n",
        "with open(\"text_vectorizer_config.pkl\", \"rb\") as f:\n",
        "    saved_text_vectorizer_config = pickle.load(f)\n",
        "\n",
        "# Create a new TextVectorization layer with the saved configuration\n",
        "loaded_text_vectorizer = tf.keras.layers.TextVectorization.from_config(saved_text_vectorizer_config)\n",
        "\n",
        "# Load the saved weights into the new TextVectorization layer\n",
        "with open(\"text_vectorizer_weights.pkl\", \"rb\") as f:\n",
        "    weights = pickle.load(f)\n",
        "loaded_text_vectorizer.set_weights(weights)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JlPYiN1tTm3",
        "outputId": "d65be80b-6753-4440-a8f7-5755d63fc718"
      },
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[2.9039268e+03 9.3528265e-01 2.0206993e+00 ... 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00]], shape=(1, 32945), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the input abstract provided by the user\n",
        "# input_abstract = \"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\"  # Replace with the user's input\n",
        "input_abstract = input()\n",
        "# Use the new TextVectorization layer to transform the preprocessed input abstract into a tensor\n",
        "abstract_tensor = loaded_text_vectorizer(input_abstract)\n",
        "print(\"1\", type(abstract_tensor), abstract_tensor.shape)\n",
        "abstract_tensor = tf.reshape(abstract_tensor, (1, -1))  # Reshape to add a batch dimension\n",
        "print(\"2\", type(abstract_tensor), abstract_tensor.shape)\n",
        "print(abstract_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIellk88Jhbt",
        "outputId": "308a603f-5e1c-4f08-f654-3d1fe26d0bdd"
      },
      "execution_count": 341,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semi-supervised learning is a classification method which makes use of both labeled data and unlabeled data for training. In this paper, we propose a semi-supervised learning algorithm using a Bayesian semi-supervised model. We make a general assumption that the observations will follow two multivariate normal distributions depending on their true labels after the same unknown transformation. We use B-splines to put a prior on the transformation function for each component. To use unlabeled data in a semi-supervised setting, we assume the labels are missing at random. The posterior distributions can then be described using our assumptions, which we compute by the Gibbs sampling technique. The proposed method is then compared with several other available methods through an extensive simulation study. Finally we apply the proposed method in real data contexts for diagnosing breast cancer and classify radar returns. We conclude that the proposed method has better prediction accuracy in a wide variety of cases.\n",
            "1 <class 'tensorflow.python.framework.ops.EagerTensor'> (32945,)\n",
            "2 <class 'tensorflow.python.framework.ops.EagerTensor'> (1, 32945)\n",
            "tf.Tensor(\n",
            "[[1.7998993e+03 0.0000000e+00 1.0103496e+00 ... 0.0000000e+00\n",
            "  0.0000000e+00 0.0000000e+00]], shape=(1, 32945), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `model1` is your trained model\n",
        "# Assuming `abstract_tensor` is the tensor representing the input abstract\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions = predict_category(abstract_tensor, loaded_model)\n",
        "\n",
        "# Define a threshold for binary classification\n",
        "threshold = 0.05\n",
        "\n",
        "# Convert probabilities to binary predictions\n",
        "binary_predictions = (predictions > threshold).astype(int)\n",
        "print(type(binary_predictions))\n",
        "print(\"Binary Predictions:\")\n",
        "print(binary_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DsSauYWtbf0",
        "outputId": "840b4d1f-30c2-4c58-ee28-28ec8e5055f6"
      },
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 38ms/step\n",
            "<class 'numpy.ndarray'>\n",
            "Binary Predictions:\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def invert_multi_hot(encoded_labels):\n",
        "    \"\"\"Reverse a single multi-hot encoded label to a list of vocab terms.\"\"\"\n",
        "    hot_indices = np.argwhere(encoded_labels == 1)\n",
        "    hot_indices = hot_indices[:, 1:]  # Remove the first column (index 0)\n",
        "    terms_list = [loaded_vocab[index] for index in hot_indices.flatten()]\n",
        "    return terms_list\n",
        "\n",
        "predicted_terms = invert_multi_hot(binary_predictions)\n",
        "print(predicted_terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QNvviZNDpM7",
        "outputId": "92a5ebe1-4553-4ee1-8965-6ac6997c7d90"
      },
      "execution_count": 361,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cs.AI', 'cs.CV', 'cs.LG', 'stat.ML']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qz37wR9GGL_i"
      },
      "execution_count": 324,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vV0Ct-2YGTMD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}